{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80158d7f-c5df-415d-9cc9-03374b0475b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointtransformer_pure_torch.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f89f7df-99c4-443d-8e4c-1072ddf8f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helpers for offset handling\n",
    "# -----------------------------\n",
    "def _offset_to_ranges(offset: torch.Tensor):\n",
    "    \"\"\"\n",
    "    offset: (B,) cumulative counts, e.g., [n1, n1+n2, n1+n2+n3, ...]\n",
    "    yields (start, end) per batch.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    start = 0\n",
    "    for i in range(offset.shape[0]):\n",
    "        end = int(offset[i].item())\n",
    "        ranges.append((start, end))\n",
    "        start = end\n",
    "    return ranges\n",
    "\n",
    "# -----------------------------\n",
    "# Pure PyTorch FPS / kNN / Interp (batched by offset)\n",
    "# -----------------------------\n",
    "def fps(points: torch.Tensor, npoint: int):\n",
    "    \"\"\"\n",
    "    Farthest Point Sampling (single set, no offsets)\n",
    "    points: (N, 3)\n",
    "    npoint: int\n",
    "    returns: (npoint,) indices into points\n",
    "    \"\"\"\n",
    "    N = points.shape[0]\n",
    "    device = points.device\n",
    "    idx = torch.zeros(npoint, dtype=torch.long, device=device)\n",
    "    distances = torch.full((N,), 1e10, device=device)\n",
    "    farthest = torch.randint(0, N, (1,), device=device, dtype=torch.long)\n",
    "    for i in range(npoint):\n",
    "        idx[i] = farthest\n",
    "        centroid = points[farthest]  # (1, 3)\n",
    "        dist = torch.sum((points - centroid) ** 2, dim=-1)\n",
    "        distances = torch.minimum(distances, dist)\n",
    "        farthest = torch.argmax(distances)\n",
    "    return idx\n",
    "\n",
    "def furthest_sampling_offset(xyz: torch.Tensor, offset: torch.Tensor, new_offset: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Batched FPS using offsets.\n",
    "    xyz: (N, 3)\n",
    "    offset: (B,)\n",
    "    new_offset: (B,) cumulative number of samples per batch\n",
    "    return: (M,) indices (global) into xyz\n",
    "    \"\"\"\n",
    "    ranges = _offset_to_ranges(offset)\n",
    "    new_ranges = _offset_to_ranges(new_offset)\n",
    "    assert len(ranges) == len(new_ranges)\n",
    "\n",
    "    out_indices = []\n",
    "    for (s, e), (ns, ne) in zip(ranges, new_ranges):\n",
    "        N_b = e - s\n",
    "        M_b = ne - ns\n",
    "        if M_b <= 0:\n",
    "            continue\n",
    "        pts = xyz[s:e, :]  # (N_b, 3)\n",
    "        if M_b >= N_b:\n",
    "            idx_local = torch.arange(N_b, device=xyz.device, dtype=torch.long)\n",
    "        else:\n",
    "            idx_local = fps(pts, M_b)  # (M_b,)\n",
    "        out_indices.append(idx_local + s)\n",
    "    if len(out_indices) == 0:\n",
    "        return torch.empty((0,), dtype=torch.long, device=xyz.device)\n",
    "    return torch.cat(out_indices, dim=0)\n",
    "\n",
    "def queryandgroup_offset(nsample: int,\n",
    "                         xyz: torch.Tensor,   # (N,3)\n",
    "                         new_xyz: torch.Tensor,  # (M,3)\n",
    "                         feat: torch.Tensor,  # (N,C)\n",
    "                         offset: torch.Tensor,  # (B,)\n",
    "                         new_offset: torch.Tensor,  # (B,)\n",
    "                         use_xyz: bool = True):\n",
    "    \"\"\"\n",
    "    Batched kNN grouping using offsets.\n",
    "    Returns:\n",
    "        (M, nsample, 3+C) if use_xyz else (M, nsample, C)\n",
    "    \"\"\"\n",
    "    ranges = _offset_to_ranges(offset)\n",
    "    new_ranges = _offset_to_ranges(new_offset)\n",
    "    assert len(ranges) == len(new_ranges)\n",
    "\n",
    "    grouped_list = []\n",
    "    for (s, e), (ns, ne) in zip(ranges, new_ranges):\n",
    "        if ne - ns == 0:\n",
    "            continue\n",
    "        xyz_b = xyz[s:e, :]            # (Nb,3)\n",
    "        feat_b = feat[s:e, :]          # (Nb,C)\n",
    "        new_xyz_b = new_xyz[ns:ne, :]  # (Mb,3)\n",
    "\n",
    "        # (Mb, Nb) pairwise distances\n",
    "        dist = torch.cdist(new_xyz_b, xyz_b)\n",
    "        idx = dist.topk(min(nsample, xyz_b.shape[0]), largest=False)[1]  # (Mb, k)\n",
    "\n",
    "        grouped_feat = feat_b[idx]  # (Mb, k, C)\n",
    "        if use_xyz:\n",
    "            grouped_xyz = xyz_b[idx]  # (Mb, k, 3)\n",
    "            rel = grouped_xyz - new_xyz_b.unsqueeze(1)  # (Mb,k,3)\n",
    "            grouped = torch.cat([rel, grouped_feat], dim=-1)  # (Mb, k, 3+C)\n",
    "        else:\n",
    "            grouped = grouped_feat  # (Mb, k, C)\n",
    "        # If k < nsample (when Nb < nsample), we can pad (optional).\n",
    "        if grouped.shape[1] < nsample:\n",
    "            pad_k = nsample - grouped.shape[1]\n",
    "            pad_shape = list(grouped.shape)\n",
    "            pad_shape[1] = pad_k\n",
    "            grouped = torch.cat([grouped, grouped.new_zeros(pad_shape)], dim=1)\n",
    "        grouped_list.append(grouped)\n",
    "\n",
    "    if len(grouped_list) == 0:\n",
    "        Cg = (3 + feat.shape[1]) if use_xyz else feat.shape[1]\n",
    "        return torch.zeros((0, nsample, Cg), device=xyz.device, dtype=xyz.dtype)\n",
    "    return torch.cat(grouped_list, dim=0)\n",
    "\n",
    "def interpolation_offset(src_xyz: torch.Tensor,  # (N2,3)\n",
    "                         tgt_xyz: torch.Tensor,  # (N1,3)\n",
    "                         src_feat: torch.Tensor, # (N2,C)\n",
    "                         src_offset: torch.Tensor,  # (B,)\n",
    "                         tgt_offset: torch.Tensor,  # (B,)\n",
    "                         k: int = 3):\n",
    "    \"\"\"\n",
    "    Batched inverse-distance interpolation from src to tgt using offsets.\n",
    "    Returns:\n",
    "        (N1, C)\n",
    "    \"\"\"\n",
    "    ranges_src = _offset_to_ranges(src_offset)\n",
    "    ranges_tgt = _offset_to_ranges(tgt_offset)\n",
    "    assert len(ranges_src) == len(ranges_tgt)\n",
    "\n",
    "    outs = []\n",
    "    for (sS, eS), (sT, eT) in zip(ranges_src, ranges_tgt):\n",
    "        if eT - sT == 0:\n",
    "            continue\n",
    "        xyz2 = src_xyz[sS:eS, :]        # (N2b,3)\n",
    "        feat2 = src_feat[sS:eS, :]      # (N2b,C)\n",
    "        xyz1 = tgt_xyz[sT:eT, :]        # (N1b,3)\n",
    "\n",
    "        if xyz2.shape[0] == 0:\n",
    "            outs.append(torch.zeros((xyz1.shape[0], feat2.shape[1]), device=src_feat.device, dtype=src_feat.dtype))\n",
    "            continue\n",
    "\n",
    "        dist = torch.cdist(xyz1, xyz2)  # (N1b, N2b)\n",
    "        k_eff = min(k, xyz2.shape[0])\n",
    "        idx = dist.topk(k_eff, largest=False)[1]  # (N1b,k_eff)\n",
    "        d = torch.gather(dist, 1, idx) + 1e-8     # (N1b,k_eff)\n",
    "\n",
    "        w = 1.0 / d\n",
    "        w = w / w.sum(dim=1, keepdim=True)  # (N1b,k_eff)\n",
    "        interp = torch.sum(feat2[idx] * w.unsqueeze(-1), dim=1)  # (N1b,C)\n",
    "        outs.append(interp)\n",
    "\n",
    "    if len(outs) == 0:\n",
    "        return torch.zeros((0, src_feat.shape[1]), device=src_feat.device, dtype=src_feat.dtype)\n",
    "    return torch.cat(outs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e396a76-cbce-4f69-b8f2-d4c381c43728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "class PointTransformerLayer(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, share_planes=8, nsample=16):\n",
    "        super().__init__()\n",
    "        self.mid_planes = mid_planes = out_planes // 1\n",
    "        self.out_planes = out_planes\n",
    "        self.share_planes = share_planes\n",
    "        self.nsample = nsample\n",
    "\n",
    "        self.linear_q = nn.Linear(in_planes, mid_planes)\n",
    "        self.linear_k = nn.Linear(in_planes, mid_planes)\n",
    "        self.linear_v = nn.Linear(in_planes, out_planes)\n",
    "\n",
    "        self.linear_p = nn.Sequential(\n",
    "            nn.Linear(3, 3),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(3, out_planes),\n",
    "        )\n",
    "        self.linear_w = nn.Sequential(\n",
    "            nn.BatchNorm1d(mid_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(mid_planes, mid_planes // share_planes),\n",
    "            nn.BatchNorm1d(mid_planes // share_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_planes // share_planes, out_planes // share_planes),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, pxo) -> torch.Tensor:\n",
    "        p, x, o = pxo  # p: (N,3), x: (N,C_in), o: (B,)\n",
    "        x_q, x_k, x_v = self.linear_q(x), self.linear_k(x), self.linear_v(x)  # (N, *)\n",
    "        # (N, nsample, 3+C_mid) and (N, nsample, C_out)\n",
    "        x_k_group = queryandgroup_offset(self.nsample, p, p, x_k, o, o, use_xyz=True)\n",
    "        x_v_group = queryandgroup_offset(self.nsample, p, p, x_v, o, o, use_xyz=False)\n",
    "        p_r, x_kn = x_k_group[:, :, 0:3], x_k_group[:, :, 3:]  # (N,ns,3), (N,ns,C_mid)\n",
    "\n",
    "        # position encoding MLP across neighbor dimension:\n",
    "        # handle BN1d by transposing to (N*ns,3) or (N*ns, out_planes)\n",
    "        Nns = p_r.shape[0] * p_r.shape[1]\n",
    "        pr = p_r.reshape(Nns, 3)\n",
    "        # replicate the original trick around BN1d position in Sequential\n",
    "        out_pr = pr\n",
    "        for i, layer in enumerate(self.linear_p):\n",
    "            if i == 1:  # BatchNorm1d\n",
    "                out_pr = layer(out_pr)\n",
    "            else:\n",
    "                out_pr = layer(out_pr)\n",
    "        p_r_enc = out_pr.view(p_r.shape[0], p_r.shape[1], self.out_planes)  # (N,ns,out_planes)\n",
    "\n",
    "        # attention weights\n",
    "        w = x_kn - x_q.unsqueeze(1) + p_r_enc.view(\n",
    "            p_r_enc.shape[0], p_r_enc.shape[1],\n",
    "            self.out_planes // self.mid_planes, self.mid_planes\n",
    "        ).sum(2)  # (N,ns,C_mid)\n",
    "\n",
    "        # apply linear_w with BN along feature dim:\n",
    "        # linear_w expects (B,C,L) or (N*,C) with BN1d on C, so we reshape\n",
    "        W = w.transpose(1, 2).contiguous()  # (N, C_mid, ns)\n",
    "        # We need to mimic the original step-by-step application with transposes around BN layers in linear_w\n",
    "        # But an easier and safe way: apply per last-dim MLP with BN1d by flattening (N*ns, C_mid)\n",
    "        w_flat = w.reshape(-1, w.shape[-1])\n",
    "        # Re-build linear_w manually with the same layers to preserve behavior with BN1d\n",
    "        # (We already created self.linear_w; we apply it by managing shapes)\n",
    "        # Pass through the sequence while toggling shapes for BN1d positions (they expect (N,C))\n",
    "        tmp = w_flat\n",
    "        for j, layer in enumerate(self.linear_w):\n",
    "            if isinstance(layer, nn.BatchNorm1d):\n",
    "                tmp = layer(tmp)\n",
    "            else:\n",
    "                tmp = layer(tmp)\n",
    "        w = tmp.view(w.shape[0], w.shape[1], -1)  # (N,ns,C_mid/ share) at the end of MLP\n",
    "        w = self.softmax(w)  # softmax along neighbor dim (dim=1)\n",
    "\n",
    "        n, nsample, c = x_v_group.shape\n",
    "        s = self.share_planes\n",
    "        # (x_v + p_r_enc) has C_out; we share along s\n",
    "        x_out = ((x_v_group + p_r_enc).view(n, nsample, s, c // s) * w.unsqueeze(2)).sum(1).view(n, c)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fb155d9-d3fb-4af7-a049-d6b1fc393864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionDown(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, nsample=16):\n",
    "        super().__init__()\n",
    "        self.stride, self.nsample = stride, nsample\n",
    "        if stride != 1:\n",
    "            self.linear = nn.Linear(3 + in_planes, out_planes, bias=False)\n",
    "            self.pool = nn.MaxPool1d(nsample)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_planes, out_planes, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, pxo):\n",
    "        p, x, o = pxo  # (N,3), (N,C), (B,) \n",
    "        print(\"Enc p=\", p.shape, \"x=\", x.shape, \"o=\", o)\n",
    "        if self.stride != 1:\n",
    "            # new_offset: downsample each batch by integer factor\n",
    "            n_o_list = []\n",
    "            count = int(o[0].item()) // self.stride\n",
    "            n_o_list.append(count)\n",
    "            for i in range(1, o.shape[0]):\n",
    "                count += int((o[i].item() - o[i-1].item()) // self.stride)\n",
    "                n_o_list.append(count)\n",
    "            n_o = torch.tensor(n_o_list, device=o.device, dtype=o.dtype)\n",
    "\n",
    "            idx = furthest_sampling_offset(p, o, n_o)  # (M,)\n",
    "            n_p = p[idx.long(), :]  # (M,3)\n",
    "            # (M, ns, 3+C_in)\n",
    "            grouped = queryandgroup_offset(self.nsample, p, n_p, x, o, n_o, use_xyz=True)\n",
    "            # linear over last dim -> (M, ns, out_planes)\n",
    "            xg = self.linear(grouped)\n",
    "            # to (M, out_planes, ns) for 1D pool\n",
    "            xg = xg.transpose(1, 2).contiguous()\n",
    "            xg = self.relu(self.bn(xg))  # BN over channel\n",
    "            xg = self.pool(xg).squeeze(-1)  # (M, out_planes)\n",
    "            p, x, o = n_p, xg, n_o\n",
    "        else:\n",
    "            x = self.relu(self.bn(self.linear(x)))  # (N,out_planes)\n",
    "        return [p, x, o]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13052381-0556-452b-bb31-234b3e461997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes=None):\n",
    "        super().__init__()\n",
    "        if out_planes is None:\n",
    "            self.linear1 = nn.Sequential(\n",
    "                nn.Linear(2 * in_planes, in_planes),\n",
    "                nn.BatchNorm1d(in_planes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.linear2 = nn.Sequential(\n",
    "                nn.Linear(in_planes, in_planes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.linear1 = nn.Sequential(\n",
    "                nn.Linear(out_planes, out_planes),\n",
    "                nn.BatchNorm1d(out_planes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.linear2 = nn.Sequential(\n",
    "                nn.Linear(in_planes, out_planes),\n",
    "                nn.BatchNorm1d(out_planes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, pxo1, pxo2=None):\n",
    "        if pxo2 is None:\n",
    "            _, x, o = pxo1  # (N,3), (N,C), (B,)\n",
    "            x_tmp = []\n",
    "            for i in range(o.shape[0]):\n",
    "                if i == 0:\n",
    "                    s_i, e_i, cnt = 0, int(o[0].item()), int(o[0].item())\n",
    "                else:\n",
    "                    s_i, e_i, cnt = int(o[i-1].item()), int(o[i].item()), int(o[i].item() - o[i-1].item())\n",
    "                x_b = x[s_i:e_i, :]\n",
    "                x_b = torch.cat((x_b, self.linear2(x_b.sum(0, keepdim=True) / cnt).repeat(cnt, 1)), 1)\n",
    "                x_tmp.append(x_b)\n",
    "            x = torch.cat(x_tmp, 0)\n",
    "            x = self.linear1(x)\n",
    "        else:\n",
    "            p1, x1, o1 = pxo1\n",
    "            p2, x2, o2 = pxo2\n",
    "            # interpolate features from p2->p1\n",
    "            interp = interpolation_offset(p2, p1, self.linear2(x2), o2, o1, k=3)\n",
    "            x = self.linear1(x1) + interp\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e39e5cc7-9dca-410a-891b-e8c70eb47dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointTransformerBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, share_planes=8, nsample=16):\n",
    "        super(PointTransformerBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_planes, planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.transformer2 = PointTransformerLayer(planes, planes, share_planes, nsample)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.linear3 = nn.Linear(planes, planes * self.expansion, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, pxo):\n",
    "        p, x, o = pxo\n",
    "        identity = x\n",
    "        x = self.relu(self.bn1(self.linear1(x)))\n",
    "        x = self.relu(self.bn2(self.transformer2([p, x, o])))\n",
    "        x = self.bn3(self.linear3(x))\n",
    "        x = x + identity\n",
    "        x = self.relu(x)\n",
    "        return [p, x, o]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "67eccab9-83a0-49d7-b68e-d81dd176b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointTransformerSeg(nn.Module):\n",
    "    def __init__(self, block, blocks, c=6, k=13):\n",
    "        super().__init__() \n",
    "        #print(\"block=\", block)\n",
    "        #print(\"blocks=\", blocks) \n",
    "        \n",
    "        self.c = c\n",
    "        self.in_planes, planes = c, [32, 64, 128, 256, 512]\n",
    "        share_planes = 8\n",
    "        stride, nsample = [1, 4, 4, 4, 4], [8, 16, 16, 16, 16]\n",
    "\n",
    "        self.enc1 = self._make_enc(block, planes[0], blocks[0], share_planes, stride=stride[0], nsample=nsample[0])  # N/1\n",
    "        self.enc2 = self._make_enc(block, planes[1], blocks[1], share_planes, stride=stride[1], nsample=nsample[1])  # N/4\n",
    "        self.enc3 = self._make_enc(block, planes[2], blocks[2], share_planes, stride=stride[2], nsample=nsample[2])  # N/16\n",
    "        self.enc4 = self._make_enc(block, planes[3], blocks[3], share_planes, stride=stride[3], nsample=nsample[3])  # N/64\n",
    "        self.enc5 = self._make_enc(block, planes[4], blocks[4], share_planes, stride=stride[4], nsample=nsample[4])  # N/256\n",
    "\n",
    "        self.dec5 = self._make_dec(block, planes[4], 2, share_planes, nsample=nsample[4], is_head=True)  # transform p5\n",
    "        self.dec4 = self._make_dec(block, planes[3], 2, share_planes, nsample=nsample[3])  # fusion p5 and p4\n",
    "        self.dec3 = self._make_dec(block, planes[2], 2, share_planes, nsample=nsample[2])  # fusion p4 and p3\n",
    "        self.dec2 = self._make_dec(block, planes[1], 2, share_planes, nsample=nsample[1])  # fusion p3 and p2\n",
    "        self.dec1 = self._make_dec(block, planes[0], 2, share_planes, nsample=nsample[0])  # fusion p2 and p1\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(planes[0], planes[0]),\n",
    "            nn.BatchNorm1d(planes[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(planes[0], k)\n",
    "        )\n",
    "\n",
    "    def _make_enc(self, block, planes, blocks, share_planes=8, stride=1, nsample=16):\n",
    "        layers = []\n",
    "        layers.append(TransitionDown(self.in_planes, planes * block.expansion, stride, nsample))\n",
    "        #print(\"layers = \", layers, \"expension=\", block.expansion)\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, self.in_planes, share_planes, nsample=nsample)) #share plane = 8\n",
    "        #print(layers)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_dec(self, block, planes, blocks, share_planes=8, nsample=16, is_head=False):\n",
    "        layers = []\n",
    "        layers.append(TransitionUp(self.in_planes, None if is_head else planes * block.expansion))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, self.in_planes, share_planes, nsample=nsample)) \n",
    "        #print(nn.Sequential(*layers))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, pxo):\n",
    "        p0, x0, o0 = pxo  # p0:(N,3), x0:(N,C_in), o0:(B,)\n",
    "        x0 = p0 if self.c == 3 else torch.cat((p0, x0), 1)\n",
    "\n",
    "        print(\"p0=\", p0.shape, \"x0=\", x0.shape, \"o0=\", o0)\n",
    "\n",
    "        p1, x1, o1 = self.enc1([p0, x0, o0])\n",
    "        print(\"p1=\", p1.shape, \"x1=\", x1.shape, \"o1=\", o1)\n",
    "        p2, x2, o2 = self.enc2([p1, x1, o1])\n",
    "        print(\"p2=\", p2.shape, \"x2=\", x2.shape, \"o2=\", o2)\n",
    "        p3, x3, o3 = self.enc3([p2, x2, o2])\n",
    "        print(\"p3=\", p3.shape, \"x3=\", x3.shape, \"o3=\", o3)\n",
    "        p4, x4, o4 = self.enc4([p3, x3, o3]) \n",
    "        print(\"p4=\", p4.shape, \"x4=\", x4.shape, \"o4=\", o4)\n",
    "        p5, x5, o5 = self.enc5([p4, x4, o4])\n",
    "        print(\"p5=\", p5.shape, \"x5=\", x5.shape, \"o5=\", o5) \n",
    "\n",
    "        print(\"****************** Decoder *****************\")\n",
    "\n",
    "        x5 = self.dec5[1:]([p5, self.dec5[0]([p5, x5, o5]), o5])[1] \n",
    "        print(\"x5=\", x5.shape)\n",
    "        x4 = self.dec4[1:]([p4, self.dec4[0]([p4, x4, o4], [p5, x5, o5]), o4])[1] \n",
    "        print(\"x4=\", x4.shape)\n",
    "        x3 = self.dec3[1:]([p3, self.dec3[0]([p3, x3, o3], [p4, x4, o4]), o3])[1] \n",
    "        print(\"x3=\", x3.shape)\n",
    "        x2 = self.dec2[1:]([p2, self.dec2[0]([p2, x2, o2], [p3, x3, o3]), o2])[1]\n",
    "        print(\"x2=\", x2.shape)\n",
    "        x1 = self.dec1[1:]([p1, self.dec1[0]([p1, x1, o1], [p2, x2, o2]), o1])[1] \n",
    "        print(\"x1=\", x1.shape)\n",
    "\n",
    "        x = self.cls(x1) \n",
    "        print(\"x=\", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "48a7ad62-64c2-4abd-b6f0-57abe438027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointtransformer_seg_repro(**kwargs):\n",
    "    model = PointTransformerSeg(PointTransformerBlock, [2, 3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6548bee-1be2-476f-bfca-d073faaba521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example shapes\n",
    "N1, N2 = 1024, 2048  # total points across batches\n",
    "B = 2                # batches\n",
    "C_in = 3             # input feature channels (excluding xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cc0067e9-5822-41d4-84dc-7542c764fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "p = torch.randn(N1, 3)                 # (N,3)\n",
    "x = torch.randn(N1, C_in)              # (N,C)\n",
    "o = torch.tensor([N1//2, N1], dtype=torch.long)  # cumulative offsets for B=2 [512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e8f7885-c224-4733-966c-748313084c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransitionDown(\n",
      "  (linear): Linear(in_features=6, out_features=32, bias=False)\n",
      "  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (linear_k): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (linear_v): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=32, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=32, out_features=4, bias=True)\n",
      "      (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=4, out_features=4, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n",
      "[TransitionDown(\n",
      "  (linear): Linear(in_features=35, out_features=64, bias=False)\n",
      "  (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=8, bias=True)\n",
      "      (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=8, bias=True)\n",
      "      (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n",
      "[TransitionDown(\n",
      "  (linear): Linear(in_features=67, out_features=128, bias=False)\n",
      "  (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=16, bias=True)\n",
      "      (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=16, bias=True)\n",
      "      (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=16, bias=True)\n",
      "      (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n",
      "[TransitionDown(\n",
      "  (linear): Linear(in_features=131, out_features=256, bias=False)\n",
      "  (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n",
      "[TransitionDown(\n",
      "  (linear): Linear(in_features=259, out_features=512, bias=False)\n",
      "  (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "), PointTransformerBlock(\n",
      "  (linear1): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (transformer2): PointTransformerLayer(\n",
      "    (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_p): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "      (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=3, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear_w): Sequential(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=64, bias=True)\n",
      "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n",
      "p0= torch.Size([1024, 3]) x0= torch.Size([1024, 6]) o0= tensor([ 512, 1024])\n",
      "Enc p= torch.Size([1024, 3]) x= torch.Size([1024, 6]) o= tensor([ 512, 1024])\n",
      "p1= torch.Size([1024, 3]) x1= torch.Size([1024, 32]) o1= tensor([ 512, 1024])\n",
      "Enc p= torch.Size([1024, 3]) x= torch.Size([1024, 32]) o= tensor([ 512, 1024])\n",
      "p2= torch.Size([256, 3]) x2= torch.Size([256, 64]) o2= tensor([128, 256])\n",
      "Enc p= torch.Size([256, 3]) x= torch.Size([256, 64]) o= tensor([128, 256])\n",
      "p3= torch.Size([64, 3]) x3= torch.Size([64, 128]) o3= tensor([32, 64])\n",
      "Enc p= torch.Size([64, 3]) x= torch.Size([64, 128]) o= tensor([32, 64])\n",
      "p4= torch.Size([16, 3]) x4= torch.Size([16, 256]) o4= tensor([ 8, 16])\n",
      "Enc p= torch.Size([16, 3]) x= torch.Size([16, 256]) o= tensor([ 8, 16])\n",
      "p5= torch.Size([4, 3]) x5= torch.Size([4, 512]) o5= tensor([2, 4])\n",
      "****************** Decoder *****************\n",
      "x5= torch.Size([4, 512])\n",
      "x4= torch.Size([16, 256])\n",
      "x3= torch.Size([64, 128])\n",
      "x2= torch.Size([256, 64])\n",
      "x1= torch.Size([1024, 32])\n",
      "x= torch.Size([1024, 13])\n",
      "torch.Size([1024, 13])\n"
     ]
    }
   ],
   "source": [
    "model = pointtransformer_seg_repro(c=C_in+3, k=13)  # if you concat xyz to feats like original code\n",
    "logits = model([p, x, o])  # (N, k)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "175c157c-03c9-4425-8f32-f0f2cf86de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef169fd-110a-4438-b273-0d10166ca368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff382db-401f-4929-a218-c08460300cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee0dde-4d02-4e85-9109-070ac80e3813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfeecd-4941-46d6-9861-e1c10788496d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804f4dc-5b3b-400f-96ea-0799218611c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f88e9e-0a7c-4c33-8ec4-02ca71b32c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bbe79-78c6-4f3d-8acf-5cc16b803b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fec69-87ac-41ca-bbcc-9720e46424c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c0a24-27f6-4b49-ba54-e05bd89e5a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
