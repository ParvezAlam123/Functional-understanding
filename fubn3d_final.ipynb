{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67cc5390-9c17-47f4-83d6-ac6fda00dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from typing import List, Optional, Tuple\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskGeneration,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    "    AutoProcessor,\n",
    "    GenerationConfig,\n",
    "    Owlv2ForObjectDetection,\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73f5de4-91f5-4ad4-9479-80917df5dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import imageio\n",
    "import cv2\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9445e3-3528-4401-ad83-dec2c1df151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/parvez/One_Touch/Scenefun3D\" \n",
    "split = \"train_val_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fee41ed-cce3-457a-a2fb-5d181d5db80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_file_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_wide_intrinsics\"\n",
    "image_file_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_wide\"\n",
    "transform_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/42445198_transform.npy\" \n",
    "pcd_file_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/420673_laser_scan.ply\"  \n",
    "crop_file_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/420673_crop_mask.npy\" \n",
    "\n",
    "intrinsic_file = sorted(os.listdir(intrinsic_file_path))[243]\n",
    "intrinsic_path = os.path.join(intrinsic_file_path, intrinsic_file)\n",
    "image_file = sorted(os.listdir(image_file_path))[243]\n",
    "image_path = os.path.join(image_file_path, image_file)\n",
    "traj_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_poses.traj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e73f48-9de0-400f-a457-8b104f1ddbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_asset_path(traj_path):\n",
    "        \"\"\"\n",
    "        Get the file path for a specified data asset.\n",
    "\n",
    "        Args:\n",
    "            data_asset_identifier (str): A string identifier for the data asset.\n",
    "            visit_id (str or int): The identifier for the visit (scene).\n",
    "            video_id (str or int, optional): The identifier for the video sequence. Required if specified data asset requires a video identifier.\n",
    "\n",
    "        Returns:\n",
    "            (Path): A Path object representing the file path to the specified data asset.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the `data_asset_identifier` is not valid or if `video_id` is required but not provided.\n",
    "        \"\"\"\n",
    "        assert data_asset_identifier in data_asset_to_path, f\"Data asset identifier '{data_asset_identifier}' is not valid\"\n",
    "\n",
    "        data_path = data_asset_to_path[data_asset_identifier]\n",
    "\n",
    "        if (\"<video_id>\" in data_path) and (video_id is None):\n",
    "            assert False, f\"video_id must be specified for the data asset identifier '{data_asset_identifier}'\"\n",
    "\n",
    "        visit_id = str(visit_id)\n",
    "\n",
    "        data_path = (\n",
    "            data_path\n",
    "                .replace(\"<data_dir>\", self.data_root_path)\n",
    "                .replace(\"<visit_id>\", visit_id)\n",
    "        )\n",
    "\n",
    "        if \"<video_id>\" in data_path:\n",
    "            video_id = str(video_id)\n",
    "            data_path = data_path.replace(\"<video_id>\", video_id)\n",
    "\n",
    "        return data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15722eb-e0ef-47bc-b7a8-8a0aced999af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_angle_axis_to_matrix3(angle_axis):\n",
    "    \"\"\"\n",
    "    Converts a rotation from angle-axis representation to a 3x3 rotation matrix.\n",
    "\n",
    "    Args:\n",
    "        angle_axis (numpy.ndarray): A 3-element array representing the rotation in angle-axis form.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): A 3x3 rotation matrix representing the same rotation as the input angle-axis.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a valid 3-element numpy array.\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(angle_axis, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy array.\")\n",
    "    \n",
    "    # Check if the input is of shape (3,)\n",
    "    if angle_axis.shape != (3,):\n",
    "        raise ValueError(\"Input must be a 3-element array representing the rotation in angle-axis representation.\")\n",
    "    \n",
    "    matrix, jacobian = cv2.Rodrigues(angle_axis)\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a524fc8-1ee5-47fa-908e-0b56d642ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrajStringToMatrix(traj_str):\n",
    "        \"\"\" \n",
    "        Converts a line from the camera trajectory file into translation and rotation matrices.\n",
    "\n",
    "        Args:\n",
    "            traj_str (str): A space-delimited string where each line represents a camera pose at a particular timestamp. \n",
    "                            The line consists of seven columns:\n",
    "                - Column 1: timestamp\n",
    "                - Columns 2-4: rotation (axis-angle representation in radians)\n",
    "                - Columns 5-7: translation (in meters)\n",
    "\n",
    "        Returns:\n",
    "            (tuple): A tuple containing:\n",
    "                - ts (str): Timestamp.\n",
    "                - Rt (numpy.ndarray): 4x4 transformation matrix representing rotation and translation.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the input string does not have exactly seven columns.\n",
    "        \"\"\"\n",
    "        tokens = traj_str.split()\n",
    "        assert len(tokens) == 7\n",
    "        ts = tokens[0]\n",
    "\n",
    "        # Rotation in angle axis\n",
    "        angle_axis = [float(tokens[1]), float(tokens[2]), float(tokens[3])]\n",
    "        r_w_to_p = convert_angle_axis_to_matrix3(np.asarray(angle_axis))\n",
    "\n",
    "        # Translation\n",
    "        t_w_to_p = np.asarray([float(tokens[4]), float(tokens[5]), float(tokens[6])])\n",
    "        extrinsics = np.eye(4, 4)\n",
    "        extrinsics[:3, :3] = r_w_to_p\n",
    "        extrinsics[:3, -1] = t_w_to_p\n",
    "        Rt = np.linalg.inv(extrinsics)\n",
    "\n",
    "        return (ts, Rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514eb95c-d740-4739-a22f-31fd76182374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_trajectory(traj_path):\n",
    "        \"\"\"\n",
    "        Retrieve the camera trajectory from a file and convert it into a dictionary whose keys are timestamps and \n",
    "        values are the corresponding camera poses.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str): The identifier of the scene.\n",
    "            video_id (str): The identifier of the video sequence.\n",
    "            pose_source (str, optional): Specifies the trajectory asset type, either \"colmap\" or \"arkit\". Defaults to \"colmap\".\n",
    "\n",
    "        Returns:\n",
    "            (dict): A dictionary where keys are timestamps (rounded to 3 decimal points) and values are 4x4 transformation matrices representing camera poses.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If an unsupported trajectory asset type is provided.\n",
    "        \"\"\"\n",
    "        #assert pose_source in [\"colmap\", \"arkit\"], f\"Unknown option {pose_source}\"\n",
    "\n",
    "        #data_asset_identifier = \"hires_poses\" if pose_source == \"colmap\" else \"lowres_poses\"\n",
    "        traj_file_path = traj_path\n",
    "        #print(\"traj_file_path = \", traj_file_path)\n",
    "\n",
    "        with open(traj_file_path) as f:\n",
    "            traj = f.readlines()\n",
    "\n",
    "        # Convert trajectory to a dictionary\n",
    "        poses_from_traj = {}\n",
    "        for line in traj:\n",
    "            traj_timestamp = line.split(\" \")[0] \n",
    "\n",
    "\n",
    "            poses_from_traj[f\"{traj_timestamp}\"] = np.array(TrajStringToMatrix(line)[1].tolist())\n",
    "            \n",
    "\n",
    "        return poses_from_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c64797-121c-4eb7-a696-02524338af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_intrinsics(intrinsics_path):\n",
    "        \"\"\"\n",
    "        Retrieve the camera intrinsics for a given scene and video sequence.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str): The identifier of the scene.\n",
    "            video_id (str): The identifier of the video sequence.\n",
    "            data_asset_identifier (str, optional): The data asset type for camera intrinsics.\n",
    "                                                   Can be either \"hires_wide_intrinsics\" or \"lowres_wide_intrinsics\". \n",
    "                                                   Defaults to \"hires_wide_intrinsics\".\n",
    "\n",
    "        Returns:\n",
    "            (dict): A dictionary mapping timestamps to file paths of camera intrinsics data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported data asset identifier is provided.\n",
    "            FileNotFoundError: If no intrinsics files are found at the specified path.\n",
    "        \"\"\"\n",
    "        intrinsics_mapping = {}\n",
    "        \n",
    "\n",
    "        intrinsics = sorted(glob.glob(os.path.join(intrinsics_path, \"*.pincam\")))\n",
    "        \n",
    "\n",
    "        intrinsics_timestamps = [os.path.basename(x).split(\".pincam\")[0].split(\"_\")[1] for x in intrinsics]\n",
    "\n",
    "        # Create mapping from timestamp to full path\n",
    "        intrinsics_mapping = {timestamp: cur_intrinsics for timestamp, cur_intrinsics in zip(intrinsics_timestamps, intrinsics)}\n",
    "\n",
    "        return intrinsics_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930145ed-2130-434a-aea8-0c74c1a441dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_camera_intrinsics(intrinsics_file_path, format=\"tuple\"):\n",
    "        \"\"\"\n",
    "        Parses a file containing camera intrinsic parameters and returns them in the specified format.\n",
    "\n",
    "        Args:\n",
    "            intrinsics_file_path (str): The path to the file containing camera intrinsic parameters.\n",
    "            format (str, optional): The format in which to return the camera intrinsic parameters.\n",
    "                                    Supported formats are \"tuple\" and \"matrix\". Defaults to \"tuple\".\n",
    "\n",
    "        Returns:\n",
    "            (Union[tuple, numpy.ndarray]): Camera intrinsic parameters in the specified format.\n",
    "\n",
    "                - If format is \"tuple\", returns a tuple \\\\(w, h, fx, fy, hw, hh\\\\).\n",
    "                - If format is \"matrix\", returns a 3x3 numpy array representing the camera matrix.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported format is specified.\n",
    "        \"\"\"\n",
    "        w, h, fx, fy, hw, hh = np.loadtxt(intrinsics_file_path)\n",
    "\n",
    "        if format == \"tuple\":\n",
    "            return (w, h, fx, fy, hw, hh)\n",
    "        elif format == \"matrix\":\n",
    "            return np.asarray([[fx, 0, hw], [0, fy, hh], [0, 0, 1]])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format {format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b4388a-76a0-4b4a-af50-f3e0636d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_from_traj = get_camera_trajectory(traj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "484dabf7-949a-4bb3-ac82-10743521321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_frame_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_wide/42445198_10904.646.jpg\"\n",
    "depth_frame_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_depth/42445198_10904.646.png\"\n",
    "intrinsics_path = get_camera_intrinsics(intrinsic_file_path)['10904.646']\n",
    "width, height, _, _, _, _ = read_camera_intrinsics(intrinsics_path)\n",
    "time_stamp = \"10904.646\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6b033-48c1-4a40-b4eb-ad0b3cdcab47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542aecf7-1b3d-4685-adb9-09394cdeebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48b68724-c940-4f80-a55d-b238927b7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParser:\n",
    "    \"\"\"\n",
    "    A class for parsing data files in the SceneFun3D dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split):\n",
    "        \"\"\"\n",
    "        Initialize the DataParser instance with the root path.\n",
    "\n",
    "        Args:\n",
    "            data_root_path (str): The root path where data is located.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.data_root_path = os.path.join(root, split)\n",
    "\n",
    "\n",
    "    def get_data_asset_path(self, data_asset_identifier, visit_id, video_id=None):\n",
    "        \"\"\"\n",
    "        Get the file path for a specified data asset.\n",
    "\n",
    "        Args:\n",
    "            data_asset_identifier (str): A string identifier for the data asset.\n",
    "            visit_id (str or int): The identifier for the visit (scene).\n",
    "            video_id (str or int, optional): The identifier for the video sequence. Required if specified data asset requires a video identifier.\n",
    "\n",
    "        Returns:\n",
    "            (Path): A Path object representing the file path to the specified data asset.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the `data_asset_identifier` is not valid or if `video_id` is required but not provided.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            data_asset_identifier in data_asset_to_path\n",
    "        ), f\"Data asset identifier '{data_asset_identifier}' is not valid\"\n",
    "\n",
    "        data_path = data_asset_to_path[data_asset_identifier]\n",
    "\n",
    "        if (\"<video_id>\" in data_path) and (video_id is None):\n",
    "            assert (\n",
    "                False\n",
    "            ), f\"video_id must be specified for the data asset identifier '{data_asset_identifier}'\"\n",
    "\n",
    "        visit_id = str(visit_id)\n",
    "\n",
    "        data_path = data_path.replace(\"<data_dir>\", self.data_root_path).replace(\n",
    "            \"<visit_id>\", visit_id\n",
    "        )\n",
    "\n",
    "        if \"<video_id>\" in data_path:\n",
    "            video_id = str(video_id)\n",
    "            data_path = data_path.replace(\"<video_id>\", video_id)\n",
    "\n",
    "        return data_path\n",
    "    def get_descriptions(self, visit_id):\n",
    "        \"\"\"\n",
    "        Retrieve the natural language task descriptions for a specified scene.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str or int): The identifier for the scene.\n",
    "\n",
    "        Returns:\n",
    "            (list): A list of descriptions, each represented as a dictionary.\n",
    "        \"\"\"\n",
    "        descriptions_path = self.get_data_asset_path(\n",
    "            data_asset_identifier=\"descriptions\", visit_id=visit_id\n",
    "        )\n",
    "\n",
    "        with open(descriptions_path, \"r\") as f:\n",
    "            descriptions_data = json.load(f)[\"descriptions\"]\n",
    "\n",
    "        return descriptions_data\n",
    "\n",
    "\n",
    "    def get_descriptions_list(self, visit_id: str):\n",
    "        \"\"\"\n",
    "        List of descriptions given a visit_id\n",
    "        \"\"\"\n",
    "\n",
    "        descs = self.get_descriptions(visit_id)\n",
    "        desc_ids = {desc[\"desc_id\"]: desc[\"description\"] for desc in descs}\n",
    "        return desc_ids\n",
    "\n",
    "\n",
    "    def get_visits(self) -> list:\n",
    "        \"\"\"\n",
    "        Given a split, returns a dict associating each visit id to the list of video ids\n",
    "        \"\"\"\n",
    "\n",
    "        with open(\n",
    "            os.path.join(f\"{self.root}/benchmark_file_lists/{self.split}_set.csv\")\n",
    "        ) as f:\n",
    "            # skip csv header\n",
    "            visit_video = f.readlines()[1:]\n",
    "\n",
    "        visits = list()\n",
    "        for line in visit_video:\n",
    "            visit_id = line.strip(\"\\n\").split(\",\")[0]\n",
    "            visits.append(visit_id)\n",
    "\n",
    "        return visits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04603655-82e9-42da-8b88-3d5c885a7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_alphanumeric(lst: str):\n",
    "    def sort_key(s):\n",
    "        return [\n",
    "            int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(\"([0-9]+)\", s)\n",
    "        ]\n",
    "\n",
    "    return sorted(lst, key=sort_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2425eae3-ec75-4319-82ec-c80dcf7bc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_PORT = 11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad727727-fbc0-42ec-9ef7-e536ab9aed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_response(statement, query):\n",
    "    client = ollama.Client(host=f\"localhost:{OLLAMA_PORT}\")\n",
    "    response = client.chat(\n",
    "        model=\"llama3.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI System that has to provide json files to a robotic system so that it can interact with our physical world, based on a natural language prompt.\\nIn particular, you have to help the robot in identify which object parts it has to interact with to solve particual tasks.\\n its set of possible actions are [rotate, key_press, tip_push, hook_pull, pinch_pull, hook_turn, foot_push, plug_in, unplug]\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": statement.format(query=query),\n",
    "            },\n",
    "        ],\n",
    "        options={\"temperature\": 0},\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"], response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908c8f07-db30-44a8-9996-ada0c59f5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path, split):\n",
    "\n",
    "    statement = \"\"\"How do I adjust the room's temperature using the radiator thermostat?\n",
    "respond directly with only the json with the following format.\n",
    "{{\n",
    "\"prompt\": a string with the prompt,\n",
    "\"task_solving_sequence\": a list of strings with the description of what I have to do to accomplish the task described by the prompt, subdivided in subtasks,\n",
    "\"acted_on_object\": a string with the name of the object part on which I have to act\n",
    "\"acted_on_object_hierarchy\": a list of object parts from the top level object to the object part,\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    "    parser = DataParser(path, split)\n",
    "   \n",
    "\n",
    "    query = \"Adjust the room's temperature using the radiator thermostat\"\n",
    "    response = get_LLM_response(statement=statement, query=query) \n",
    "\n",
    "    return response[0]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d399c72-08b7-4f7f-b1c5-94d8ed198a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = main(path=path, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b91fbb8e-a46a-4ace-937e-72f04aa88a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8dbb9b-5e27-4c43-a9f2-91c3d046409b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Adjust the room's temperature using the radiator thermostat\",\n",
       " 'task_solving_sequence': ['Locate the radiator thermostat',\n",
       "  'Identify the temperature control knob',\n",
       "  'Rotate the temperature control knob to adjust the temperature'],\n",
       " 'acted_on_object': 'temperature control knob',\n",
       " 'acted_on_object_hierarchy': ['radiator',\n",
       "  'thermostat',\n",
       "  'temperature control knob']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a5eccf-2850-416f-b997-e59004f31643",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_object = response[\"acted_on_object_hierarchy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18547ba2-bbb6-4d0e-9285-b1dba825e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model = SamModel.from_pretrained(\"jadechoghari/robustsam-vit-large\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"jadechoghari/robustsam-vit-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e774f9b6-34fb-482c-9f13-259fb9aa1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f851a4-8d1d-4f1e-9384-0598c80b6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e20d53c9-0c9a-4fb8-9aba-ac4cb7bdfbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "owl_processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "owl_model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ecbaa5-020c-4cbc-bab1-88d42d6d819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = rgb_frame_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8e5b982-108f-4355-afc7-aa027e3164e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "text_labels = [parent_object]\n",
    "inputs = owl_processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = owl_model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d0d7e54-7188-44bb-9e4e-ea29decaf94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05570712-4e61-4d0b-8a2e-2b3514733da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W , _ = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2109c44b-a251-4a71-84a9-00cbfa0a4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(H, W)])\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = owl_processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "366704ac-3b8e-4692-a1ca-06b945ed786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698ed294-dd0a-468a-8eb6-7aca9663c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8117e+03, -3.3676e-01,  1.8618e+03,  2.9077e+01],\n",
       "        [ 8.2128e+02,  6.3764e+02,  8.4435e+02,  6.8429e+02],\n",
       "        [ 1.5998e+00,  3.7068e+02,  8.2636e+02,  9.1892e+02]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e645573f-33ee-4f66-a892-7b4a83904012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1352, 0.1020, 0.8591], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f70dcc5-02ee-4cc5-864d-dc989a9b7896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thermostat', 'temperature control knob', 'radiator']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23b008e3-1114-45ea-8a09-4ad4a4de443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected thermostat with confidence 0.135 at location [1811.7, -0.34, 1861.79, 29.08]\n",
      "Detected temperature control knob with confidence 0.102 at location [821.28, 637.64, 844.35, 684.29]\n",
      "Detected radiator with confidence 0.859 at location [1.6, 370.68, 826.36, 918.92]\n"
     ]
    }
   ],
   "source": [
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    #print(box)\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b072ee-15f8-4389-b8b1-7434a9660d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "901d4c6e-facd-48ca-b0da-4c1f257901ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_polar_coords(shape: Tuple[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Make polar coordinates mask with a fixed size\n",
    "    \"\"\"\n",
    "    h, w = shape\n",
    "    xs, ys = np.meshgrid(np.arange(0, w, 1), np.arange(0, h, 1))\n",
    "    xs = (xs - int(w / 2)) / (w / 2)\n",
    "    ys = (ys - int(h / 2)) / (h / 2)\n",
    "\n",
    "    arctans = np.arctan2(ys, xs)\n",
    "    module = np.sqrt(np.power(xs, 2) + np.power(ys, 2))\n",
    "\n",
    "    return torch.tensor(module), torch.tensor(arctans)\n",
    "\n",
    "\n",
    "M_1920_1440, A_1920_1440 = get_image_polar_coords((1920, 1440))\n",
    "M_1440_1920, A_1440_1920 = get_image_polar_coords((1440, 1920))\n",
    "\n",
    "\n",
    "def get_mask_score(mask: np.ndarray, n_bins: Optional[int] = 30) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute polar coordinate score for the mask\n",
    "    \"\"\"\n",
    "    if mask.shape == (1920, 1440):\n",
    "        module, arctans = M_1920_1440, A_1920_1440\n",
    "    elif mask.shape == (1440, 1920):\n",
    "        module, arctans = M_1440_1920, A_1440_1920\n",
    "    else:\n",
    "        print(\"Must compute for shape \", mask.shape)\n",
    "        module, arctans = get_image_polar_coords(mask.shape)\n",
    "\n",
    "    if torch.count_nonzero(mask) > 0:\n",
    "\n",
    "        m_arctans = arctans[mask == 1]\n",
    "        m_mod = module[mask == 1]\n",
    "\n",
    "        hist_arc, _ = torch.histogram(\n",
    "            m_arctans, bins=n_bins, range=(-torch.pi, torch.pi)\n",
    "        )\n",
    "        hist_mod, bins_mod = torch.histogram(\n",
    "            m_mod, bins=n_bins, range=(0, math.sqrt(2))\n",
    "        )\n",
    "        arc_dist = torch.ones(n_bins)\n",
    "\n",
    "        mod_dist = torch.zeros(n_bins)\n",
    "        max_mod = torch.max(m_mod)\n",
    "        max_bin = torch.min(torch.nonzero(max_mod <= bins_mod)[:, 0])\n",
    "        # get the bin before that, if it isnt the last\n",
    "        mod_dist[0:max_bin] = 1\n",
    "        arc_score = entropy(hist_arc, arc_dist)\n",
    "        mod_score = entropy(hist_mod, mod_dist)\n",
    "    else:\n",
    "        arc_score, mod_score = 0.0, 0.0\n",
    "\n",
    "    return arc_score, mod_score\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6295d011-9748-4fc8-b37a-6d39fa809eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_boxes_list = [] \n",
    "boxes = boxes.detach().numpy()\n",
    "for bb in boxes:\n",
    "    bb_boxes_list.append(list(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afb9c332-7d6c-4fd0-8927-03c09fb3c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(\"cuda\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "\n",
    "input_boxes = [bb_boxes_list] # 2D localization of a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19a40d67-cb82-4868-b929-b9b0e42632b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(image, input_boxes=input_boxes, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e12a2-e89c-4b75-8294-a7c195f46c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dab35a98-8dbb-41b6-84ce-032767b5da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3caa6c1d-c2dd-4966-ae71-d95cbca6b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_obj = masks[0][2].numpy()\n",
    "#C, H, W = mask_obj.shape\n",
    "#mask_obj = np.reshape(mask_obj, (H, W, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d513d3-4b9a-4b48-ad8d-0960fa187d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87d5eb87-c4d4-4c11-89e1-8fe21f831830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9313, 0.9366, 0.9241],\n",
       "         [0.9382, 0.9454, 0.9377],\n",
       "         [0.9749, 0.9910, 0.9064]]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b242c3b-51d6-4981-b35c-b04ce2859cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "185fe3a9-1ed9-401b-9a63-7e43cf1d9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = poses_from_traj[time_stamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1323a3dc-a85c-44b2-8cd9-25cc499dd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_depth_frame(depth_frame_path, conversion_factor=1000):\n",
    "        \"\"\"\n",
    "        Read a depth frame from the specified path and convert it to depth values.\n",
    "\n",
    "        Args:\n",
    "            depth_frame_path (str): The full path to the depth frame file.\n",
    "            conversion_factor (float, optional): The conversion factor to convert pixel values to depth values. Defaults to 1000 to convert millimeters to meters.\n",
    "\n",
    "        Returns:\n",
    "            (numpy.ndarray): The depth frame as a NumPy array with the depth values.\n",
    "        \"\"\"\n",
    "\n",
    "        depth = imageio.v2.imread(depth_frame_path) / conversion_factor\n",
    "\n",
    "        return depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b1d49b7-6eaf-4ba9-9eb3-61958720177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cv2.imread(rgb_frame_path)\n",
    "depth = read_depth_frame(depth_frame_path)\n",
    "intrinsic = read_camera_intrinsics(intrinsics_path, format=\"matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b675b602-4647-4b6f-a7df-609501fb2663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58614e+03, 0.00000e+00, 9.51658e+02],\n",
       "       [0.00000e+00, 1.58614e+03, 7.22847e+02],\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d12b0550-ea7d-43af-a36b-ace2a35b21c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 1920, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a5275f1-2672-47a5-b581-220dfee3125f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 1920)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9448762-949a-4521-9c8c-66677f69994e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.93899058e-01,  6.76734645e-02, -8.70917085e-02,\n",
       "        -6.84857986e-01],\n",
       "       [ 1.05157806e-01,  3.43285310e-01, -9.33325791e-01,\n",
       "        -2.80315536e+00],\n",
       "       [-3.32640856e-02, -9.36789997e-01, -3.48307339e-01,\n",
       "         3.93821489e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75177d82-cdbf-4db1-bad5-935e1a2f2fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74bb0b44-579f-4ad7-9bcf-42882083fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mapping(camera_to_world, coords, depth, intrinsic, image_dim, cut_bound, vis_thres):\n",
    "        \"\"\"\n",
    "        :param camera_to_world: 4 x 4\n",
    "        :param coords: N x 3 format\n",
    "        :param depth: H x W format\n",
    "        :param intrinsic: 3 x 3 format\n",
    "        :return: mapping, N x 3 format, (H, W, mask)\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"coord =\", coords.shape)\n",
    "        coords_new = np.concatenate([coords, np.ones([coords.shape[0], 1])], axis=1).T \n",
    "        #print(\"coord_new = \", coords_new.shape)\n",
    "        assert coords_new.shape[0] == 4, \"[!] Shape error\"\n",
    "\n",
    "        world_to_camera = np.linalg.inv(camera_to_world)\n",
    "        \n",
    "        p = np.matmul(world_to_camera, coords_new) \n",
    "        #print(\"p=\", p[:, 0])\n",
    "        p[0] = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "        p[1] = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "        pi = np.round(p).astype(int) # simply round the projected coordinates\n",
    "        inside_mask = (pi[0] >= cut_bound) * (pi[1] >= cut_bound) \\\n",
    "                    * (pi[0] < image_dim[0]-cut_bound) \\\n",
    "                    * (pi[1] < image_dim[1]-cut_bound)\n",
    "\n",
    "\n",
    "        \n",
    "         \n",
    "        #print(\"inside_mask=\", inside_mask.shape)\n",
    "        if depth is not None:\n",
    "          depth_cur = depth[pi[1][inside_mask], pi[0][inside_mask]]\n",
    "          #print(\"depth_cur=\", depth_cur.shape)\n",
    "          occlusion_mask = np.abs(depth_cur - p[2][inside_mask]) <= vis_thres * depth_cur \n",
    "          #print(\"occlusion_maks=\", occlusion_mask.shape) \n",
    "          inside_mask[inside_mask] = occlusion_mask\n",
    "        else:\n",
    "          front_mask = p[2] > 0 # make sure the depth is in front\n",
    "          inside_mask = front_mask * inside_mask\n",
    "\n",
    "        mapping = np.zeros((3, coords.shape[0]), dtype=int)\n",
    "        mapping[0][inside_mask] = pi[1][inside_mask]\n",
    "        mapping[1][inside_mask] = pi[0][inside_mask]\n",
    "        mapping[2][inside_mask] = 1\n",
    "\n",
    "\n",
    "        return mapping.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e8c7f-3f66-4b7a-8f59-f25eb12673e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca829b6d-a57a-49a1-bc02-c593b27ac281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pcd file \n",
    "pcd = o3d.io.read_point_cloud(pcd_file_path) \n",
    "points = np.asarray(pcd.points)\n",
    "colors = np.asarray(pcd.colors) \n",
    "\n",
    "# read the crop index file to crop the noisy points \n",
    "crop_index = np.load(crop_file_path) \n",
    "\n",
    "point_positions = points[crop_index]\n",
    "points_colors = colors[crop_index]\n",
    "num_points = point_positions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e91b5244-35c9-468e-9ffe-eb38763e7a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010016, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfb75d-9d4b-4dda-bd8a-18bdd9368456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7530eb8b-a3ac-4d9a-8763-4b3610a5c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 3D-2D mapping based on the depth\n",
    "mapping = np.ones([num_points, 4], dtype=int) \n",
    "mapping[:, 1:4] = compute_mapping(camera_to_world=pose, coords=point_positions, depth=depth, \n",
    "                                  intrinsic=intrinsic, image_dim=(width,height), cut_bound=5, vis_thres=0.25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bf13200-834e-4054-a9c2-30610c7c9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mask = mapping[:, 3]\n",
    "valid_point_mask = points_mask != 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af44da36-a925-49b5-b87e-56d3543f8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_mapping(\n",
    "        camera_to_world, coords, mask, depth, intrinsic, image_dim, cut_bound, vis_thres, device\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param camera_to_world: 4 x 4\n",
    "        :param coords: N x 3 format\n",
    "        :param depth: H x W format\n",
    "        :param intrinsic: 3x3 format\n",
    "        :return: mapping, N x 3 format, (H,W,mask)\n",
    "        \"\"\"\n",
    "        depth = torch.tensor(depth).to(device)\n",
    "        mask = torch.tensor(mask).to(device)\n",
    "        intrinsic = torch.tensor(intrinsic).to(device)\n",
    "        coords = torch.tensor(coords).to(device)\n",
    "        mapping = torch.zeros((3, coords.shape[0]), dtype=torch.int).to(device)\n",
    "        coords_new = torch.cat(\n",
    "            [coords, torch.ones([coords.shape[0], 1]).to(device)], dim=1\n",
    "        ).transpose(1, 0)\n",
    "        assert coords_new.shape[0] == 4, \"[!] Shape error\"\n",
    "\n",
    "        world_to_camera = torch.tensor(np.linalg.inv(camera_to_world)).to(device)\n",
    "        p = torch.matmul(world_to_camera, coords_new)\n",
    "        p[0] = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "        p[1] = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "        pi = torch.round(p).to(torch.int)  # simply round the projected coordinates\n",
    "        inside_mask = (\n",
    "            (pi[0] >= cut_bound)\n",
    "            * (pi[1] >= cut_bound)\n",
    "            * (pi[0] < image_dim[0] - cut_bound)\n",
    "            * (pi[1] < image_dim[1] - cut_bound)\n",
    "        )\n",
    "\n",
    "        _depth = depth.clone() \n",
    "        _depth = torch.where(mask != 1, 0, _depth)\n",
    "\n",
    "        # Suppose depth is your 2D NumPy array\n",
    "        #plt.figure(figsize=(8, 6))\n",
    "        #plt.imshow(_depth.cpu().numpy(), cmap='plasma')  # 'plasma', 'viridis', or 'gray' are good for depth\n",
    "        #plt.colorbar(label=\"Depth Value\")\n",
    "        #plt.title(\"Depth Image\")\n",
    "        #plt.show()\n",
    "    \n",
    "        depth_cur = _depth[pi[1][inside_mask], pi[0][inside_mask]]\n",
    "        occlusion_mask = (\n",
    "            torch.abs(depth[pi[1][inside_mask], pi[0][inside_mask]] - p[2][inside_mask])\n",
    "            <= vis_thres * depth_cur\n",
    "        )\n",
    "\n",
    "        inside_mask[inside_mask == True] = occlusion_mask\n",
    "\n",
    "        mapping[0][inside_mask] = pi[1][inside_mask]\n",
    "        mapping[1][inside_mask] = pi[0][inside_mask]\n",
    "        mapping[2][inside_mask] = 1\n",
    "\n",
    "        return mapping.transpose(1, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71c81668-b602-43d5-bf87-a4ed0960eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_obj = masks[0][1][0].numpy()\n",
    "mask = mask_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "878b3b6a-81cf-498b-82c2-15f616b9fdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppose depth is your 2D NumPy array\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(mask, cmap='plasma')  # 'plasma', 'viridis', or 'gray' are good for depth\n",
    "plt.colorbar(label=\"Depth Value\")\n",
    "plt.title(\"Depth Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b818f4b-a99b-44e4-8b6d-ffc82e0d19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 3D-2D mapping based on the depth\n",
    "mapping_seg = np.ones([num_points, 4], dtype=int) \n",
    "mapping_seg[:, 1:4] = compute_masked_mapping(camera_to_world=pose, coords=point_positions, mask=mask,  depth=depth, \n",
    "                                  intrinsic=intrinsic, image_dim=(width,height), cut_bound=5, vis_thres=0.25, device=device) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15cc467e-695b-47f5-9ea4-64cc99b2b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mask_seg = mapping_seg[:, 3]\n",
    "valid_point_mask_seg = points_mask_seg != 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc61a9-b34e-439c-bf37-cd5329e36f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3bb97a7-c1f0-41bc-bba9-c84ced58e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 1920)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "020f0b16-f4d6-428d-94b6-5a923ffe178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_point_mask = np.logical_and(valid_point_mask, valid_point_mask_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94392449-b641-4fc5-adbb-0fceba0bd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_colors[valid_point_mask_seg] = [0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90e1ff80-dd4e-42dc-adbc-d37be96c8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_point_mask_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c77d7bb-980e-4c06-b9a5-e2cbe274541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_positions)\n",
    "pcd.colors = o3d.utility.Vector3dVector(points_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebe8fe-1262-4739-b1eb-99f02f3cccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pcd],\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549a53b-faa0-4789-afef-71fe48212797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b979f78-8b38-4319-a884-028717d5ed6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "21769e52-81fd-4c99-bb9a-e6fc839f7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c8df5d97-f23e-4555-a3d2-63458f965e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    print(color.reshape(1, 1, -1).shape, mask.reshape(h, w, 1).shape) \n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1) \n",
    "    print(mask[np.nonzero(mask)[0]])\n",
    "    plt.imshow(mask_image)\n",
    "    \n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c90e89-e6f6-4ce0-8a86-9eeffb9b4e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "725e660c-3207-4575-b48d-e1998028c06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(image))\n",
    "ax = plt.gca()\n",
    "for i in range(1):\n",
    "    #print(mask.shape)\n",
    "    mask = masks[0][2][0]\n",
    "    plt.imshow(mask)\n",
    "    \n",
    "    ax.imshow(mask)\n",
    "    #show_mask(mask, ax=ax, random_color=False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b78b346-4d08-435a-b3e3-a83dce695f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1440, 1920])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cefa624a-6b45-4574-a7be-192e8aeda2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_scores, mod_scores = get_mask_score(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20514f5c-2554-4d49-b1f4-773f421e1880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.233478841304535"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d91c0474-b315-40a8-afe8-f5613136bc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.36561552161389"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7a08c2b-92b2-4ccd-9c0a-80be7073245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc958a4d-779a-4c9b-964d-4394f802b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1440, 1920])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "aadd9132-9354-4b4a-baa9-389f8dc6b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1440, 1920)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0][2].numpy().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5571d5-7e62-4906-88c8-cecdceb61aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d11fcb0a-97e8-4493-80cf-8f29cbeb3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_angle_axis_to_matrix3(angle_axis):\n",
    "    \"\"\"\n",
    "    Converts a rotation from angle-axis representation to a 3x3 rotation matrix.\n",
    "\n",
    "    Args:\n",
    "        angle_axis (numpy.ndarray): A 3-element array representing the rotation in angle-axis form.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): A 3x3 rotation matrix representing the same rotation as the input angle-axis.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a valid 3-element numpy array.\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(angle_axis, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy array.\")\n",
    "    \n",
    "    # Check if the input is of shape (3,)\n",
    "    if angle_axis.shape != (3,):\n",
    "        raise ValueError(\"Input must be a 3-element array representing the rotation in angle-axis representation.\")\n",
    "    \n",
    "    matrix, jacobian = cv2.Rodrigues(angle_axis)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81ee516c-24cf-42fc-880b-11271dfdc7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c13b1b1a-a606-4262-b9f2-0125e948b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arkit_poses(traj_file):\n",
    "    poses = []\n",
    "    with open(traj_file, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) != 7:\n",
    "                continue  # skip malformed lines\n",
    "            ts = tokens[0]\n",
    "\n",
    "            # Rotation in angle axis\n",
    "            angle_axis = [float(tokens[1]), float(tokens[2]), float(tokens[3])]\n",
    "            r_w_to_p = convert_angle_axis_to_matrix3(np.asarray(angle_axis))\n",
    "\n",
    "            # Translation\n",
    "            t_w_to_p = np.asarray([float(tokens[4]), float(tokens[5]), float(tokens[6])])\n",
    "            extrinsics = np.eye(4, 4)\n",
    "            extrinsics[:3, :3] = r_w_to_p\n",
    "            extrinsics[:3, -1] = t_w_to_p\n",
    "            Rt = extrinsics\n",
    "\n",
    "            poses.append((ts, Rt))\n",
    "    return poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d16b5c5e-6915-4194-b501-30c9e596a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "poses = load_arkit_poses(traj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6303cba3-c14d-4028-a6af-318df07172d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = poses[243][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3bd5601-f516-4812-b229-b56443bb417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9d36c54-d898-40a2-b399-4e4b52968578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_pincam_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        line = f.readline()\n",
    "        parts = list(map(float, line.strip().split()))\n",
    "        assert len(parts) == 6, \"Expected 6 values: WIDTH HEIGHT fx fy cx cy\"\n",
    "\n",
    "        width, height, fx, fy, cx, cy = parts\n",
    "        K = np.array([[fx, 0, cx],\n",
    "                      [0, fy, cy],\n",
    "                      [0,  0,  1]])\n",
    "\n",
    "        image_size = (int(height), int(width))  # (H, W)\n",
    "\n",
    "        return K, image_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30b63c5d-3877-4d06-8f44-671a7d0fda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K, image_size = read_pincam_file(intrinsic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "616c5348-07e2-44f9-aaef-236c436281d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05ab7ed3-1f77-4f94-9ee2-e0d7c196eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_lidar_to_cam = np.load(transform_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24f80152-d841-48ed-88db-51b88752b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_lidar_to_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d43f0143-d36d-4700-ad5d-11a58b23ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pcd file \n",
    "pcd = o3d.io.read_point_cloud(pcd_file_path) \n",
    "points = np.asarray(pcd.points)\n",
    "colors = np.asarray(pcd.colors) \n",
    "\n",
    "# read the crop index file to crop the noisy points \n",
    "crop_index = np.load(crop_file_path) \n",
    "\n",
    "points = points[crop_index]\n",
    "colors = colors[crop_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e64f1959-bc03-4840-bb74-0a3d543a8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points_to_mask(points_lidar,colors, T_lidar_to_cam, K,pose, mask):\n",
    "    # Convert points to homogeneous\n",
    "    points_h = np.hstack([points_lidar, np.ones((points_lidar.shape[0], 1))])\n",
    "    #print(points_h.shape)\n",
    "    \n",
    "    # Transform to camera frame\n",
    "    points_cam_cord = (T_lidar_to_cam @ points_h.T).T[:, :3]\n",
    "    #points_cam = points_lidar\n",
    "    #print(points_cam.shape) \n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(points_cam_cord)\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(colors )\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "                                  \n",
    "    # Keep points in front of camera\n",
    "    valid = points_cam_cord[:, 2] > 0\n",
    "    points_cam = points_cam_cord[valid] \n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(points_cam)\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(colors[valid] )\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "                                  \n",
    "    #print(points_cam.shape)\n",
    "    points_lidar_cam = points_lidar[valid] \n",
    "    colors_cam = colors[valid] \n",
    "\n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(points_lidar_cam)\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(colors_cam )\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "    \n",
    "\n",
    "    outside_cam_points = points_lidar[~valid] \n",
    "    outside_cam_colors = colors[~valid] \n",
    "\n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(outside_cam_points )\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(outside_cam_colors)\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #print(points_lidar.shape)\n",
    "    #print(T_lidar_to_cam.shape)\n",
    "    points_cam =  np.hstack([points_cam, np.ones((points_cam.shape[0], 1))]) \n",
    "    points_after_pose = pose @ points_cam.T\n",
    "    \n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(points_after_pose.T[:, 0:3])\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(colors_cam)\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "    \n",
    "    \n",
    "    # Project to image plane\n",
    "    pixels = (K@ points_after_pose[0:3, :]).T \n",
    "    \n",
    "    #print(pixels.shape)\n",
    "    pixels /= pixels[:, 2:3]  # Normalize by z\n",
    "    u = pixels[:, 0].astype(int)\n",
    "    v = pixels[:, 1].astype(int)\n",
    "    #print(mask.shape)\n",
    "    H, W = mask.shape \n",
    "    inside = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n",
    "    outside_mask_points = points_lidar_cam[~inside] \n",
    "    outside_mask_colors = colors_cam[~inside] \n",
    "\n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(outside_mask_points)\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(outside_mask_colors)\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "    \n",
    "    \n",
    "    u, v = u[inside], v[inside]\n",
    "    points_lidar_mask = points_lidar_cam[inside] \n",
    "    color_mask = colors_cam[inside] \n",
    "    #n, _ = color_mask.shape\n",
    "    #color_mask = np.zeros((n, 3))\n",
    "    #print(points_lidar.shape)\n",
    "    #pcd = o3d.geometry.PointCloud()\n",
    "    #pcd.points = o3d.utility.Vector3dVector(points_lidar_mask)\n",
    "    #pcd.colors = o3d.utility.Vector3dVector(color_mask)\n",
    "    #o3d.visualization.draw_geometries([pcd],)\n",
    "        \n",
    "    return outside_cam_points, outside_mask_points, points_lidar_mask , outside_cam_colors, outside_mask_colors, color_mask\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6db90e8-eb45-434f-a3e3-c1bbc0b2acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_cam_points, outside_mask_points, points_lidar_mask , outside_cam_colors, outside_mask_colors, color_mask = project_points_to_mask(points,colors, T_lidar_to_cam, K, pose,  mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aeb4d0de-161a-4e38-95fd-ce3a9f63efc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010016, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3349852-963f-4a1a-8133-a85b764dc1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010016, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "908fe142-d55a-457d-bb04-5a27a9049dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_points = np.vstack((outside_cam_points, outside_mask_points, points_lidar_mask))\n",
    "new_colors = np.vstack((outside_cam_colors, outside_mask_colors, color_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a8a0e4d-6188-4259-bcad-95c359f39bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010016, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82225653-748c-41df-b590-6f5f77aa83a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010016, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d735092-d24a-432d-a69b-2649c3398572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.26001918e-01,  3.36170167e-01,  3.94974915e+02],\n",
       "       [-7.30261028e-01,  3.31226021e-01,  3.94589935e+02],\n",
       "       [-7.33365238e-01,  3.30598325e-01,  3.94799408e+02],\n",
       "       ...,\n",
       "       [-2.75541246e-02, -6.08261061e+00,  3.94089539e+02],\n",
       "       [ 2.49890423e+00,  7.95314372e-01,  3.95368774e+02],\n",
       "       [ 2.92300129e+00,  1.32856464e+00,  3.95362732e+02]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " points_lidar_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e768b723-9d7c-4a86-ae12-0b436ff0d4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59607843, 0.5254902 , 0.48627451],\n",
       "       [0.65098039, 0.57647059, 0.57647059],\n",
       "       [0.59607843, 0.50588235, 0.53333333],\n",
       "       ...,\n",
       "       [0.30980392, 0.30980392, 0.30980392],\n",
       "       [0.85490196, 0.85490196, 0.85490196],\n",
       "       [0.76470588, 0.76470588, 0.76470588]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e26cf2c9-2669-41f1-a850-5867bc8d268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(new_points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(new_colors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5499d53-3cf9-44d9-9e55-6f81febafdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pcd],\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1252551e-397e-4caa-b178-bb6262811652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aafc0483-65c4-4fe0-85dd-0b5bf4c37cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "778459ca-db56-4b8f-8680-2924a7e3ab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c459e-5003-4618-888c-d1e3b1a77f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
