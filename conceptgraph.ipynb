{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f2136a-0d18-48c5-b851-d48a912afa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import os \n",
    "import cv2\n",
    "import ollama\n",
    "import re\n",
    "import json\n",
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "import torch \n",
    "import torch.nn as  nn \n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskGeneration,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    "    AutoProcessor,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from transformers import SamModel, SamProcessor\n",
    "from scipy.stats import entropy\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    SKLEARN_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386d0b8-4f2d-4e1f-8e84-3a381ad39e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f06131-e7a9-46d3-9b50-fbbacdea37e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fc9d9b-eef0-4334-9372-3abd661bd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Object Node\n",
    "# -------------------------\n",
    "class ObjectNode:\n",
    "    def __init__(self, obj_id, points, feature, mask, object_name, frame_path=None, seen=1, caption=None, bbox=None, extra=None, parent=None):\n",
    "        self.id = obj_id\n",
    "        self.points = points\n",
    "        self.feature = feature\n",
    "        self.seen = seen\n",
    "        self.caption = caption                # define whether the node is parent or child \n",
    "        self.parent = parent                  # define the parent node name \n",
    "        self.mask = mask                      # mask of the object on the image\n",
    "        self.name = object_name               # name of the node\n",
    "        self.frame_path = frame_path          # this is the image path containing these objects for uniqueness \n",
    "        self.bbox = bbox\n",
    "        self.extra = extra if extra is not None else {}\n",
    "        self.update_bbox()\n",
    "\n",
    "    def update_bbox(self):\n",
    "        if self.points.size == 0:\n",
    "            self.bbox = None\n",
    "            return\n",
    "        mn = self.points.min(axis=0)\n",
    "        mx = self.points.max(axis=0)\n",
    "        self.bbox = (mn, mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443b56bf-36cc-4824-98fd-bba79dc9a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Scene Graph\n",
    "# -------------------------\n",
    "class SceneGraph:\n",
    "    def __init__(self):\n",
    "        self.objects = {}\n",
    "        self.edges = []\n",
    "\n",
    "    def add_object(self, node):\n",
    "        self.objects[node.id] = node\n",
    "\n",
    "    def add_edge(self, a, b, relation):\n",
    "        self.edges.append((a, b, relation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf2f8c5-0de1-49cd-94a9-a50080fef216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Camera helpers\n",
    "# -------------------------\n",
    "def backproject_mask_to_points(depth, mask, K, max_depth=10.0):\n",
    "    assert depth.ndim == 2 and mask.shape == depth.shape, \"mask/depth shape mismatch\"\n",
    "    fy, fx, cx, cy = K[1, 1], K[0, 0], K[0, 2], K[1, 2]\n",
    "    ys, xs = np.where(mask)\n",
    "    zs = depth[ys, xs]\n",
    "    valid = np.isfinite(zs) & (zs > 0) & (zs < max_depth)\n",
    "    xs, ys, zs = xs[valid], ys[valid], zs[valid]\n",
    "    X = (xs - cx) * zs / fx\n",
    "    Y = (ys - cy) * zs / fy\n",
    "    pts = np.stack([X, Y, zs], axis=1)\n",
    "    return pts\n",
    "\n",
    "def transform_points(points_cam, T_cam2world):\n",
    "    if points_cam.size == 0:\n",
    "        return points_cam\n",
    "    pts_h = np.hstack([points_cam, np.ones((points_cam.shape[0], 1))])\n",
    "    pts_w = (T_cam2world @ pts_h.T).T[:, :3]\n",
    "    return pts_w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c786186a-3af7-450c-9996-e101047c83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Denoising & Downsampling\n",
    "# -------------------------\n",
    "def dbscan_largest_cluster(points, eps=0.03, min_samples=20):\n",
    "    if points.size == 0:\n",
    "        return points\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(points)\n",
    "        labels = db.labels_\n",
    "        if len(labels) == 0 or np.all(labels == -1):\n",
    "            return points\n",
    "        unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "        if unique.size == 0:\n",
    "            return points\n",
    "        largest = unique[np.argmax(counts)]\n",
    "        return points[labels == largest]\n",
    "    return points\n",
    "\n",
    "def voxel_downsample(points, voxel_size=0.02):\n",
    "    if points.size == 0:\n",
    "        return points\n",
    "    vox = np.floor(points / voxel_size)\n",
    "    _, idx = np.unique(vox, axis=0, return_index=True)\n",
    "    return points[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3feac6d-60eb-4488-b67f-8090a4171a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Features\n",
    "# -------------------------\n",
    "def l2_normalize(x, eps=1e-8):\n",
    "    n = np.linalg.norm(x) + eps\n",
    "    return x / n\n",
    "\n",
    "def fallback_feature(rgb, mask, pts_world):\n",
    "    sel = rgb[mask]\n",
    "    if sel.size == 0:\n",
    "        color_feat = np.zeros(6, dtype=np.float32)\n",
    "    else:\n",
    "        mean = sel.reshape(-1, 3).mean(axis=0)\n",
    "        std = sel.reshape(-1, 3).std(axis=0)\n",
    "        color_feat = np.hstack([mean, std]).astype(np.float32)\n",
    "    if pts_world.shape[0] >= 3:\n",
    "        mu = pts_world.mean(axis=0, keepdims=True)\n",
    "        C = np.cov((pts_world - mu).T)\n",
    "        vals = np.sqrt(np.clip(np.linalg.eigvalsh(C), 0, None))\n",
    "    else:\n",
    "        vals = np.zeros(3, dtype=np.float32)\n",
    "    size = np.array([float(pts_world.shape[0])], dtype=np.float32)\n",
    "    feat = np.hstack([color_feat, vals, size]).astype(np.float32)\n",
    "    return l2_normalize(feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c30e87c-70df-403a-9b2e-5890ced52e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Similarity\n",
    "# -------------------------\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n",
    "\n",
    "def nnratio(new_pts, obj_pts, radius=0.05):\n",
    "    if new_pts.size == 0 or obj_pts.size == 0:\n",
    "        return 0.0\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        nbrs = NearestNeighbors(radius=radius, algorithm='kd_tree').fit(obj_pts)\n",
    "        ind = nbrs.radius_neighbors(new_pts, radius=radius, return_distance=False)\n",
    "        hits = sum(len(ix) > 0 for ix in ind)\n",
    "        return hits / len(new_pts)\n",
    "    diffs = new_pts[:, None, :] - obj_pts[None, :, :]\n",
    "    d2 = np.sum(diffs * diffs, axis=2)\n",
    "    hits = np.any(d2 <= radius * radius, axis=1).sum()\n",
    "    return float(hits) / float(new_pts.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4b699c-1c6a-4c98-8952-a7c40c9fb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Assoc Params & Greedy\n",
    "# -------------------------\n",
    "class AssocParams:\n",
    "    def __init__(self, delta_nn=0.05, delta_sim=0.7, w_sem=1.0, w_geo=1.0):\n",
    "        self.delta_nn = delta_nn\n",
    "        self.delta_sim = delta_sim\n",
    "        self.w_sem = w_sem\n",
    "        self.w_geo = w_geo\n",
    "\n",
    "class GreedyAssociator:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def match(self, detections, graph): \n",
    "        #print(\"graphs = \", graph.objects, graph.edges)\n",
    "        results = []\n",
    "        for i, (pts_i, feat_i, meta_i, parent, object_name, frame_path) in enumerate(detections): \n",
    "            best_obj, best_score = None, -1e9\n",
    "            for obj in graph.objects.values(): \n",
    "                #print(\"obj = \", obj.points)\n",
    "                geo = nnratio(pts_i, obj.points, radius=self.params.delta_nn)\n",
    "                sem = cosine_sim(feat_i, obj.feature)\n",
    "                sem_n = 0.5 * (sem + 1.0)\n",
    "                score = self.params.w_geo * geo + self.params.w_sem * sem_n\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_obj = obj.id\n",
    "            if best_score >= self.params.delta_sim:\n",
    "                results.append((i, best_obj, meta_i, parent, object_name, frame_path))\n",
    "            else:\n",
    "                results.append((i, None, meta_i, parent, object_name, frame_path))\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41297535-5488-4f93-a6d4-1cf52318f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Fusion & Caption hooks\n",
    "# -------------------------\n",
    "def fuse_object(existing, new_points, new_feature, voxel_size=0.02):\n",
    "    fused_feat = (existing.seen * existing.feature + new_feature) / (existing.seen + 1)\n",
    "    fused_feat = l2_normalize(fused_feat)\n",
    "    fused_pts = np.vstack([existing.points, new_points])\n",
    "    fused_pts = voxel_downsample(fused_pts, voxel_size=voxel_size)\n",
    "    existing.points = fused_pts\n",
    "    existing.feature = fused_feat\n",
    "    existing.seen += 1\n",
    "    existing.update_bbox()\n",
    "    return existing\n",
    "\n",
    "def caption_with_lvml(rgb, mask):\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3328e050-14d2-49e7-82dc-f31fb00277e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Relation inference\n",
    "# -------------------------\n",
    "def infer_relations_spatial(graph, dist_thresh=0.2, height_eps=0.05):\n",
    "    ids = list(graph.objects.keys())\n",
    "    centers, bboxes = {}, {}\n",
    "    for oid in ids:\n",
    "        obj = graph.objects[oid]\n",
    "        if obj.bbox is None:\n",
    "            obj.update_bbox()\n",
    "        bboxes[oid] = obj.bbox\n",
    "        mn, mx = obj.bbox if obj.bbox is not None else (np.zeros(3), np.zeros(3))\n",
    "        centers[oid] = 0.5 * (mn + mx)\n",
    "    graph.edges = []\n",
    "    for i in range(len(ids)):\n",
    "        for j in range(i + 1, len(ids)):\n",
    "            a, b = ids[i], ids[j] \n",
    "            name_a, name_b = graph.objects[a].name, graph.objects[b].name \n",
    "            parent_a, parent_b = graph.objects[a].parent, graph.objects[b].parent \n",
    "\n",
    "            if parent_a == None and parent_b == None:\n",
    "                graph.add_edge(a, b, \"No Relation\")\n",
    "            else:\n",
    "                if parent_a != None and parent_b == None and parent_a == name_b:\n",
    "                    graph.add_edge(a, b, \"a is child of b\")\n",
    "                if parent_b != None and parent_a == None and parent_b == name_a:\n",
    "                    graph_add_edge(a, b, \"b is child of a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1448b78-7002-49b2-8a70-d57249707609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Mapper Config & Mapper\n",
    "# -------------------------\n",
    "class MapperConfig:\n",
    "    def __init__(self, max_depth_m=10.0, dbscan_eps_m=0.03, dbscan_min_samples=20,\n",
    "                 voxel_size_m=0.02, assoc=None, use_clip_dino=False):\n",
    "        self.max_depth_m = max_depth_m\n",
    "        self.dbscan_eps_m = dbscan_eps_m\n",
    "        self.dbscan_min_samples = dbscan_min_samples\n",
    "        self.voxel_size_m = voxel_size_m\n",
    "        self.assoc = assoc if assoc else AssocParams()\n",
    "        self.use_clip_dino = use_clip_dino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f164e1-12a0-4914-8b65-d654fa198f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptGraphsMapper:\n",
    "    def __init__(self, K, cfg=None):\n",
    "        self.graph = SceneGraph()\n",
    "        self.cfg = cfg if cfg else MapperConfig()\n",
    "        self.K = K.astype(np.float32)\n",
    "        self._next_id = 0\n",
    "        self._associator = GreedyAssociator(self.cfg.assoc)\n",
    "\n",
    "    def _new_id(self):\n",
    "        nid = self._next_id\n",
    "        self._next_id += 1\n",
    "        return nid\n",
    "\n",
    "    def process_frame(self, rgb, depth, T_cam2world, masks, parent_list, object_name, frame_path, caption):\n",
    "        detections = []\n",
    "        for i  in range(len(masks)):\n",
    "            m = masks[i]\n",
    "            pts_cam = backproject_mask_to_points(depth, m, self.K, self.cfg.max_depth_m)\n",
    "            if pts_cam.size == 0:\n",
    "                continue\n",
    "            pts_cam = dbscan_largest_cluster(pts_cam, eps=self.cfg.dbscan_eps_m,\n",
    "                                             min_samples=self.cfg.dbscan_min_samples)\n",
    "            pts_w = transform_points(pts_cam, T_cam2world)\n",
    "            pts_w = voxel_downsample(pts_w, voxel_size=self.cfg.voxel_size_m)\n",
    "            feat = fallback_feature(rgb, m, pts_w)\n",
    "            detections.append((pts_w, feat, {\"mask\": m}, parent_list[i], object_name[i], frame_path))\n",
    "\n",
    "        #print(\"graph=\", self.graph.objects, self.graph.edges)\n",
    "        #print(\"associator=\",self._associator.params.delta_nn)\n",
    "        matches = self._associator.match(detections, self.graph)\n",
    "        for (det_idx, obj_id, meta, parent, object_name, frame_path) in matches:\n",
    "            pts_w, feat, _, _, _, _ = detections[det_idx]\n",
    "            if obj_id is None:\n",
    "                nid = self._new_id()\n",
    "                node = ObjectNode(nid, pts_w, feat, meta, object_name)\n",
    "                cap = caption\n",
    "                node.caption = cap\n",
    "                node.parent = parent\n",
    "                node.frame_path = frame_path\n",
    "                self.graph.add_object(node)\n",
    "                #print(\" first frame graph=\", self.graph.objects, self.graph.edges)\n",
    "            else:\n",
    "                node = self.graph.objects[obj_id]\n",
    "                node.frame_path = frame_path\n",
    "                fuse_object(node, pts_w, feat, voxel_size=self.cfg.voxel_size_m) \n",
    "                #print(\" After first frame graph=\", self.graph.objects, self.graph.edges)\n",
    "\n",
    "        infer_relations_spatial(self.graph) \n",
    "        #print(\" Relation =\", self.graph.edges)\n",
    "        return self.graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92618c34-752d-495d-a1d9-f2403c127578",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_wide\" \n",
    "depth_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_depth\" \n",
    "traj_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_poses.traj\"\n",
    "intrinsic_files_path = \"/media/parvez/One_Touch/Scenefun3D/train_val_set/420673/42445198/hires_wide_intrinsics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e2036e-1c72-461d-bc52-bb63f6510955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_angle_axis_to_matrix3(angle_axis):\n",
    "    \"\"\"\n",
    "    Converts a rotation from angle-axis representation to a 3x3 rotation matrix.\n",
    "\n",
    "    Args:\n",
    "        angle_axis (numpy.ndarray): A 3-element array representing the rotation in angle-axis form.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): A 3x3 rotation matrix representing the same rotation as the input angle-axis.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a valid 3-element numpy array.\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(angle_axis, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy array.\")\n",
    "    \n",
    "    # Check if the input is of shape (3,)\n",
    "    if angle_axis.shape != (3,):\n",
    "        raise ValueError(\"Input must be a 3-element array representing the rotation in angle-axis representation.\")\n",
    "    \n",
    "    matrix, jacobian = cv2.Rodrigues(angle_axis)\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4b8004-2e8a-4839-9f57-c5bd66ab3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrajStringToMatrix(traj_str):\n",
    "        \"\"\" \n",
    "        Converts a line from the camera trajectory file into translation and rotation matrices.\n",
    "\n",
    "        Args:\n",
    "            traj_str (str): A space-delimited string where each line represents a camera pose at a particular timestamp. \n",
    "                            The line consists of seven columns:\n",
    "                - Column 1: timestamp\n",
    "                - Columns 2-4: rotation (axis-angle representation in radians)\n",
    "                - Columns 5-7: translation (in meters)\n",
    "\n",
    "        Returns:\n",
    "            (tuple): A tuple containing:\n",
    "                - ts (str): Timestamp.\n",
    "                - Rt (numpy.ndarray): 4x4 transformation matrix representing rotation and translation.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the input string does not have exactly seven columns.\n",
    "        \"\"\"\n",
    "        tokens = traj_str.split()\n",
    "        assert len(tokens) == 7\n",
    "        ts = tokens[0]\n",
    "\n",
    "        # Rotation in angle axis\n",
    "        angle_axis = [float(tokens[1]), float(tokens[2]), float(tokens[3])]\n",
    "        r_w_to_p = convert_angle_axis_to_matrix3(np.asarray(angle_axis))\n",
    "\n",
    "        # Translation\n",
    "        t_w_to_p = np.asarray([float(tokens[4]), float(tokens[5]), float(tokens[6])])\n",
    "        extrinsics = np.eye(4, 4)\n",
    "        extrinsics[:3, :3] = r_w_to_p\n",
    "        extrinsics[:3, -1] = t_w_to_p\n",
    "        Rt = np.linalg.inv(extrinsics)\n",
    "\n",
    "        return (ts, Rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ff8d5f-f5d5-4eb6-9f41-b36b9362ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_trajectory(traj_path):\n",
    "        \"\"\"\n",
    "        Retrieve the camera trajectory from a file and convert it into a dictionary whose keys are timestamps and \n",
    "        values are the corresponding camera poses.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str): The identifier of the scene.\n",
    "            video_id (str): The identifier of the video sequence.\n",
    "            pose_source (str, optional): Specifies the trajectory asset type, either \"colmap\" or \"arkit\". Defaults to \"colmap\".\n",
    "\n",
    "        Returns:\n",
    "            (dict): A dictionary where keys are timestamps (rounded to 3 decimal points) and values are 4x4 transformation matrices representing camera poses.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If an unsupported trajectory asset type is provided.\n",
    "        \"\"\"\n",
    "        #assert pose_source in [\"colmap\", \"arkit\"], f\"Unknown option {pose_source}\"\n",
    "\n",
    "        #data_asset_identifier = \"hires_poses\" if pose_source == \"colmap\" else \"lowres_poses\"\n",
    "        traj_file_path = traj_path\n",
    "        #print(\"traj_file_path = \", traj_file_path)\n",
    "\n",
    "        with open(traj_file_path) as f:\n",
    "            traj = f.readlines()\n",
    "\n",
    "        # Convert trajectory to a dictionary\n",
    "        poses_from_traj = {}\n",
    "        for line in traj:\n",
    "            traj_timestamp = line.split(\" \")[0] \n",
    "\n",
    "\n",
    "            poses_from_traj[f\"{traj_timestamp}\"] = np.array(TrajStringToMatrix(line)[1].tolist())\n",
    "            \n",
    "\n",
    "        return poses_from_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ddd698-3f1b-4db8-8b0c-bdf337bb79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_intrinsics(intrinsics_path):\n",
    "        \"\"\"\n",
    "        Retrieve the camera intrinsics for a given scene and video sequence.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str): The identifier of the scene.\n",
    "            video_id (str): The identifier of the video sequence.\n",
    "            data_asset_identifier (str, optional): The data asset type for camera intrinsics.\n",
    "                                                   Can be either \"hires_wide_intrinsics\" or \"lowres_wide_intrinsics\". \n",
    "                                                   Defaults to \"hires_wide_intrinsics\".\n",
    "\n",
    "        Returns:\n",
    "            (dict): A dictionary mapping timestamps to file paths of camera intrinsics data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported data asset identifier is provided.\n",
    "            FileNotFoundError: If no intrinsics files are found at the specified path.\n",
    "        \"\"\"\n",
    "        intrinsics_mapping = {}\n",
    "        \n",
    "\n",
    "        intrinsics = sorted(glob.glob(os.path.join(intrinsics_path, \"*.pincam\")))\n",
    "        \n",
    "\n",
    "        intrinsics_timestamps = [os.path.basename(x).split(\".pincam\")[0].split(\"_\")[1] for x in intrinsics]\n",
    "\n",
    "        # Create mapping from timestamp to full path\n",
    "        intrinsics_mapping = {timestamp: cur_intrinsics for timestamp, cur_intrinsics in zip(intrinsics_timestamps, intrinsics)}\n",
    "\n",
    "        return intrinsics_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d2e6b4d-8fee-47d6-8270-25f089db1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_camera_intrinsics(intrinsics_file_path, format=\"tuple\"):\n",
    "        \"\"\"\n",
    "        Parses a file containing camera intrinsic parameters and returns them in the specified format.\n",
    "\n",
    "        Args:\n",
    "            intrinsics_file_path (str): The path to the file containing camera intrinsic parameters.\n",
    "            format (str, optional): The format in which to return the camera intrinsic parameters.\n",
    "                                    Supported formats are \"tuple\" and \"matrix\". Defaults to \"tuple\".\n",
    "\n",
    "        Returns:\n",
    "            (Union[tuple, numpy.ndarray]): Camera intrinsic parameters in the specified format.\n",
    "\n",
    "                - If format is \"tuple\", returns a tuple \\\\(w, h, fx, fy, hw, hh\\\\).\n",
    "                - If format is \"matrix\", returns a 3x3 numpy array representing the camera matrix.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported format is specified.\n",
    "        \"\"\"\n",
    "        w, h, fx, fy, hw, hh = np.loadtxt(intrinsics_file_path)\n",
    "\n",
    "        if format == \"tuple\":\n",
    "            return (w, h, fx, fy, hw, hh)\n",
    "        elif format == \"matrix\":\n",
    "            return np.asarray([[fx, 0, hw], [0, fy, hh], [0, 0, 1]])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format {format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd822cd-e32b-42c3-ba4c-765bd9d7231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_files = sorted(os.listdir(rgb_path))\n",
    "depth_files = sorted(os.listdir(depth_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9983d27f-c7c3-4854-8364-8e3fc0133280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intrinsics_path = get_camera_intrinsics(intrinsic_file_path)\n",
    "#print(intrinsics_path)\n",
    "#width, height, _, _, _, _ = read_camera_intrinsics(intrinsics_path)\n",
    "#poses_from_traj = get_camera_trajectory(traj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ad511eb-e86a-4a4e-996a-3f1f5467a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParser:\n",
    "    \"\"\"\n",
    "    A class for parsing data files in the SceneFun3D dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split):\n",
    "        \"\"\"\n",
    "        Initialize the DataParser instance with the root path.\n",
    "\n",
    "        Args:\n",
    "            data_root_path (str): The root path where data is located.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.data_root_path = os.path.join(root, split)\n",
    "\n",
    "\n",
    "    def get_data_asset_path(self, data_asset_identifier, visit_id, video_id=None):\n",
    "        \"\"\"\n",
    "        Get the file path for a specified data asset.\n",
    "\n",
    "        Args:\n",
    "            data_asset_identifier (str): A string identifier for the data asset.\n",
    "            visit_id (str or int): The identifier for the visit (scene).\n",
    "            video_id (str or int, optional): The identifier for the video sequence. Required if specified data asset requires a video identifier.\n",
    "\n",
    "        Returns:\n",
    "            (Path): A Path object representing the file path to the specified data asset.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If the `data_asset_identifier` is not valid or if `video_id` is required but not provided.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            data_asset_identifier in data_asset_to_path\n",
    "        ), f\"Data asset identifier '{data_asset_identifier}' is not valid\"\n",
    "\n",
    "        data_path = data_asset_to_path[data_asset_identifier]\n",
    "\n",
    "        if (\"<video_id>\" in data_path) and (video_id is None):\n",
    "            assert (\n",
    "                False\n",
    "            ), f\"video_id must be specified for the data asset identifier '{data_asset_identifier}'\"\n",
    "\n",
    "        visit_id = str(visit_id)\n",
    "\n",
    "        data_path = data_path.replace(\"<data_dir>\", self.data_root_path).replace(\n",
    "            \"<visit_id>\", visit_id\n",
    "        )\n",
    "\n",
    "        if \"<video_id>\" in data_path:\n",
    "            video_id = str(video_id)\n",
    "            data_path = data_path.replace(\"<video_id>\", video_id)\n",
    "\n",
    "        return data_path\n",
    "    def get_descriptions(self, visit_id):\n",
    "        \"\"\"\n",
    "        Retrieve the natural language task descriptions for a specified scene.\n",
    "\n",
    "        Args:\n",
    "            visit_id (str or int): The identifier for the scene.\n",
    "\n",
    "        Returns:\n",
    "            (list): A list of descriptions, each represented as a dictionary.\n",
    "        \"\"\"\n",
    "        descriptions_path = self.get_data_asset_path(\n",
    "            data_asset_identifier=\"descriptions\", visit_id=visit_id\n",
    "        )\n",
    "\n",
    "        with open(descriptions_path, \"r\") as f:\n",
    "            descriptions_data = json.load(f)[\"descriptions\"]\n",
    "\n",
    "        return descriptions_data\n",
    "\n",
    "\n",
    "    def get_descriptions_list(self, visit_id: str):\n",
    "        \"\"\"\n",
    "        List of descriptions given a visit_id\n",
    "        \"\"\"\n",
    "\n",
    "        descs = self.get_descriptions(visit_id)\n",
    "        desc_ids = {desc[\"desc_id\"]: desc[\"description\"] for desc in descs}\n",
    "        return desc_ids\n",
    "\n",
    "\n",
    "    def get_visits(self) -> list:\n",
    "        \"\"\"\n",
    "        Given a split, returns a dict associating each visit id to the list of video ids\n",
    "        \"\"\"\n",
    "\n",
    "        with open(\n",
    "            os.path.join(f\"{self.root}/benchmark_file_lists/{self.split}_set.csv\")\n",
    "        ) as f:\n",
    "            # skip csv header\n",
    "            visit_video = f.readlines()[1:]\n",
    "\n",
    "        visits = list()\n",
    "        for line in visit_video:\n",
    "            visit_id = line.strip(\"\\n\").split(\",\")[0]\n",
    "            visits.append(visit_id)\n",
    "\n",
    "        return visits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95769e92-57b7-44f1-9b6b-4861b3dfe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcbab500-c5b3-4e04-b8ab-222d9c619ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyDsWhaBPW5fHBZz-R3t4ImQ0MFTCBPtw2I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b05e5347-f346-4ec5-a99e-f61528c9c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model = SamModel.from_pretrained(\"jadechoghari/robustsam-vit-large\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"jadechoghari/robustsam-vit-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d3c632f-becc-45b4-bf9e-c1c1b237cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "owl_processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "owl_model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d775e071-cef0-4ab2-b770-295e31593bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(objects, rgb):\n",
    "    H, W, _ = rgb.shape \n",
    "    #create the parent node \n",
    "    text_labels = [objects] \n",
    "    inputs = owl_processor(text=text_labels, images=rgb, return_tensors=\"pt\")\n",
    "    outputs = owl_model(**inputs)\n",
    "    # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "    target_sizes = torch.tensor([(H, W)]) \n",
    "    # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "    results = owl_processor.post_process_grounded_object_detection(\n",
    "                  outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    "       )\n",
    "    # Retrieve predictions for the first image for the corresponding text queries\n",
    "    result = results[0]\n",
    "    boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"] \n",
    "\n",
    "    #print(\"text_labels = \", text_labels)\n",
    "\n",
    "    #for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    #   #print(box)\n",
    "    #   box = [round(i, 2) for i in box.tolist()]\n",
    "    #   print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "    bb_boxes_list = [] \n",
    "    boxes = boxes.detach().numpy()\n",
    "    for bb in boxes:\n",
    "       bb_boxes_list.append(list(bb))\n",
    "\n",
    "    model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "    input_boxes = [bb_boxes_list] # 2D localization of a window \n",
    "    image = rgb\n",
    "\n",
    "    inputs = processor(image, input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    scores = outputs.iou_scores \n",
    "\n",
    "    return masks , text_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3e2b97c-4457-4839-98e2-c852dcdc1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(image_path):\n",
    "    OLLAMA_PORT = 11434\n",
    "    client = ollama.Client(host=f\"localhost:{OLLAMA_PORT}\") \n",
    "    response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[\n",
    "         {\n",
    "                \"role\": \"system\",\n",
    "                \"content\":  \"You are an AI system that receives an image of a scene and outputs a JSON\\n representation of all the objects present and their functional parts \\n that can be acted upon by a robot. \\n Your task is to:\\n1. Identify all objects in the given image.\\n2. For each object, identify its functional parts.\\n3. Output them in JSON format.\",\n",
    "            },{\n",
    "        'role': 'user',\n",
    "        'content': 'Respond directly with the following JSON format: \\n {{{\"object\": the visible object, \"functional_part\": list of functional part of the object}}}',\n",
    "        'images': [image_path]\n",
    "    }]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827d83a-d133-44a5-809b-1745a27b7d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ce53e-5a40-4679-85ff-4e31b0721bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a9520f-c858-44b3-8f2a-58144294a621",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 1 column 33 (char 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m objects \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_output, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Parse each JSON object individually\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m parsed_objects \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objects]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Wrap into one valid JSON structure\u001b[39;00m\n\u001b[1;32m     28\u001b[0m final_json \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m\"\u001b[39m: parsed_objects}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid control character at: line 1 column 33 (char 32)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(len(rgb_files)):\n",
    "       time_stamp = rgb_files[i].split(\"_\")[1].split(\".jpg\")[0] \n",
    "       intrinsic_path = get_camera_intrinsics(intrinsic_files_path)[time_stamp]\n",
    "       width, height, fx, fy, ox, oy = read_camera_intrinsics(intrinsic_path)\n",
    "       pose = get_camera_trajectory(traj_path)[time_stamp] \n",
    "       K = np.zeros((3,3), dtype=np.float32)\n",
    "       K[0][0] = fx \n",
    "       K[1][1] = fy \n",
    "       K[2][2] = 1.0 \n",
    "       K[0][2] = ox \n",
    "       K[1][2] = oy \n",
    "\n",
    "       mapper = ConceptGraphsMapper(K) \n",
    "       rgb_file_path = os.path.join(rgb_path, rgb_files[i])\n",
    "       depth_file_path = os.path.join(depth_path, depth_files[i]) \n",
    "\n",
    "       #gemini code \n",
    "       my_file = client.files.upload(file=rgb_file_path) \n",
    "       raw_output = get_llm_response(rgb_file_path) \n",
    "       # Find all {...} JSON objects using regex\n",
    "       objects = re.findall(r'\\{.*?\\}', raw_output, re.DOTALL)\n",
    "\n",
    "       # Parse each JSON object individually\n",
    "       parsed_objects = [json.loads(obj) for obj in objects]\n",
    "\n",
    "       # Wrap into one valid JSON structure\n",
    "       final_json = {\"objects\": parsed_objects}\n",
    "\n",
    "       final_json = json.loads(json.dumps(final_json, indent=4))\n",
    "        \n",
    "    \n",
    "       functional_output = final_json[\"objects\"]\n",
    "       #print(functional_output)\n",
    "\n",
    "       # create the list of object and their children functional part \n",
    "       objects = []\n",
    "       children_list = []\n",
    "       for k in range(len(functional_output)):\n",
    "          obj_dict = functional_output[k]\n",
    "          obj = obj_dict[\"object\"]            # string \n",
    "          children = obj_dict[\"functional_part\"]  # list of childer list \n",
    "          objects.append(obj) \n",
    "          children_list.append(children) \n",
    "          \n",
    "\n",
    "       rgb = cv2.imread(rgb_file_path) \n",
    "       depth = cv2.imread(depth_file_path)[:, :, 0] \n",
    "      \n",
    "       #create node of the objects \n",
    "       masks, text_labels = generate_mask(objects=objects, rgb=rgb )\n",
    "       parent_list = []\n",
    "       \n",
    "       T = pose\n",
    "       masks = masks[0][:, 0, :, :]\n",
    "       # generate the parent list for the objects (None)\n",
    "       total_mask, _, _ = masks.shape \n",
    "       for k in range(total_mask):\n",
    "           parent_list.append(None)\n",
    "       graph = mapper.process_frame(rgb, depth, T, masks, parent_list, text_labels, rgb_file_path, None)\n",
    "\n",
    "      \n",
    "        \n",
    "print(f\"Frame  objects={len(graph.objects)}, edges={len(graph.edges)}\")\n",
    "print(\"time = \", time.time() - start_time)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339ab43-9ddb-415b-8fc5-ac242af1ae52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d853b45-0385-42d5-b405-e95b05fca7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e0091-e8e3-4eb9-9600-60c2c986a126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c472d8c-3603-402e-b299-d80901b9c353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712f3ef-9f04-4fba-8b7f-8e8751689a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaafbb3-51e4-4de6-b99f-3c930d0319e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c9338-2b84-4229-b184-45c1f9d4bba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494356a-4f9e-4420-ba7c-e541eda8d506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aaf31-3162-4bd1-9d8a-8c11c4798ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be346c1-41df-4d33-b413-3d76ccdf2c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c574fd-7f9f-4b4a-95cb-a84c0f25f96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161e58f-d64c-429c-a0af-4b3bc3642c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c408f8-4cf9-4d58-869e-e4402aebc6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f8420-88bc-4272-91ea-3c21700543dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514382df-1700-4af4-a6ee-1b42cebe2f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1607a-8872-4e11-8877-326e2d2148ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3d6d7-245c-4441-ba6f-a700e3cb5379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afd387-d474-4827-9c7d-0c18103ca6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa30c0-d898-4af4-b860-8cec8b21c73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc4055-5410-475c-b850-165b9ad348aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277ed67-fe5f-4c72-9baa-f5b5c4f7d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bc2ea-49d4-4d92-ad28-48b830caeab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
