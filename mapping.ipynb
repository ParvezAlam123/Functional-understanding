{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96840731-7e1b-47a6-95bc-704d29aad2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Union\n",
    "from typing import List, Tuple \n",
    "import cv2\n",
    "import numpy as np \n",
    "from typing import List, Tuple\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from typing import Dict, Union\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "from typing import Any, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numba import njit \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc34ae77-d374-45ba-b6e8-b6a5abfe6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryVisualizer:\n",
    "    _num_drawn_points: int = 1\n",
    "    _cached_path_mask: Union[np.ndarray, None] = None\n",
    "    _origin_in_img: Union[np.ndarray, None] = None\n",
    "    _pixels_per_meter: Union[float, None] = None\n",
    "    agent_line_length: int = 10\n",
    "    agent_line_thickness: int = 3\n",
    "    path_color: tuple = (0, 255, 0)\n",
    "    path_thickness: int = 3\n",
    "    scale_factor: float = 1.0\n",
    "\n",
    "    def __init__(self, origin_in_img: np.ndarray, pixels_per_meter: float):\n",
    "        self._origin_in_img = origin_in_img\n",
    "        self._pixels_per_meter = pixels_per_meter\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._num_drawn_points = 1\n",
    "        self._cached_path_mask = None\n",
    "\n",
    "    def draw_trajectory(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        camera_positions: Union[np.ndarray, List[np.ndarray]],\n",
    "        camera_yaw: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Draws the trajectory on the image and returns it\"\"\"\n",
    "        img = self._draw_path(img, camera_positions)\n",
    "        img = self._draw_agent(img, camera_positions[-1], camera_yaw)\n",
    "        return img\n",
    "\n",
    "    def _draw_path(self, img: np.ndarray, camera_positions: Union[np.ndarray, List[np.ndarray]]) -> np.ndarray:\n",
    "        \"\"\"Draws the path on the image and returns it\"\"\"\n",
    "        if len(camera_positions) < 2:\n",
    "            return img\n",
    "        if self._cached_path_mask is not None:\n",
    "            path_mask = self._cached_path_mask.copy()\n",
    "        else:\n",
    "            path_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        for i in range(self._num_drawn_points - 1, len(camera_positions) - 1):\n",
    "            path_mask = self._draw_line(path_mask, camera_positions[i], camera_positions[i + 1])\n",
    "\n",
    "        img[path_mask == 255] = self.path_color\n",
    "\n",
    "        self._cached_path_mask = path_mask\n",
    "        self._num_drawn_points = len(camera_positions)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _draw_line(self, img: np.ndarray, pt_a: np.ndarray, pt_b: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Draws a line between two points and returns it\"\"\"\n",
    "        # Convert metric coordinates to pixel coordinates\n",
    "        px_a = self._metric_to_pixel(pt_a)\n",
    "        px_b = self._metric_to_pixel(pt_b)\n",
    "\n",
    "        if np.array_equal(px_a, px_b):\n",
    "            return img\n",
    "\n",
    "        cv2.line(\n",
    "            img,\n",
    "            tuple(px_a[::-1]),\n",
    "            tuple(px_b[::-1]),\n",
    "            255,\n",
    "            int(self.path_thickness * self.scale_factor),\n",
    "        )\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _draw_agent(self, img: np.ndarray, camera_position: np.ndarray, camera_yaw: float) -> np.ndarray:\n",
    "        \"\"\"Draws the agent on the image and returns it\"\"\"\n",
    "        px_position = self._metric_to_pixel(camera_position)\n",
    "        cv2.circle(\n",
    "            img,\n",
    "            tuple(px_position[::-1]),\n",
    "            int(8 * self.scale_factor),\n",
    "            (255, 192, 15),\n",
    "            -1,\n",
    "        )\n",
    "        heading_end_pt = (\n",
    "            int(px_position[0] - self.agent_line_length * self.scale_factor * np.cos(camera_yaw)),\n",
    "            int(px_position[1] - self.agent_line_length * self.scale_factor * np.sin(camera_yaw)),\n",
    "        )\n",
    "        cv2.line(\n",
    "            img,\n",
    "            tuple(px_position[::-1]),\n",
    "            tuple(heading_end_pt[::-1]),\n",
    "            (0, 0, 0),\n",
    "            int(self.agent_line_thickness * self.scale_factor),\n",
    "        )\n",
    "\n",
    "        return img\n",
    "\n",
    "    def draw_circle(self, img: np.ndarray, position: np.ndarray, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Draws the point as a circle on the image and returns it\"\"\"\n",
    "        px_position = self._metric_to_pixel(position)\n",
    "        cv2.circle(img, tuple(px_position[::-1]), **kwargs)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _metric_to_pixel(self, pt: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Converts a metric coordinate to a pixel coordinate\"\"\"\n",
    "        # Need to flip y-axis because pixel coordinates start from top left\n",
    "        px = pt * self._pixels_per_meter * np.array([-1, -1]) + self._origin_in_img\n",
    "        # px = pt * self._pixels_per_meter + self._origin_in_img\n",
    "        px = px.astype(np.int32)\n",
    "        return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85caf851-01fa-4355-854f-027c93375707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMap:\n",
    "    _camera_positions: List[np.ndarray] = []\n",
    "    _last_camera_yaw: float = 0.0\n",
    "    _map_dtype: np.dtype = np.dtype(np.float32)\n",
    "\n",
    "    def __init__(self, size: int = 1000, pixels_per_meter: int = 20, *args: Any, **kwargs: Any):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: The size of the map in pixels.\n",
    "        \"\"\"\n",
    "        self.pixels_per_meter = pixels_per_meter\n",
    "        self.size = size\n",
    "        self._map = np.zeros((size, size), dtype=self._map_dtype)\n",
    "        self._episode_pixel_origin = np.array([size // 2, size // 2])\n",
    "        self._traj_vis = TrajectoryVisualizer(self._episode_pixel_origin, self.pixels_per_meter)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._map.fill(0)\n",
    "        self._camera_positions = []\n",
    "        self._traj_vis = TrajectoryVisualizer(self._episode_pixel_origin, self.pixels_per_meter)\n",
    "\n",
    "    def update_agent_traj(self, robot_xy: np.ndarray, robot_heading: float) -> None:\n",
    "        self._camera_positions.append(robot_xy)\n",
    "        self._last_camera_yaw = robot_heading\n",
    "\n",
    "    def _xy_to_px(self, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Converts an array of (x, y) coordinates to pixel coordinates.\n",
    "\n",
    "        Args:\n",
    "            points: The array of (x, y) coordinates to convert.\n",
    "\n",
    "        Returns:\n",
    "            The array of (x, y) pixel coordinates.\n",
    "        \"\"\"\n",
    "        px = np.rint(points[:, ::-1] * self.pixels_per_meter) + self._episode_pixel_origin\n",
    "        px[:, 0] = self._map.shape[0] - px[:, 0]\n",
    "        return px.astype(int)\n",
    "\n",
    "    def _px_to_xy(self, px: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Converts an array of pixel coordinates to (x, y) coordinates.\n",
    "\n",
    "        Args:\n",
    "            px: The array of pixel coordinates to convert.\n",
    "\n",
    "        Returns:\n",
    "            The array of (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        px_copy = px.copy()\n",
    "        px_copy[:, 0] = self._map.shape[0] - px_copy[:, 0]\n",
    "        points = (px_copy - self._episode_pixel_origin) / self.pixels_per_meter\n",
    "        return points[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfe028c-cf90-4a96-9e3d-cbc1825ba947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Paste the TrajectoryVisualizer and BaseMap classes here before this code\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Create a base map (like a blank canvas)\n",
    "base_map = BaseMap(size=600, pixels_per_meter=40)\n",
    "\n",
    "# Create a blank RGB image (for visualization)\n",
    "img = np.ones((base_map.size, base_map.size, 3), dtype=np.uint8) * 255\n",
    "\n",
    "# Generate random trajectory points (robot positions)\n",
    "num_points = 15\n",
    "trajectory = []\n",
    "x, y = 0.0, 0.0  # start at origin\n",
    "for _ in range(num_points):\n",
    "    x += np.random.uniform(-0.2, 0.5)  # random motion in meters\n",
    "    y += np.random.uniform(-0.3, 0.3)\n",
    "    trajectory.append(np.array([x, y]))\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "# Simulate random yaw (robot heading)\n",
    "yaw = np.deg2rad(45)  # 45 degrees in radians\n",
    "\n",
    "# Update map with trajectory\n",
    "for i in range(len(trajectory)):\n",
    "    base_map.update_agent_traj(trajectory[i], yaw)\n",
    "\n",
    "# Draw trajectory on image\n",
    "output_img = base_map._traj_vis.draw_trajectory(\n",
    "    img.copy(),\n",
    "    np.array(base_map._camera_positions),\n",
    "    base_map._last_camera_yaw,\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Trajectory Visualization\", output_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44063d55-3882-4796-9052-7a7cc0369add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frontier:\n",
    "    def __init__(self, xyz: np.ndarray, cosine: float):\n",
    "        self.xyz = xyz\n",
    "        self.cosine = cosine \n",
    "\n",
    "class FrontierMap:\n",
    "    frontiers: List[Frontier] = []\n",
    "\n",
    "    def __init__(self, encoding_type: str = \"cosine\"):\n",
    "        self.encoder: BLIP2ITMClient = BLIP2ITMClient()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.frontiers = []\n",
    "\n",
    "    def update(self, frontier_locations: List[np.ndarray], curr_image: np.ndarray, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Takes in a list of frontier coordinates and the current image observation from\n",
    "        the robot. Any stored frontiers that are not present in the given list are\n",
    "        removed. Any frontiers in the given list that are not already stored are added.\n",
    "        When these frontiers are added, their cosine field is set to the encoding\n",
    "        of the given image. The image will only be encoded if a new frontier is added.\n",
    "\n",
    "        Args:\n",
    "            frontier_locations (List[np.ndarray]): A list of frontier coordinates.\n",
    "            curr_image (np.ndarray): The current image observation from the robot.\n",
    "            text (str): The text to compare the image to.\n",
    "        \"\"\"\n",
    "        # Remove any frontiers that are not in the given list. Use np.array_equal.\n",
    "        self.frontiers = [\n",
    "            frontier\n",
    "            for frontier in self.frontiers\n",
    "            if any(np.array_equal(frontier.xyz, location) for location in frontier_locations)\n",
    "        ]\n",
    "\n",
    "        # Add any frontiers that are not already stored. Set their image field to the\n",
    "        # given image.\n",
    "        cosine = None\n",
    "        for location in frontier_locations:\n",
    "            if not any(np.array_equal(frontier.xyz, location) for frontier in self.frontiers):\n",
    "                if cosine is None:\n",
    "                    cosine = self._encode(curr_image, text)\n",
    "                self.frontiers.append(Frontier(location, cosine))\n",
    "\n",
    "    def _encode(self, image: np.ndarray, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Encodes the given image using the encoding type specified in the constructor.\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): The image to encode.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        return self.encoder.cosine(image, text)\n",
    "\n",
    "    def sort_waypoints(self) -> Tuple[np.ndarray, List[float]]:\n",
    "        \"\"\"\n",
    "        Returns the frontier with the highest cosine and the value of that cosine.\n",
    "        \"\"\"\n",
    "        # Use np.argsort to get the indices of the sorted cosines\n",
    "        cosines = [f.cosine for f in self.frontiers]\n",
    "        waypoints = [f.xyz for f in self.frontiers]\n",
    "        sorted_inds = np.argsort([-c for c in cosines])  # sort in descending order\n",
    "        sorted_values = [cosines[i] for i in sorted_inds]\n",
    "        sorted_frontiers = np.array([waypoints[i] for i in sorted_inds])\n",
    "\n",
    "        return sorted_frontiers, sorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1694a2c-61ed-42ac-a56c-09a5551da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yaw(matrix: np.ndarray) -> float:\n",
    "    \"\"\"Extract the yaw angle from a 4x4 transformation matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): A 4x4 transformation matrix.\n",
    "    Returns:\n",
    "        float: The yaw angle in radians.\n",
    "    \"\"\"\n",
    "    assert matrix.shape == (4, 4), \"The input matrix must be 4x4\"\n",
    "    rotation_matrix = matrix[:3, :3]\n",
    "\n",
    "    # Compute the yaw angle\n",
    "    yaw = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "\n",
    "    return yaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bf4bc28-9521-448d-b67d-a36653f81bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_cloud(depth_image: np.ndarray, mask: np.ndarray, fx: float, fy: float) -> np.ndarray:\n",
    "    \"\"\"Calculates the 3D coordinates (x, y, z) of points in the depth image based on\n",
    "    the horizontal field of view (HFOV), the image width and height, the depth values,\n",
    "    and the pixel x and y coordinates.\n",
    "\n",
    "    Args:\n",
    "        depth_image (np.ndarray): 2D depth image.\n",
    "        mask (np.ndarray): 2D binary mask identifying relevant pixels.\n",
    "        fx (float): Focal length in the x direction.\n",
    "        fy (float): Focal length in the y direction.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of 3D coordinates (x, y, z) of the points in the image plane.\n",
    "    \"\"\"\n",
    "    v, u = np.where(mask)\n",
    "    z = depth_image[v, u]\n",
    "    x = (u - depth_image.shape[1] // 2) * z / fx\n",
    "    y = (v - depth_image.shape[0] // 2) * z / fy\n",
    "    cloud = np.stack((z, -x, -y), axis=-1)\n",
    "\n",
    "    return cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d37aa96-9b03-49fd-90c0-5d14052fbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points(transformation_matrix: np.ndarray, points: np.ndarray) -> np.ndarray:\n",
    "    # Add a homogeneous coordinate of 1 to each point for matrix multiplication\n",
    "    homogeneous_points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "\n",
    "    # Apply the transformation matrix to the points\n",
    "    transformed_points = np.dot(transformation_matrix, homogeneous_points.T).T\n",
    "\n",
    "    # Remove the added homogeneous coordinate and divide by the last coordinate\n",
    "    return transformed_points[:, :3] / transformed_points[:, 3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887964cf-a22c-4d05-a541-b3de6aeb391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_fov_cone(\n",
    "    cone_origin: np.ndarray,\n",
    "    cone_angle: float,\n",
    "    cone_fov: float,\n",
    "    cone_range: float,\n",
    "    points: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Checks if points are within a cone of a given origin, angle, fov, and range.\n",
    "\n",
    "    Args:\n",
    "        cone_origin (np.ndarray): The origin of the cone.\n",
    "        cone_angle (float): The angle of the cone in radians.\n",
    "        cone_fov (float): The field of view of the cone in radians.\n",
    "        cone_range (float): The range of the cone.\n",
    "        points (np.ndarray): The points to check.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The subarray of points that are within the cone.\n",
    "    \"\"\"\n",
    "    directions = points[:, :3] - cone_origin\n",
    "    dists = np.linalg.norm(directions, axis=1)\n",
    "    angles = np.arctan2(directions[:, 1], directions[:, 0])\n",
    "    angle_diffs = np.mod(angles - cone_angle + np.pi, 2 * np.pi) - np.pi\n",
    "\n",
    "    mask = np.logical_and(dists <= cone_range, np.abs(angle_diffs) <= cone_fov / 2)\n",
    "    return points[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbc9d886-ee38-42d4-b266-5bcde95b776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectPointCloudMap:\n",
    "    clouds: Dict[str, np.ndarray] = {}\n",
    "    use_dbscan: bool = True\n",
    "\n",
    "    def __init__(self, erosion_size: float) -> None:\n",
    "        self._erosion_size = erosion_size\n",
    "        self.last_target_coord: Union[np.ndarray, None] = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.clouds = {}\n",
    "        self.last_target_coord = None\n",
    "\n",
    "    def has_object(self, target_class: str) -> bool:\n",
    "        return target_class in self.clouds and len(self.clouds[target_class]) > 0\n",
    "\n",
    "    def update_map(\n",
    "        self,\n",
    "        object_name: str,\n",
    "        depth_img: np.ndarray,\n",
    "        object_mask: np.ndarray,\n",
    "        tf_camera_to_episodic: np.ndarray,\n",
    "        min_depth: float,\n",
    "        max_depth: float,\n",
    "        fx: float,\n",
    "        fy: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Updates the object map with the latest information from the agent.\"\"\"\n",
    "        local_cloud = self._extract_object_cloud(depth_img, object_mask, min_depth, max_depth, fx, fy)\n",
    "        if len(local_cloud) == 0:\n",
    "            return\n",
    "\n",
    "        # For second-class, bad detections that are too offset or out of range, we\n",
    "        # assign a random number to the last column of its point cloud that can later\n",
    "        # be used to identify which points came from the same detection.\n",
    "        if too_offset(object_mask):\n",
    "            within_range = np.ones_like(local_cloud[:, 0]) * np.random.rand()\n",
    "        else:\n",
    "            # Mark all points of local_cloud whose distance from the camera is too far\n",
    "            # as being out of range\n",
    "            within_range = (local_cloud[:, 0] <= max_depth * 0.95) * 1.0  # 5% margin\n",
    "            # All values of 1 in within_range will be considered within range, and all\n",
    "            # values of 0 will be considered out of range; these 0s need to be\n",
    "            # assigned with a random number so that they can be identified later.\n",
    "            within_range = within_range.astype(np.float32)\n",
    "            within_range[within_range == 0] = np.random.rand()\n",
    "        global_cloud = transform_points(tf_camera_to_episodic, local_cloud)\n",
    "        global_cloud = np.concatenate((global_cloud, within_range[:, None]), axis=1)\n",
    "\n",
    "        curr_position = tf_camera_to_episodic[:3, 3]\n",
    "        closest_point = self._get_closest_point(global_cloud, curr_position)\n",
    "        dist = np.linalg.norm(closest_point[:3] - curr_position)\n",
    "        if dist < 1.0:\n",
    "            # Object is too close to trust as a valid object\n",
    "            return\n",
    "\n",
    "        if object_name in self.clouds:\n",
    "            self.clouds[object_name] = np.concatenate((self.clouds[object_name], global_cloud), axis=0)\n",
    "        else:\n",
    "            self.clouds[object_name] = global_cloud\n",
    "\n",
    "    def get_best_object(self, target_class: str, curr_position: np.ndarray) -> np.ndarray:\n",
    "        target_cloud = self.get_target_cloud(target_class)\n",
    "\n",
    "        closest_point_2d = self._get_closest_point(target_cloud, curr_position)[:2]\n",
    "\n",
    "        if self.last_target_coord is None:\n",
    "            self.last_target_coord = closest_point_2d\n",
    "        else:\n",
    "            # Do NOT update self.last_target_coord if:\n",
    "            # 1. the closest point is only slightly different\n",
    "            # 2. the closest point is a little different, but the agent is too far for\n",
    "            #    the difference to matter much\n",
    "            delta_dist = np.linalg.norm(closest_point_2d - self.last_target_coord)\n",
    "            if delta_dist < 0.1:\n",
    "                # closest point is only slightly different\n",
    "                return self.last_target_coord\n",
    "            elif delta_dist < 0.5 and np.linalg.norm(curr_position - closest_point_2d) > 2.0:\n",
    "                # closest point is a little different, but the agent is too far for\n",
    "                # the difference to matter much\n",
    "                return self.last_target_coord\n",
    "            else:\n",
    "                self.last_target_coord = closest_point_2d\n",
    "\n",
    "        return self.last_target_coord\n",
    "\n",
    "    def update_explored(self, tf_camera_to_episodic: np.ndarray, max_depth: float, cone_fov: float) -> None:\n",
    "        \"\"\"\n",
    "        This method will remove all point clouds in self.clouds that were originally\n",
    "        detected to be out-of-range, but are now within range. This is just a heuristic\n",
    "        that suppresses ephemeral false positives that we now confirm are not actually\n",
    "        target objects.\n",
    "\n",
    "        Args:\n",
    "            tf_camera_to_episodic: The transform from the camera to the episode frame.\n",
    "            max_depth: The maximum distance from the camera that we consider to be\n",
    "                within range.\n",
    "            cone_fov: The field of view of the camera.\n",
    "        \"\"\"\n",
    "        camera_coordinates = tf_camera_to_episodic[:3, 3]\n",
    "        camera_yaw = extract_yaw(tf_camera_to_episodic)\n",
    "\n",
    "        for obj in self.clouds:\n",
    "            within_range = within_fov_cone(\n",
    "                camera_coordinates,\n",
    "                camera_yaw,\n",
    "                cone_fov,\n",
    "                max_depth * 0.5,\n",
    "                self.clouds[obj],\n",
    "            )\n",
    "            range_ids = set(within_range[..., -1].tolist())\n",
    "            for range_id in range_ids:\n",
    "                if range_id == 1:\n",
    "                    # Detection was originally within range\n",
    "                    continue\n",
    "                # Remove all points from self.clouds[obj] that have the same range_id\n",
    "                self.clouds[obj] = self.clouds[obj][self.clouds[obj][..., -1] != range_id]\n",
    "\n",
    "    def get_target_cloud(self, target_class: str) -> np.ndarray:\n",
    "        target_cloud = self.clouds[target_class].copy()\n",
    "        # Determine whether any points are within range\n",
    "        within_range_exists = np.any(target_cloud[:, -1] == 1)\n",
    "        if within_range_exists:\n",
    "            # Filter out all points that are not within range\n",
    "            target_cloud = target_cloud[target_cloud[:, -1] == 1]\n",
    "        return target_cloud\n",
    "\n",
    "    def _extract_object_cloud(\n",
    "        self,\n",
    "        depth: np.ndarray,\n",
    "        object_mask: np.ndarray,\n",
    "        min_depth: float,\n",
    "        max_depth: float,\n",
    "        fx: float,\n",
    "        fy: float,\n",
    "    ) -> np.ndarray:\n",
    "        final_mask = object_mask * 255\n",
    "        final_mask = cv2.erode(final_mask, None, iterations=self._erosion_size)  # type: ignore\n",
    "\n",
    "        valid_depth = depth.copy()\n",
    "        valid_depth[valid_depth == 0] = 1  # set all holes (0) to just be far (1)\n",
    "        valid_depth = valid_depth * (max_depth - min_depth) + min_depth\n",
    "        cloud = get_point_cloud(valid_depth, final_mask, fx, fy)\n",
    "        cloud = get_random_subarray(cloud, 5000)\n",
    "        if self.use_dbscan:\n",
    "            cloud = open3d_dbscan_filtering(cloud)\n",
    "\n",
    "        return cloud\n",
    "\n",
    "    def _get_closest_point(self, cloud: np.ndarray, curr_position: np.ndarray) -> np.ndarray:\n",
    "        ndim = curr_position.shape[0]\n",
    "        if self.use_dbscan:\n",
    "            # Return the point that is closest to curr_position, which is 2D\n",
    "            closest_point = cloud[np.argmin(np.linalg.norm(cloud[:, :ndim] - curr_position, axis=1))]\n",
    "        else:\n",
    "            # Calculate the Euclidean distance from each point to the reference point\n",
    "            if ndim == 2:\n",
    "                ref_point = np.concatenate((curr_position, np.array([0.5])))\n",
    "            else:\n",
    "                ref_point = curr_position\n",
    "            distances = np.linalg.norm(cloud[:, :3] - ref_point, axis=1)\n",
    "\n",
    "            # Use argsort to get the indices that would sort the distances\n",
    "            sorted_indices = np.argsort(distances)\n",
    "\n",
    "            # Get the top 20% of points\n",
    "            percent = 0.25\n",
    "            top_percent = sorted_indices[: int(percent * len(cloud))]\n",
    "            try:\n",
    "                median_index = top_percent[int(len(top_percent) / 2)]\n",
    "            except IndexError:\n",
    "                median_index = 0\n",
    "            closest_point = cloud[median_index]\n",
    "        return closest_point\n",
    "\n",
    "\n",
    "def open3d_dbscan_filtering(points: np.ndarray, eps: float = 0.2, min_points: int = 100) -> np.ndarray:\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    labels = np.array(pcd.cluster_dbscan(eps, min_points))\n",
    "\n",
    "    # Count the points in each cluster\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # Exclude noise points, which are given the label -1\n",
    "    non_noise_labels_mask = unique_labels != -1\n",
    "    non_noise_labels = unique_labels[non_noise_labels_mask]\n",
    "    non_noise_label_counts = label_counts[non_noise_labels_mask]\n",
    "\n",
    "    if len(non_noise_labels) == 0:  # only noise was detected\n",
    "        return np.array([])\n",
    "\n",
    "    # Find the label of the largest non-noise cluster\n",
    "    largest_cluster_label = non_noise_labels[np.argmax(non_noise_label_counts)]\n",
    "\n",
    "    # Get the indices of points in the largest non-noise cluster\n",
    "    largest_cluster_indices = np.where(labels == largest_cluster_label)[0]\n",
    "\n",
    "    # Get the points in the largest non-noise cluster\n",
    "    largest_cluster_points = points[largest_cluster_indices]\n",
    "\n",
    "    return largest_cluster_points\n",
    "\n",
    "\n",
    "def visualize_and_save_point_cloud(point_cloud: np.ndarray, save_path: str) -> None:\n",
    "    \"\"\"Visualizes an array of 3D points and saves the visualization as a PNG image.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): Array of 3D points with shape (N, 3).\n",
    "        save_path (str): Path to save the PNG image.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    x = point_cloud[:, 0]\n",
    "    y = point_cloud[:, 1]\n",
    "    z = point_cloud[:, 2]\n",
    "\n",
    "    ax.scatter(x, y, z, c=\"b\", marker=\"o\")\n",
    "\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_random_subarray(points: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns a subarray of a given 3D points array. The size of the\n",
    "    subarray is specified by the user. The elements of the subarray are randomly\n",
    "    selected from the original array. If the size of the original array is smaller than\n",
    "    the specified size, the function will simply return the original array.\n",
    "\n",
    "    Args:\n",
    "        points (numpy array): A numpy array of 3D points. Each element of the array is a\n",
    "            3D point represented as a numpy array of size 3.\n",
    "        size (int): The desired size of the subarray.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: A subarray of the original points array.\n",
    "    \"\"\"\n",
    "    if len(points) <= size:\n",
    "        return points\n",
    "    indices = np.random.choice(len(points), size, replace=False)\n",
    "    return points[indices]\n",
    "\n",
    "\n",
    "def too_offset(mask: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    This will return true if the entire bounding rectangle of the mask is either on the\n",
    "    left or right third of the mask. This is used to determine if the object is too far\n",
    "    to the side of the image to be a reliable detection.\n",
    "\n",
    "    Args:\n",
    "        mask (numpy array): A 2D numpy array of 0s and 1s representing the mask of the\n",
    "            object.\n",
    "    Returns:\n",
    "        bool: True if the object is too offset, False otherwise.\n",
    "    \"\"\"\n",
    "    # Find the bounding rectangle of the mask\n",
    "    x, y, w, h = cv2.boundingRect(mask)\n",
    "\n",
    "    # Calculate the thirds of the mask\n",
    "    third = mask.shape[1] // 3\n",
    "\n",
    "    # Check if the entire bounding rectangle is in the left or right third of the mask\n",
    "    if x + w <= third:\n",
    "        # Check if the leftmost point is at the edge of the image\n",
    "        # return x == 0\n",
    "        return x <= int(0.05 * mask.shape[1])\n",
    "    elif x >= 2 * third:\n",
    "        # Check if the rightmost point is at the edge of the image\n",
    "        # return x + w == mask.shape[1]\n",
    "        return x + w >= int(0.95 * mask.shape[1])\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953d6bda-4ce8-4dd5-8f61-94c6eef63bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid object points found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "\n",
    "# Paste your full code (all functions and the ObjectPointCloudMap class) above this line\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# ---- Step 1. Create a fake depth image (like what comes from a depth sensor) ----\n",
    "H, W = 100, 120\n",
    "depth_image = np.random.uniform(0.2, 2.0, (H, W)).astype(np.float32)  # depth in meters\n",
    "\n",
    "# ---- Step 2. Create a fake binary mask of an object ----\n",
    "object_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "cv2.circle(object_mask, (60, 50), 15, 1, -1)  # circular object mask\n",
    "\n",
    "# ---- Step 3. Set fake camera intrinsics (focal lengths) ----\n",
    "fx, fy = 150.0, 150.0\n",
    "\n",
    "# ---- Step 4. Create a fake camera-to-world transform ----\n",
    "# Simple translation (0,0,0) and rotation identity (no rotation)\n",
    "tf_camera_to_world = np.eye(4)\n",
    "tf_camera_to_world[:3, 3] = np.array([1.0, 0.0, 0.5])  # shift camera a bit\n",
    "\n",
    "# ---- Step 5. Initialize ObjectPointCloudMap ----\n",
    "obj_map = ObjectPointCloudMap(erosion_size=1)\n",
    "\n",
    "# ---- Step 6. Update the map with random example object ----\n",
    "obj_map.update_map(\n",
    "    object_name=\"bottle\",\n",
    "    depth_img=depth_image,\n",
    "    object_mask=object_mask,\n",
    "    tf_camera_to_episodic=tf_camera_to_world,\n",
    "    min_depth=0.1,\n",
    "    max_depth=3.0,\n",
    "    fx=fx,\n",
    "    fy=fy,\n",
    ")\n",
    "\n",
    "# ---- Step 7. Get the object cloud and visualize it ----\n",
    "if obj_map.has_object(\"bottle\"):\n",
    "    cloud = obj_map.get_target_cloud(\"bottle\")\n",
    "    print(f\"Extracted {len(cloud)} 3D points for 'bottle' object.\")\n",
    "\n",
    "    # Visualize using matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(cloud[:, 0], cloud[:, 1], cloud[:, 2], c='b', s=2)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    plt.title(\"Random Object Point Cloud\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid object points found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f645996-35ae-41a3-a35b-b78fd54f0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_small_holes(depth_img: np.ndarray, area_thresh: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Identifies regions in the depth image that have a value of 0 and fills them in\n",
    "    with 1 if the region is smaller than a given area threshold.\n",
    "\n",
    "    Args:\n",
    "        depth_img (np.ndarray): The input depth image\n",
    "        area_thresh (int): The area threshold for filling in holes\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The depth image with small holes filled in\n",
    "    \"\"\"\n",
    "    # Create a binary image where holes are 1 and the rest is 0\n",
    "    binary_img = np.where(depth_img == 0, 1, 0).astype(\"uint8\")\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    filled_holes = np.zeros_like(binary_img)\n",
    "\n",
    "    for cnt in contours:\n",
    "        # If the area of the contour is smaller than the threshold\n",
    "        if cv2.contourArea(cnt) < area_thresh:\n",
    "            # Fill the contour\n",
    "            cv2.drawContours(filled_holes, [cnt], 0, 1, -1)\n",
    "\n",
    "    # Create the filled depth image\n",
    "    filled_depth_img = np.where(filled_holes == 1, 1, depth_img)\n",
    "\n",
    "    return filled_depth_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99094747-482d-47a5-88eb-e1a9d3cfcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time:  0.00018384239890358666\n"
     ]
    }
   ],
   "source": [
    "def _bresenhamline_nslope(slope):\n",
    "    \"\"\"\n",
    "    Normalize slope for Bresenham's line algorithm.\n",
    "\n",
    "    >>> s = np.array([[-2, -2, -2, 0]])\n",
    "    >>> _bresenhamline_nslope(s)\n",
    "    array([[-1., -1., -1.,  0.]])\n",
    "\n",
    "    >>> s = np.array([[0, 0, 0, 0]])\n",
    "    >>> _bresenhamline_nslope(s)\n",
    "    array([[ 0.,  0.,  0.,  0.]])\n",
    "\n",
    "    >>> s = np.array([[0, 0, 9, 0]])\n",
    "    >>> _bresenhamline_nslope(s)\n",
    "    array([[ 0.,  0.,  1.,  0.]])\n",
    "    \"\"\"\n",
    "    scale = np.amax(np.abs(slope), axis=1).reshape(-1, 1)\n",
    "    zeroslope = (scale == 0).all(1)\n",
    "    scale[zeroslope] = np.ones(1)\n",
    "    normalizedslope = np.array(slope, dtype=np.double) / scale\n",
    "    normalizedslope[zeroslope] = np.zeros(slope[0].shape)\n",
    "    return normalizedslope\n",
    "\n",
    "\n",
    "def _bresenhamlines(start, end, max_iter):\n",
    "    \"\"\"\n",
    "    Returns npts lines of length max_iter each. (npts x max_iter x dimension)\n",
    "\n",
    "    >>> s = np.array([[3, 1, 9, 0],[0, 0, 3, 0]])\n",
    "    >>> _bresenhamlines(s, np.zeros(s.shape[1]), max_iter=-1)\n",
    "    array([[[ 3,  1,  8,  0],\n",
    "            [ 2,  1,  7,  0],\n",
    "            [ 2,  1,  6,  0],\n",
    "            [ 2,  1,  5,  0],\n",
    "            [ 1,  0,  4,  0],\n",
    "            [ 1,  0,  3,  0],\n",
    "            [ 1,  0,  2,  0],\n",
    "            [ 0,  0,  1,  0],\n",
    "            [ 0,  0,  0,  0]],\n",
    "    <BLANKLINE>\n",
    "           [[ 0,  0,  2,  0],\n",
    "            [ 0,  0,  1,  0],\n",
    "            [ 0,  0,  0,  0],\n",
    "            [ 0,  0, -1,  0],\n",
    "            [ 0,  0, -2,  0],\n",
    "            [ 0,  0, -3,  0],\n",
    "            [ 0,  0, -4,  0],\n",
    "            [ 0,  0, -5,  0],\n",
    "            [ 0,  0, -6,  0]]])\n",
    "    \"\"\"\n",
    "    if max_iter == -1:\n",
    "        max_iter = np.amax(np.amax(np.abs(end - start), axis=1))\n",
    "    npts, dim = start.shape\n",
    "    nslope = _bresenhamline_nslope(end - start)\n",
    "\n",
    "    # steps to iterate on\n",
    "    stepseq = np.arange(1, max_iter + 1)\n",
    "    stepmat = np.tile(stepseq, (dim, 1)).T\n",
    "\n",
    "    # some hacks for broadcasting properly\n",
    "    bline = start[:, np.newaxis, :] + nslope[:, np.newaxis, :] * stepmat\n",
    "\n",
    "    # Approximate to nearest int\n",
    "    return np.array(np.rint(bline), dtype=start.dtype)\n",
    "\n",
    "\n",
    "def bresenhamline(start, end, max_iter=5):\n",
    "    \"\"\"\n",
    "    Returns a list of points from (start, end) by ray tracing a line b/w the\n",
    "    points.\n",
    "    Parameters:\n",
    "        start: An array of start points (number of points x dimension)\n",
    "        end:   An end points (1 x dimension)\n",
    "            or An array of end point corresponding to each start point\n",
    "                (number of points x dimension)\n",
    "        max_iter: Max points to traverse. if -1, maximum number of required\n",
    "                  points are traversed\n",
    "\n",
    "    Returns:\n",
    "        linevox (n x dimension) A cumulative array of all points traversed by\n",
    "        all the lines so far.\n",
    "\n",
    "    >>> s = np.array([[3, 1, 9, 0],[0, 0, 3, 0]])\n",
    "    >>> bresenhamline(s, np.zeros(s.shape[1]), max_iter=-1)\n",
    "    array([[ 3,  1,  8,  0],\n",
    "           [ 2,  1,  7,  0],\n",
    "           [ 2,  1,  6,  0],\n",
    "           [ 2,  1,  5,  0],\n",
    "           [ 1,  0,  4,  0],\n",
    "           [ 1,  0,  3,  0],\n",
    "           [ 1,  0,  2,  0],\n",
    "           [ 0,  0,  1,  0],\n",
    "           [ 0,  0,  0,  0],\n",
    "           [ 0,  0,  2,  0],\n",
    "           [ 0,  0,  1,  0],\n",
    "           [ 0,  0,  0,  0],\n",
    "           [ 0,  0, -1,  0],\n",
    "           [ 0,  0, -2,  0],\n",
    "           [ 0,  0, -3,  0],\n",
    "           [ 0,  0, -4,  0],\n",
    "           [ 0,  0, -5,  0],\n",
    "           [ 0,  0, -6,  0]])\n",
    "    \"\"\"\n",
    "    # Return the points as a single array\n",
    "    return _bresenhamlines(start, end, max_iter).reshape(-1, start.shape[-1])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import cv2\n",
    "    import time\n",
    "\n",
    "    DRAW = False\n",
    "\n",
    "    img = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "    np.random.seed(0)\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        st = time.time()\n",
    "        x0, y0 = np.random.randint(0, 500, 2)\n",
    "        x1, y1 = np.random.randint(0, 500, 2)\n",
    "        pts = bresenhamline(np.array([[x0, y0]]), np.array([[x1, y1]]), max_iter=-1)\n",
    "        times.append(time.time() - st)\n",
    "        if DRAW:\n",
    "            img_copy = img.copy()\n",
    "            for idx, (x, y) in enumerate(pts):\n",
    "                # Change the color using cv2.COLORMAP_RAINBOW so red is closer to start\n",
    "                # use the length of pts to normalize idx\n",
    "                color = cv2.applyColorMap(\n",
    "                    np.uint8([255 * (idx + 1) / len(pts)]), cv2.COLORMAP_RAINBOW\n",
    "                )[0][0]\n",
    "                color = tuple(int(i) for i in color)\n",
    "                img_copy[y, x] = color\n",
    "            # Draw start and end with circles\n",
    "            cv2.circle(img_copy, (x0, y0), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(img_copy, (x1, y1), 3, (0, 0, 255), -1)\n",
    "            cv2.imshow(\"img\", img_copy)\n",
    "            cv2.waitKey(0)\n",
    "    print(\"Average time: \", np.mean(times[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c3de863-36e9-422a-a21a-633c8cf1d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 0.000048 seconds\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mline(img, \u001b[38;5;28mtuple\u001b[39m(s[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(s[\u001b[38;5;241m1\u001b[39m]), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Draw the closest segment in green\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m cv2\u001b[38;5;241m.\u001b[39mline(img, \u001b[38;5;28mtuple\u001b[39m(closest[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(closest[\u001b[38;5;241m1\u001b[39m]), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     47\u001b[0m cv2\u001b[38;5;241m.\u001b[39mcircle(img, coord, \u001b[38;5;241m10\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Display\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n"
     ]
    }
   ],
   "source": [
    "def closest_line_segment(\n",
    "    coord: np.ndarray, segments: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    closest_points = closest_point_on_segment(coord, segments[:, 0], segments[:, 1])\n",
    "    # Identify the segment that yielded the closest point\n",
    "    min_idx = np.argmin(np.linalg.norm(closest_points - coord, axis=1))\n",
    "    closest_segment, closest_point = segments[min_idx], closest_points[min_idx]\n",
    "\n",
    "    return closest_segment, closest_point\n",
    "\n",
    "\n",
    "def closest_point_on_segment(p: np.ndarray, a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    segment = b - a\n",
    "    t = np.einsum(\"ij,ij->i\", p - a, segment) / np.einsum(\"ij,ij->i\", segment, segment)\n",
    "    t = np.clip(t, 0, 1)\n",
    "    return a + t[:, np.newaxis] * segment\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    # Test data\n",
    "    coord = np.array(np.random.rand(2) * 500, dtype=int)\n",
    "\n",
    "    # Create a list of line segments using random sampling\n",
    "    segments = (np.random.rand(10, 2, 2) * 500).astype(int)\n",
    "\n",
    "    # Time the function\n",
    "    start = time.perf_counter()\n",
    "    for i in range(10000):\n",
    "        closest = closest_line_segment(coord, segments)\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "\n",
    "    # Print the average execution time\n",
    "    print(f\"Average execution time: {elapsed / 10000:.6f} seconds\")\n",
    "\n",
    "    # Use OpenCV to draw the segments, highlighting the closest one\n",
    "    img = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "    # Draw all segments in red\n",
    "    for s in segments:\n",
    "        cv2.line(img, tuple(s[0]), tuple(s[1]), (0, 0, 255), 1)\n",
    "    # Draw the closest segment in green\n",
    "    cv2.line(img, tuple(closest[0]), tuple(closest[1]), (0, 255, 0), 3)\n",
    "    cv2.circle(img, coord, 10, (255, 0, 0), -1)\n",
    "    # Display\n",
    "    cv2.imshow(\"Closest segment\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7e2913d-7b4e-4994-912f-1fcc0f800766",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = os.environ.get(\"MAP_VISUALIZE\", \"False\").lower() == \"true\"\n",
    "DEBUG = os.environ.get(\"MAP_DEBUG\", \"False\").lower() == \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e568fc05-c09a-456f-a7a7-b5cc620b5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_frontier_waypoints(\n",
    "    full_map: np.ndarray,\n",
    "    explored_mask: np.ndarray,\n",
    "    area_thresh: Optional[int] = -1,\n",
    "    xy: Optional[np.ndarray] = None,\n",
    "):\n",
    "    if DEBUG:\n",
    "        import time\n",
    "\n",
    "        os.makedirs(\"map_debug\", exist_ok=True)\n",
    "        cv2.imwrite(\n",
    "            f\"map_debug/{int(time.time())}_debug_full_map_{area_thresh}.png\", full_map\n",
    "        )\n",
    "        cv2.imwrite(\n",
    "            f\"map_debug/{int(time.time())}_debug_explored_mask_{area_thresh}.png\",\n",
    "            explored_mask,\n",
    "        )\n",
    "\n",
    "    if VISUALIZE:\n",
    "        img = cv2.cvtColor(full_map * 255, cv2.COLOR_GRAY2BGR)\n",
    "        img[explored_mask > 0] = (127, 127, 127)\n",
    "\n",
    "        cv2.imshow(\"inputs\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    explored_mask[full_map == 0] = 0\n",
    "    frontiers = detect_frontiers(full_map, explored_mask, area_thresh)\n",
    "    if VISUALIZE:\n",
    "        img = cv2.cvtColor(full_map * 255, cv2.COLOR_GRAY2BGR)\n",
    "        img[explored_mask > 0] = (127, 127, 127)\n",
    "        # Draw a dot at each point on each frontier\n",
    "        for idx, frontier in enumerate(frontiers):\n",
    "            # Uniformly sample colors from the COLORMAP_RAINBOW\n",
    "            color = cv2.applyColorMap(\n",
    "                np.uint8([255 * (idx + 1) / len(frontiers)]), cv2.COLORMAP_RAINBOW\n",
    "            )[0][0]\n",
    "            color = tuple(int(i) for i in color)\n",
    "            for idx2, p in enumerate(frontier):\n",
    "                if idx2 < len(frontier) - 1:\n",
    "                    cv2.line(img, p[0], frontier[idx2 + 1][0], color, 3)\n",
    "        cv2.imshow(\"frontiers\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    waypoints = frontier_waypoints(frontiers, xy)\n",
    "    return waypoints\n",
    "\n",
    "\n",
    "def detect_frontiers(\n",
    "    full_map: np.ndarray, explored_mask: np.ndarray, area_thresh: Optional[int] = -1\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"Detects frontiers in a map.\n",
    "\n",
    "    Args:\n",
    "        full_map (np.ndarray): White polygon on black image, where white is navigable.\n",
    "        Mono-channel mask.\n",
    "        explored_mask (np.ndarray): Portion of white polygon that has been seen already.\n",
    "        This is also a mono-channel mask.\n",
    "        area_thresh (int, optional): Minimum unexplored area (in pixels) needed adjacent\n",
    "        to a frontier for that frontier to be valid. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A mono-channel mask where white contours represent each frontier.\n",
    "    \"\"\"\n",
    "    # Find the contour of the explored area\n",
    "    filtered_explored_mask = filter_out_small_unexplored(\n",
    "        full_map, explored_mask, area_thresh\n",
    "    )\n",
    "    contours, _ = cv2.findContours(\n",
    "        filtered_explored_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    if VISUALIZE:\n",
    "        img = cv2.cvtColor(full_map * 255, cv2.COLOR_GRAY2BGR)\n",
    "        img[explored_mask > 0] = (127, 127, 127)\n",
    "        cv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n",
    "        cv2.imshow(\"contours\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    unexplored_mask = np.where(filtered_explored_mask > 0, 0, full_map)\n",
    "    unexplored_mask = cv2.blur(  # blurring for some leeway\n",
    "        np.where(unexplored_mask > 0, 255, unexplored_mask), (3, 3)\n",
    "    )\n",
    "    frontiers = []\n",
    "    # TODO: There shouldn't be more than one contour (only one explored area on map)\n",
    "    for contour in contours:\n",
    "        frontiers.extend(\n",
    "            contour_to_frontiers(interpolate_contour(contour), unexplored_mask)\n",
    "        )\n",
    "    return frontiers\n",
    "\n",
    "\n",
    "def filter_out_small_unexplored(\n",
    "    full_map: np.ndarray, explored_mask: np.ndarray, area_thresh: int\n",
    "):\n",
    "    \"\"\"Edit the explored map to add small unexplored areas, which ignores their\n",
    "    frontiers.\"\"\"\n",
    "    if area_thresh == -1:\n",
    "        return explored_mask\n",
    "\n",
    "    unexplored_mask = full_map.copy()\n",
    "    unexplored_mask[explored_mask > 0] = 0\n",
    "\n",
    "    if VISUALIZE:\n",
    "        img = cv2.cvtColor(unexplored_mask * 255, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.imshow(\"unexplored mask\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Find contours in the unexplored mask\n",
    "    contours, _ = cv2.findContours(\n",
    "        unexplored_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    if VISUALIZE:\n",
    "        img = cv2.cvtColor(unexplored_mask * 255, cv2.COLOR_GRAY2BGR)\n",
    "        # Draw the contours in red\n",
    "        cv2.drawContours(img, contours, -1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"unexplored mask with contours\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Add small unexplored areas to the explored map\n",
    "    small_contours = []\n",
    "    for i, contour in enumerate(contours):\n",
    "        if cv2.contourArea(contour) < area_thresh:\n",
    "            mask = np.zeros_like(explored_mask)\n",
    "            mask = cv2.drawContours(mask, [contour], 0, 1, -1)\n",
    "            masked_values = unexplored_mask[mask.astype(bool)]\n",
    "            values = set(masked_values.tolist())\n",
    "            if 1 in values and len(values) == 1:\n",
    "                small_contours.append(contour)\n",
    "    new_explored_mask = explored_mask.copy()\n",
    "    cv2.drawContours(new_explored_mask, small_contours, -1, 255, -1)\n",
    "\n",
    "    if VISUALIZE and len(small_contours) > 0:\n",
    "        # Draw the full map and the new explored mask, then outline the contours that\n",
    "        # were added to the explored mask\n",
    "        img = cv2.cvtColor(full_map * 255, cv2.COLOR_GRAY2BGR)\n",
    "        img[new_explored_mask > 0] = (127, 127, 127)\n",
    "        cv2.drawContours(img, small_contours, -1, (0, 0, 255), 3)\n",
    "        cv2.imshow(\"small unexplored areas\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return new_explored_mask\n",
    "\n",
    "\n",
    "def interpolate_contour(contour):\n",
    "    \"\"\"Given a cv2 contour, this function will add points in between each pair of\n",
    "    points in the contour using the bresenham algorithm to make the contour more\n",
    "    continuous.\n",
    "    :param contour: A cv2 contour of shape (N, 1, 2)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # First, reshape and expand the frontier to be a 2D array of shape (N-1, 2, 2)\n",
    "    # representing line segments between adjacent points\n",
    "    line_segments = np.concatenate((contour[:-1], contour[1:]), axis=1).reshape(\n",
    "        (-1, 2, 2)\n",
    "    )\n",
    "    # Also add a segment connecting the last point to the first point\n",
    "    line_segments = np.concatenate(\n",
    "        (line_segments, np.array([contour[-1], contour[0]]).reshape((1, 2, 2)))\n",
    "    )\n",
    "    pts = []\n",
    "    for (x0, y0), (x1, y1) in line_segments:\n",
    "        pts.append(\n",
    "            bresenhamline(np.array([[x0, y0]]), np.array([[x1, y1]]), max_iter=-1)\n",
    "        )\n",
    "    pts = np.concatenate(pts).reshape((-1, 1, 2))\n",
    "    return pts\n",
    "\n",
    "\n",
    "@njit\n",
    "def contour_to_frontiers(contour, unexplored_mask):\n",
    "    \"\"\"Given a contour from OpenCV, return a list of numpy arrays. Each array contains\n",
    "    contiguous points forming a single frontier. The contour is assumed to be a set of\n",
    "    contiguous points, but some of these points are not on any frontier, indicated by\n",
    "    having a value of 0 in the unexplored mask. This function will split the contour\n",
    "    into multiple arrays that exclude such points.\"\"\"\n",
    "    bad_inds = []\n",
    "    num_contour_points = len(contour)\n",
    "    for idx in range(num_contour_points):\n",
    "        x, y = contour[idx][0]\n",
    "        if unexplored_mask[y, x] == 0:\n",
    "            bad_inds.append(idx)\n",
    "    frontiers = np.split(contour, bad_inds)\n",
    "    # np.split is fast but does NOT remove the element at the split index\n",
    "    filtered_frontiers = []\n",
    "    front_last_split = (\n",
    "        0 not in bad_inds\n",
    "        and len(bad_inds) > 0\n",
    "        and max(bad_inds) < num_contour_points - 2\n",
    "    )\n",
    "    for idx, f in enumerate(frontiers):\n",
    "        # a frontier must have at least 2 points (3 with bad ind)\n",
    "        if len(f) > 2 or (idx == 0 and front_last_split):\n",
    "            if idx == 0:\n",
    "                filtered_frontiers.append(f)\n",
    "            else:\n",
    "                filtered_frontiers.append(f[1:])\n",
    "    # Combine the first and last frontier if the first point of the first frontier and\n",
    "    # the last point of the last frontier are the first and last points of the original\n",
    "    # contour. Only check if there are at least 2 frontiers.\n",
    "    if len(filtered_frontiers) > 1 and front_last_split:\n",
    "        last_frontier = filtered_frontiers.pop()\n",
    "        filtered_frontiers[0] = np.concatenate((last_frontier, filtered_frontiers[0]))\n",
    "    return filtered_frontiers\n",
    "\n",
    "\n",
    "def frontier_waypoints(\n",
    "    frontiers: List[np.ndarray], xy: Optional[np.ndarray] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"For each given frontier, returns the point on the frontier closest (euclidean\n",
    "    distance) to the given coordinate. If coordinate is not given, will just return\n",
    "    the midpoints of each frontier.\n",
    "\n",
    "    Args:\n",
    "        frontiers (List[np.ndarray]): list of arrays of shape (X, 1, 2), where each\n",
    "        array is a frontier and X is NOT the same across arrays\n",
    "        xy (np.ndarray): the given coordinate\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array of waypoints, one for each frontier\n",
    "    \"\"\"\n",
    "    if xy is None:\n",
    "        return np.array([get_frontier_midpoint(i) for i in frontiers])\n",
    "    return np.array([get_closest_frontier_point(xy, i) for i in frontiers])\n",
    "\n",
    "\n",
    "@njit\n",
    "def get_frontier_midpoint(frontier) -> np.ndarray:\n",
    "    \"\"\"Given a list of contiguous points (numpy arrays) representing a frontier, first\n",
    "    calculate the total length of the frontier, then find the midpoint of the\n",
    "    frontier\"\"\"\n",
    "    # First, reshape and expand the frontier to be a 2D array of shape (X, 2, 2)\n",
    "    # representing line segments between adjacent points\n",
    "    line_segments = np.concatenate((frontier[:-1], frontier[1:]), axis=1).reshape(\n",
    "        (-1, 2, 2)\n",
    "    )\n",
    "    # Calculate the length of each line segment\n",
    "    line_lengths = np.sqrt(\n",
    "        np.square(line_segments[:, 0, 0] - line_segments[:, 1, 0])\n",
    "        + np.square(line_segments[:, 0, 1] - line_segments[:, 1, 1])\n",
    "    )\n",
    "    cum_sum = np.cumsum(line_lengths)\n",
    "    total_length = cum_sum[-1]\n",
    "    # Find the midpoint of the frontier\n",
    "    half_length = total_length / 2\n",
    "    # Find the line segment that contains the midpoint\n",
    "    line_segment_idx = np.argmax(cum_sum > half_length)\n",
    "    # Calculate the coordinates of the midpoint\n",
    "    line_segment = line_segments[line_segment_idx]\n",
    "    line_length = line_lengths[line_segment_idx]\n",
    "    # Use the difference between the midpoint length and cumsum\n",
    "    # to find the proportion of the line segment that the midpoint is at\n",
    "    length_up_to = cum_sum[line_segment_idx - 1] if line_segment_idx > 0 else 0\n",
    "    proportion = (half_length - length_up_to) / line_length\n",
    "    # Calculate the midpoint coordinates\n",
    "    midpoint = line_segment[0] + proportion * (line_segment[1] - line_segment[0])\n",
    "    return midpoint\n",
    "\n",
    "\n",
    "def get_closest_frontier_point(xy, frontier):\n",
    "    \"\"\"Returns the point on the frontier closest to the given coordinate.\"\"\"\n",
    "    # First, reshape and expand the frontier to be a 2D array of shape (X, 2)\n",
    "    # representing line segments between adjacent points\n",
    "    line_segments = np.concatenate([frontier[:-1], frontier[1:]], axis=1).reshape(\n",
    "        (-1, 2, 2)\n",
    "    )\n",
    "    closest_segment, closest_point = closest_line_segment(xy, line_segments)\n",
    "    return closest_point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc19adcf-ea6a-4465-9d72-4745e7a35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def wrap_heading(heading):\n",
    "    \"\"\"Ensures input heading is between -180 an 180; can be float or np.ndarray\"\"\"\n",
    "    return (heading + np.pi) % (2 * np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f46678a-397e-4e1c-9fea-088f5baf7119",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 266\u001b[0m\n\u001b[1;32m    262\u001b[0m viz \u001b[38;5;241m=\u001b[39m visualize(\n\u001b[1;32m    263\u001b[0m     top_down_map, fog, current_point, current_angle, agent_radius\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    265\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviz\u001b[39m\u001b[38;5;124m\"\u001b[39m, viz)\n\u001b[0;32m--> 266\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    267\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_two_farthest_points(source, cnt, agent_yaw):\n",
    "    \"\"\"Returns the two points in the contour cnt that form the smallest and largest\n",
    "    angles from the source point.\"\"\"\n",
    "    pts = cnt.reshape(-1, 2)\n",
    "    pts = pts - source\n",
    "    rotation_matrix = np.array(\n",
    "        [\n",
    "            [np.cos(-agent_yaw), -np.sin(-agent_yaw)],\n",
    "            [np.sin(-agent_yaw), np.cos(-agent_yaw)],\n",
    "        ]\n",
    "    )\n",
    "    pts = np.matmul(pts, rotation_matrix)\n",
    "    angles = np.arctan2(pts[:, 1], pts[:, 0])\n",
    "    # Get the two points that form the smallest and largest angles from the source\n",
    "    min_idx = np.argmin(angles)\n",
    "    max_idx = np.argmax(angles)\n",
    "    return cnt[min_idx], cnt[max_idx]\n",
    "\n",
    "\n",
    "def vectorize_get_line_points(current_point, points, max_line_len):\n",
    "    angles = np.arctan2(\n",
    "        points[..., 1] - current_point[1], points[..., 0] - current_point[0]\n",
    "    )\n",
    "    endpoints = np.stack(\n",
    "        (\n",
    "            points[..., 0] + max_line_len * np.cos(angles),\n",
    "            points[..., 1] + max_line_len * np.sin(angles),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    "    endpoints = endpoints.astype(np.int32)\n",
    "\n",
    "    line_points = np.stack([points.reshape(-1, 2), endpoints.reshape(-1, 2)], axis=1)\n",
    "    return line_points\n",
    "\n",
    "\n",
    "def get_line_points(current_point, points, maxlen):\n",
    "    current_point = np.repeat(current_point[np.newaxis, :], 2 * len(points), axis=0)\n",
    "    points = np.repeat(points, 2, axis=0)\n",
    "    diffs = current_point - points\n",
    "    angles = np.arctan2(diffs[:, 1], diffs[:, 0])\n",
    "    end_points = current_point + maxlen * np.column_stack(\n",
    "        (np.cos(angles), np.sin(angles))\n",
    "    )\n",
    "    line_points = np.concatenate((points, end_points), axis=1)\n",
    "    line_points = np.array(line_points, dtype=np.int32)\n",
    "    return line_points\n",
    "\n",
    "\n",
    "def reveal_fog_of_war(\n",
    "    top_down_map: np.ndarray,\n",
    "    current_fog_of_war_mask: np.ndarray,\n",
    "    current_point: np.ndarray,\n",
    "    current_angle: float,\n",
    "    fov: float = 90,\n",
    "    max_line_len: float = 100,\n",
    "    enable_debug_visualization: bool = False,\n",
    ") -> np.ndarray:\n",
    "    curr_pt_cv2 = current_point[::-1].astype(int)\n",
    "    angle_cv2 = np.rad2deg(wrap_heading(-current_angle + np.pi / 2))\n",
    "\n",
    "    cone_mask = cv2.ellipse(\n",
    "        np.zeros_like(top_down_map),\n",
    "        curr_pt_cv2,\n",
    "        (int(max_line_len), int(max_line_len)),\n",
    "        0,\n",
    "        angle_cv2 - fov / 2,\n",
    "        angle_cv2 + fov / 2,\n",
    "        1,\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    # Create a mask of pixels that are both in the cone and NOT in the top_down_map\n",
    "    obstacles_in_cone = cv2.bitwise_and(cone_mask, 1 - top_down_map)\n",
    "\n",
    "    # Find the contours of the obstacles in the cone\n",
    "    obstacle_contours, _ = cv2.findContours(\n",
    "        obstacles_in_cone, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    if enable_debug_visualization:\n",
    "        vis_top_down_map = top_down_map * 255\n",
    "        vis_top_down_map = cv2.cvtColor(vis_top_down_map, cv2.COLOR_GRAY2BGR)\n",
    "        vis_top_down_map[top_down_map > 0] = (60, 60, 60)\n",
    "        vis_top_down_map[top_down_map == 0] = (255, 255, 255)\n",
    "        cv2.circle(vis_top_down_map, tuple(curr_pt_cv2), 3, (255, 192, 15), -1)\n",
    "        cv2.imshow(\"vis_top_down_map\", vis_top_down_map)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_top_down_map\")\n",
    "\n",
    "        cone_minus_obstacles = cv2.bitwise_and(cone_mask, top_down_map)\n",
    "        vis_cone_minus_obstacles = vis_top_down_map.copy()\n",
    "        vis_cone_minus_obstacles[cone_minus_obstacles == 1] = (127, 127, 127)\n",
    "        cv2.imshow(\"vis_cone_minus_obstacles\", vis_cone_minus_obstacles)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_cone_minus_obstacles\")\n",
    "\n",
    "        vis_obstacles_mask = vis_cone_minus_obstacles.copy()\n",
    "        cv2.drawContours(vis_obstacles_mask, obstacle_contours, -1, (0, 0, 255), 1)\n",
    "        cv2.imshow(\"vis_obstacles_mask\", vis_obstacles_mask)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_obstacles_mask\")\n",
    "\n",
    "    if len(obstacle_contours) == 0:\n",
    "        return current_fog_of_war_mask  # there were no obstacles in the cone\n",
    "\n",
    "    # Find the two points in each contour that form the smallest and largest angles\n",
    "    # from the current position\n",
    "    points = []\n",
    "    for cnt in obstacle_contours:\n",
    "        if cv2.isContourConvex(cnt):\n",
    "            pt1, pt2 = get_two_farthest_points(curr_pt_cv2, cnt, angle_cv2)\n",
    "            points.append(pt1.reshape(-1, 2))\n",
    "            points.append(pt2.reshape(-1, 2))\n",
    "        else:\n",
    "            # Just add every point in the contour\n",
    "            points.append(cnt.reshape(-1, 2))\n",
    "    points = np.concatenate(points, axis=0)\n",
    "\n",
    "    # Fragment the cone using obstacles and two lines per obstacle in the cone\n",
    "    visible_cone_mask = cv2.bitwise_and(cone_mask, top_down_map)\n",
    "    line_points = vectorize_get_line_points(curr_pt_cv2, points, max_line_len * 1.05)\n",
    "    # Draw all lines simultaneously using cv2.polylines\n",
    "    cv2.polylines(visible_cone_mask, line_points, isClosed=False, color=0, thickness=2)\n",
    "\n",
    "    # Identify the contour that is closest to the current position\n",
    "    final_contours, _ = cv2.findContours(\n",
    "        visible_cone_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    visible_area = None\n",
    "    min_dist = np.inf\n",
    "    for cnt in final_contours:\n",
    "        pt = tuple([int(i) for i in curr_pt_cv2])\n",
    "        dist = abs(cv2.pointPolygonTest(cnt, pt, True))\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            visible_area = cnt\n",
    "\n",
    "    if enable_debug_visualization:\n",
    "        vis_points_mask = vis_obstacles_mask.copy()\n",
    "        for point in points.reshape(-1, 2):\n",
    "            cv2.circle(vis_points_mask, tuple(point), 3, (0, 255, 0), -1)\n",
    "        cv2.imshow(\"vis_points_mask\", vis_points_mask)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_points_mask\")\n",
    "\n",
    "        vis_lines_mask = vis_points_mask.copy()\n",
    "        cv2.polylines(\n",
    "            vis_lines_mask, line_points, isClosed=False, color=(0, 0, 255), thickness=2\n",
    "        )\n",
    "        cv2.imshow(\"vis_lines_mask\", vis_lines_mask)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_lines_mask\")\n",
    "\n",
    "        vis_final_contours = vis_top_down_map.copy()\n",
    "        # Draw each contour in a random color\n",
    "        for cnt in final_contours:\n",
    "            color = tuple([int(i) for i in np.random.randint(0, 255, 3)])\n",
    "            cv2.drawContours(vis_final_contours, [cnt], -1, color, -1)\n",
    "        cv2.imshow(\"vis_final_contours\", vis_final_contours)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_final_contours\")\n",
    "\n",
    "        vis_final = vis_top_down_map.copy()\n",
    "        # Draw each contour in a random color\n",
    "        cv2.drawContours(vis_final, [visible_area], -1, (127, 127, 127), -1)\n",
    "        cv2.imshow(\"vis_final\", vis_final)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"vis_final\")\n",
    "\n",
    "    if min_dist > 3:\n",
    "        return current_fog_of_war_mask  # the closest contour was too far away\n",
    "\n",
    "    new_fog = cv2.drawContours(current_fog_of_war_mask, [visible_area], 0, 1, -1)\n",
    "\n",
    "    return new_fog\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    top_down: np.ndarray,\n",
    "    fog_mask: np.ndarray,\n",
    "    agent_pos: np.ndarray,\n",
    "    agent_yaw: float,\n",
    "    agent_size: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Visualize the top-down map with the fog of war and the current position/heading of\n",
    "    the agent superimposed on top. Fog of war is shown in gray, the current position is\n",
    "    shown in blue, and the current heading is shown as a line segment stemming from the\n",
    "    center of the agent towards the heading direction.\n",
    "\n",
    "    Args:\n",
    "        top_down: The top-down map of the environment.\n",
    "        fog_mask: The fog of war mask.\n",
    "        agent_pos: The current position of the agent.\n",
    "        agent_yaw: The current heading of the agent.\n",
    "        agent_size: The size (radius) of the agent, in pixels.\n",
    "    Returns:\n",
    "        The visualization of the top-down map with the fog of war and the current\n",
    "        position/heading of the agent superimposed on top.\n",
    "    \"\"\"\n",
    "    img_size = (*top_down.shape[:2], 3)\n",
    "    viz = np.ones(img_size, dtype=np.uint8) * np.array((60, 60, 60), dtype=np.uint8)\n",
    "    viz[top_down == 0] = (255, 255, 255)\n",
    "    viz[fog_mask > 0] = (127, 127, 127)\n",
    "    cv2.circle(viz, agent_pos[::-1], agent_size, (255, 192, 15), -1)\n",
    "\n",
    "    heading_end_pt = (\n",
    "        agent_size * 1.4 * np.array([np.sin(agent_yaw), np.cos(agent_yaw)])\n",
    "    ) + agent_pos[::-1]\n",
    "\n",
    "    # Draw a line from the current position showing the current_angle\n",
    "    cv2.line(\n",
    "        viz,\n",
    "        agent_pos[::-1],\n",
    "        (int(heading_end_pt[0]), int(heading_end_pt[1])),\n",
    "        (0, 0, 0),\n",
    "        max(1, agent_size // 4),\n",
    "    )\n",
    "    return viz\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    SHOW = True  # whether to imshow the results\n",
    "    window_size = 1000\n",
    "    N = 100\n",
    "    L = (20, 50)\n",
    "    max_line_len = 500\n",
    "    fov = 90\n",
    "    agent_radius = 20\n",
    "    blank = np.ones((window_size, window_size), dtype=np.uint8)\n",
    "    times = []\n",
    "    for _ in range(500):\n",
    "        t_start = time.time()\n",
    "        top_down_map = blank.copy()\n",
    "        # Populate the image with N random rectangles, with a (min, max) length of L\n",
    "        for _ in range(N):\n",
    "            rect_0 = np.random.randint(0, window_size, 2)\n",
    "            rect_1 = rect_0 + np.random.randint(*L, 2)\n",
    "            cv2.rectangle(top_down_map, rect_0, rect_1, 0, -1)\n",
    "        # Sample random position and heading\n",
    "        current_point = np.random.randint(window_size * 0.25, window_size * 0.75, 2)\n",
    "        # Re-sample current_point if it is inside an obstacle\n",
    "        while top_down_map[current_point[1], current_point[0]] != 1:\n",
    "            current_point = np.random.randint(window_size * 0.25, window_size * 0.75, 2)\n",
    "        current_angle = np.random.uniform(-np.pi, np.pi)\n",
    "\n",
    "        fog = reveal_fog_of_war(\n",
    "            top_down_map=top_down_map,\n",
    "            current_fog_of_war_mask=np.zeros_like(top_down_map),\n",
    "            current_point=current_point,\n",
    "            current_angle=current_angle,\n",
    "            fov=fov,\n",
    "            max_line_len=max_line_len,\n",
    "        )\n",
    "\n",
    "        times.append(time.time() - t_start)\n",
    "\n",
    "        if SHOW:\n",
    "            viz = visualize(\n",
    "                top_down_map, fog, current_point, current_angle, agent_radius\n",
    "            )\n",
    "            cv2.imshow(\"viz\", viz)\n",
    "            key = cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    print(f\"Average time: {np.mean(times[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95576c86-5291-49b3-a826-fb184bf5f806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35a49ef6-b931-49d7-8209-50be498e2802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e676d8-74c1-425f-a2bd-385f45923d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752eaaa-4c6d-40fb-9c9e-00b6701b3d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6069fd0-d5d4-424a-95fd-2a4b4d4060d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d30f27-088d-4e1c-af05-2a981ec2e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObstacleMap(BaseMap):\n",
    "    \"\"\"Generates two maps; one representing the area that the robot has explored so far,\n",
    "    and another representing the obstacles that the robot has seen so far.\n",
    "    \"\"\"\n",
    "\n",
    "    _map_dtype: np.dtype = np.dtype(bool)\n",
    "    _frontiers_px: np.ndarray = np.array([])\n",
    "    frontiers: np.ndarray = np.array([])\n",
    "    radius_padding_color: tuple = (100, 100, 100)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_height: float,\n",
    "        max_height: float,\n",
    "        agent_radius: float,\n",
    "        area_thresh: float = 3.0,  # square meters\n",
    "        hole_area_thresh: int = 100000,  # square pixels\n",
    "        size: int = 1000,\n",
    "        pixels_per_meter: int = 20,\n",
    "    ):\n",
    "        super().__init__(size, pixels_per_meter)\n",
    "        self.explored_area = np.zeros((size, size), dtype=bool)\n",
    "        self._map = np.zeros((size, size), dtype=bool)\n",
    "        self._navigable_map = np.zeros((size, size), dtype=bool)\n",
    "        self._min_height = min_height\n",
    "        self._max_height = max_height\n",
    "        self._area_thresh_in_pixels = area_thresh * (self.pixels_per_meter**2)\n",
    "        self._hole_area_thresh = hole_area_thresh\n",
    "        kernel_size = self.pixels_per_meter * agent_radius * 2\n",
    "        # round kernel_size to nearest odd number\n",
    "        kernel_size = int(kernel_size) + (int(kernel_size) % 2 == 0)\n",
    "        self._navigable_kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self._navigable_map.fill(0)\n",
    "        self.explored_area.fill(0)\n",
    "        self._frontiers_px = np.array([])\n",
    "        self.frontiers = np.array([])\n",
    "\n",
    "    def update_map(\n",
    "        self,\n",
    "        depth: Union[np.ndarray, Any],\n",
    "        tf_camera_to_episodic: np.ndarray,\n",
    "        min_depth: float,\n",
    "        max_depth: float,\n",
    "        fx: float,\n",
    "        fy: float,\n",
    "        topdown_fov: float,\n",
    "        explore: bool = True,\n",
    "        update_obstacles: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds all obstacles from the current view to the map. Also updates the area\n",
    "        that the robot has explored so far.\n",
    "\n",
    "        Args:\n",
    "            depth (np.ndarray): The depth image to use for updating the object map. It\n",
    "                is normalized to the range [0, 1] and has a shape of (height, width).\n",
    "\n",
    "            tf_camera_to_episodic (np.ndarray): The transformation matrix from the\n",
    "                camera to the episodic coordinate frame.\n",
    "            min_depth (float): The minimum depth value (in meters) of the depth image.\n",
    "            max_depth (float): The maximum depth value (in meters) of the depth image.\n",
    "            fx (float): The focal length of the camera in the x direction.\n",
    "            fy (float): The focal length of the camera in the y direction.\n",
    "            topdown_fov (float): The field of view of the depth camera projected onto\n",
    "                the topdown map.\n",
    "            explore (bool): Whether to update the explored area.\n",
    "            update_obstacles (bool): Whether to update the obstacle map.\n",
    "        \"\"\"\n",
    "        if update_obstacles:\n",
    "            if self._hole_area_thresh == -1:\n",
    "                filled_depth = depth.copy()\n",
    "                filled_depth[depth == 0] = 1.0\n",
    "            else:\n",
    "                filled_depth = fill_small_holes(depth, self._hole_area_thresh)\n",
    "            scaled_depth = filled_depth * (max_depth - min_depth) + min_depth\n",
    "            mask = scaled_depth < max_depth\n",
    "            point_cloud_camera_frame = get_point_cloud(scaled_depth, mask, fx, fy)\n",
    "            point_cloud_episodic_frame = transform_points(tf_camera_to_episodic, point_cloud_camera_frame)\n",
    "            obstacle_cloud = filter_points_by_height(point_cloud_episodic_frame, self._min_height, self._max_height)\n",
    "\n",
    "            # Populate topdown map with obstacle locations\n",
    "            xy_points = obstacle_cloud[:, :2]\n",
    "            pixel_points = self._xy_to_px(xy_points)\n",
    "            self._map[pixel_points[:, 1], pixel_points[:, 0]] = 1\n",
    "\n",
    "            # Update the navigable area, which is an inverse of the obstacle map after a\n",
    "            # dilation operation to accommodate the robot's radius.\n",
    "            self._navigable_map = 1 - cv2.dilate(\n",
    "                self._map.astype(np.uint8),\n",
    "                self._navigable_kernel,\n",
    "                iterations=1,\n",
    "            ).astype(bool)\n",
    "\n",
    "        if not explore:\n",
    "            return\n",
    "\n",
    "        # Update the explored area\n",
    "        agent_xy_location = tf_camera_to_episodic[:2, 3]\n",
    "        agent_pixel_location = self._xy_to_px(agent_xy_location.reshape(1, 2))[0]\n",
    "        new_explored_area = reveal_fog_of_war(\n",
    "            top_down_map=self._navigable_map.astype(np.uint8),\n",
    "            current_fog_of_war_mask=np.zeros_like(self._map, dtype=np.uint8),\n",
    "            current_point=agent_pixel_location[::-1],\n",
    "            current_angle=-extract_yaw(tf_camera_to_episodic),\n",
    "            fov=np.rad2deg(topdown_fov),\n",
    "            max_line_len=max_depth * self.pixels_per_meter,\n",
    "        )\n",
    "        new_explored_area = cv2.dilate(new_explored_area, np.ones((3, 3), np.uint8), iterations=1)\n",
    "        self.explored_area[new_explored_area > 0] = 1\n",
    "        self.explored_area[self._navigable_map == 0] = 0\n",
    "        contours, _ = cv2.findContours(\n",
    "            self.explored_area.astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE,\n",
    "        )\n",
    "        if len(contours) > 1:\n",
    "            min_dist = np.inf\n",
    "            best_idx = 0\n",
    "            for idx, cnt in enumerate(contours):\n",
    "                dist = cv2.pointPolygonTest(cnt, tuple([int(i) for i in agent_pixel_location]), True)\n",
    "                if dist >= 0:\n",
    "                    best_idx = idx\n",
    "                    break\n",
    "                elif abs(dist) < min_dist:\n",
    "                    min_dist = abs(dist)\n",
    "                    best_idx = idx\n",
    "            new_area = np.zeros_like(self.explored_area, dtype=np.uint8)\n",
    "            cv2.drawContours(new_area, contours, best_idx, 1, -1)  # type: ignore\n",
    "            self.explored_area = new_area.astype(bool)\n",
    "\n",
    "        # Compute frontier locations\n",
    "        self._frontiers_px = self._get_frontiers()\n",
    "        if len(self._frontiers_px) == 0:\n",
    "            self.frontiers = np.array([])\n",
    "        else:\n",
    "            self.frontiers = self._px_to_xy(self._frontiers_px)\n",
    "\n",
    "    def _get_frontiers(self) -> np.ndarray:\n",
    "        \"\"\"Returns the frontiers of the map.\"\"\"\n",
    "        # Dilate the explored area slightly to prevent small gaps between the explored\n",
    "        # area and the unnavigable area from being detected as frontiers.\n",
    "        explored_area = cv2.dilate(\n",
    "            self.explored_area.astype(np.uint8),\n",
    "            np.ones((5, 5), np.uint8),\n",
    "            iterations=1,\n",
    "        )\n",
    "        frontiers = detect_frontier_waypoints(\n",
    "            self._navigable_map.astype(np.uint8),\n",
    "            explored_area,\n",
    "            self._area_thresh_in_pixels,\n",
    "        )\n",
    "        return frontiers\n",
    "\n",
    "    def visualize(self) -> np.ndarray:\n",
    "        \"\"\"Visualizes the map.\"\"\"\n",
    "        vis_img = np.ones((*self._map.shape[:2], 3), dtype=np.uint8) * 255\n",
    "        # Draw explored area in light green\n",
    "        vis_img[self.explored_area == 1] = (200, 255, 200)\n",
    "        # Draw unnavigable areas in gray\n",
    "        vis_img[self._navigable_map == 0] = self.radius_padding_color\n",
    "        # Draw obstacles in black\n",
    "        vis_img[self._map == 1] = (0, 0, 0)\n",
    "        # Draw frontiers in blue (200, 0, 0)\n",
    "        for frontier in self._frontiers_px:\n",
    "            cv2.circle(vis_img, tuple([int(i) for i in frontier]), 5, (200, 0, 0), 2)\n",
    "\n",
    "        vis_img = cv2.flip(vis_img, 0)\n",
    "\n",
    "        if len(self._camera_positions) > 0:\n",
    "            self._traj_vis.draw_trajectory(\n",
    "                vis_img,\n",
    "                self._camera_positions,\n",
    "                self._last_camera_yaw,\n",
    "            )\n",
    "\n",
    "        return vis_img\n",
    "\n",
    "\n",
    "def filter_points_by_height(points: np.ndarray, min_height: float, max_height: float) -> np.ndarray:\n",
    "    return points[(points[:, 2] >= min_height) & (points[:, 2] <= max_height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b833371e-55af-40bc-bd5c-a81e3217c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x1f9ed940) is not the object's thread (0x20201010).\n",
      "Cannot move to target thread (0x1f9ed940)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# -----------------------\n",
    "# Dummy functions required\n",
    "# -----------------------\n",
    "def fill_small_holes(depth, threshold):\n",
    "    # Simple placeholder: just return depth as is\n",
    "    return depth\n",
    "\n",
    "def get_point_cloud(depth, mask, fx, fy):\n",
    "    \"\"\"Return random point cloud for testing.\"\"\"\n",
    "    h, w = depth.shape\n",
    "    num_points = np.sum(mask)\n",
    "    points = np.random.rand(num_points, 3) * 5.0  # random XYZ in meters\n",
    "    return points\n",
    "\n",
    "def transform_points(tf, points):\n",
    "    \"\"\"Apply dummy transform: just add translation from tf\"\"\"\n",
    "    return points + tf[:3, 3]\n",
    "\n",
    "def extract_yaw(tf):\n",
    "    \"\"\"Dummy yaw extraction\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def _xy_to_px(self, xy):\n",
    "    \"\"\"Convert XY coordinates to pixel coordinates for testing\"\"\"\n",
    "    # assume 1m = pixels_per_meter pixels, and center at map middle\n",
    "    center = self._map.shape[0] // 2\n",
    "    px = (xy * self.pixels_per_meter + center).astype(int)\n",
    "    return px\n",
    "\n",
    "def detect_frontier_waypoints(navigable, explored, area_thresh):\n",
    "    \"\"\"Dummy: return random points as frontiers\"\"\"\n",
    "    return np.array([[50,50], [100,150]])\n",
    "\n",
    "# -----------------------\n",
    "# Add missing methods to ObstacleMap\n",
    "# -----------------------\n",
    "ObstacleMap._xy_to_px = _xy_to_px\n",
    "\n",
    "# -----------------------\n",
    "# Test function\n",
    "# -----------------------\n",
    "def test_obstacle_map():\n",
    "    size = 200\n",
    "    pixels_per_meter = 20\n",
    "    obstacle_map = ObstacleMap(\n",
    "        min_height=0.0,\n",
    "        max_height=2.0,\n",
    "        agent_radius=0.5,\n",
    "        size=size,\n",
    "        pixels_per_meter=pixels_per_meter,\n",
    "    )\n",
    "\n",
    "    # Random depth image\n",
    "    depth_image = np.random.rand(size, size).astype(np.float32)\n",
    "    tf_camera_to_map = np.eye(4)  # identity transform\n",
    "    min_depth = 0.1\n",
    "    max_depth = 5.0\n",
    "    fx = fy = 100\n",
    "    topdown_fov = np.pi / 2  # 90 degrees\n",
    "\n",
    "    # Update map\n",
    "    obstacle_map.update_map(\n",
    "        depth=depth_image,\n",
    "        tf_camera_to_episodic=tf_camera_to_map,\n",
    "        min_depth=min_depth,\n",
    "        max_depth=max_depth,\n",
    "        fx=fx,\n",
    "        fy=fy,\n",
    "        topdown_fov=topdown_fov,\n",
    "        explore=True,\n",
    "        update_obstacles=True,\n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    vis = obstacle_map.visualize()\n",
    "    cv2.imshow(\"Obstacle Map\", vis)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# -----------------------\n",
    "# Run the test\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_obstacle_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec23240d-e4fc-4b97-be2c-53c932b5da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(angle: float, ndims: int = 2) -> np.ndarray:\n",
    "    \"\"\"Returns a 2x2 or 3x3 rotation matrix for a given angle; if 3x3, the z-axis is\n",
    "    rotated.\"\"\"\n",
    "    if ndims == 2:\n",
    "        return np.array(\n",
    "            [\n",
    "                [np.cos(angle), -np.sin(angle)],\n",
    "                [np.sin(angle), np.cos(angle)],\n",
    "            ]\n",
    "        )\n",
    "    elif ndims == 3:\n",
    "        return np.array(\n",
    "            [\n",
    "                [np.cos(angle), -np.sin(angle), 0],\n",
    "                [np.sin(angle), np.cos(angle), 0],\n",
    "                [0, 0, 1],\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"ndims must be 2 or 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f34a839d-3e29-452c-81b2-bd72a5ba0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(\n",
    "    image: np.ndarray,\n",
    "    radians: float,\n",
    "    border_value: Union[int, Tuple[int, int, int]] = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Rotate an image by the specified angle in radians.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        radians (float): The angle of rotation in radians.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rotated image.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, np.degrees(radians), 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height), borderValue=border_value)\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def place_img_in_img(img1: np.ndarray, img2: np.ndarray, row: int, col: int) -> np.ndarray:\n",
    "    \"\"\"Place img2 in img1 such that img2's center is at the specified coordinates (xy)\n",
    "    in img1.\n",
    "\n",
    "    Args:\n",
    "        img1 (numpy.ndarray): The base image.\n",
    "        img2 (numpy.ndarray): The image to be placed.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The updated base image with img2 placed.\n",
    "    \"\"\"\n",
    "    assert 0 <= row < img1.shape[0] and 0 <= col < img1.shape[1], \"Pixel location is outside the image.\"\n",
    "    top = row - img2.shape[0] // 2\n",
    "    left = col - img2.shape[1] // 2\n",
    "    bottom = top + img2.shape[0]\n",
    "    right = left + img2.shape[1]\n",
    "\n",
    "    img1_top = max(0, top)\n",
    "    img1_left = max(0, left)\n",
    "    img1_bottom = min(img1.shape[0], bottom)\n",
    "    img1_right = min(img1.shape[1], right)\n",
    "\n",
    "    img2_top = max(0, -top)\n",
    "    img2_left = max(0, -left)\n",
    "    img2_bottom = img2_top + (img1_bottom - img1_top)\n",
    "    img2_right = img2_left + (img1_right - img1_left)\n",
    "\n",
    "    img1[img1_top:img1_bottom, img1_left:img1_right] = img2[img2_top:img2_bottom, img2_left:img2_right]\n",
    "\n",
    "    return img1\n",
    "\n",
    "\n",
    "def monochannel_to_inferno_rgb(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert a monochannel float32 image to an RGB representation using the Inferno\n",
    "    colormap.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input monochannel float32 image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The RGB image with Inferno colormap.\n",
    "    \"\"\"\n",
    "    # Normalize the input image to the range [0, 1]\n",
    "    min_val, max_val = np.min(image), np.max(image)\n",
    "    peak_to_peak = max_val - min_val\n",
    "    if peak_to_peak == 0:\n",
    "        normalized_image = np.zeros_like(image)\n",
    "    else:\n",
    "        normalized_image = (image - min_val) / peak_to_peak\n",
    "\n",
    "    # Apply the Inferno colormap\n",
    "    inferno_colormap = cv2.applyColorMap((normalized_image * 255).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
    "\n",
    "    return inferno_colormap\n",
    "\n",
    "\n",
    "def resize_images(images: List[np.ndarray], match_dimension: str = \"height\", use_max: bool = True) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Resize images to match either their heights or their widths.\n",
    "\n",
    "    Args:\n",
    "        images (List[np.ndarray]): List of NumPy images.\n",
    "        match_dimension (str): Specify 'height' to match heights, or 'width' to match\n",
    "            widths.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of resized images.\n",
    "    \"\"\"\n",
    "    if len(images) == 1:\n",
    "        return images\n",
    "\n",
    "    if match_dimension == \"height\":\n",
    "        if use_max:\n",
    "            new_height = max(img.shape[0] for img in images)\n",
    "        else:\n",
    "            new_height = min(img.shape[0] for img in images)\n",
    "        resized_images = [\n",
    "            cv2.resize(img, (int(img.shape[1] * new_height / img.shape[0]), new_height)) for img in images\n",
    "        ]\n",
    "    elif match_dimension == \"width\":\n",
    "        if use_max:\n",
    "            new_width = max(img.shape[1] for img in images)\n",
    "        else:\n",
    "            new_width = min(img.shape[1] for img in images)\n",
    "        resized_images = [cv2.resize(img, (new_width, int(img.shape[0] * new_width / img.shape[1]))) for img in images]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'match_dimension' argument. Use 'height' or 'width'.\")\n",
    "\n",
    "    return resized_images\n",
    "\n",
    "\n",
    "def crop_white_border(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Crop the image to the bounding box of non-white pixels.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image (BGR format).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The cropped image. If the image is entirely white, the original\n",
    "            image is returned.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale for easier processing\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the bounding box of non-white pixels\n",
    "    non_white_pixels = np.argwhere(gray_image != 255)\n",
    "\n",
    "    if len(non_white_pixels) == 0:\n",
    "        return image  # Return the original image if it's entirely white\n",
    "\n",
    "    min_row, min_col = np.min(non_white_pixels, axis=0)\n",
    "    max_row, max_col = np.max(non_white_pixels, axis=0)\n",
    "\n",
    "    # Crop the image to the bounding box\n",
    "    cropped_image = image[min_row : max_row + 1, min_col : max_col + 1, :]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def pad_to_square(\n",
    "    img: np.ndarray,\n",
    "    padding_color: Tuple[int, int, int] = (255, 255, 255),\n",
    "    extra_pad: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad an image to make it square by adding padding to the left and right sides\n",
    "    if its height is larger than its width, or adding padding to the top and bottom\n",
    "    if its width is larger.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image.\n",
    "        padding_color (Tuple[int, int, int], optional): The padding color in (R, G, B)\n",
    "            format. Defaults to (255, 255, 255).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The squared and padded image.\n",
    "    \"\"\"\n",
    "    height, width, _ = img.shape\n",
    "    larger_side = max(height, width)\n",
    "    square_size = larger_side + extra_pad\n",
    "    padded_img = np.ones((square_size, square_size, 3), dtype=np.uint8) * np.array(padding_color, dtype=np.uint8)\n",
    "    padded_img = place_img_in_img(padded_img, img, square_size // 2, square_size // 2)\n",
    "\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "def pad_larger_dim(image: np.ndarray, target_dimension: int) -> np.ndarray:\n",
    "    \"\"\"Pads an image to the specified target dimension by adding whitespace borders.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image as a NumPy array with shape (height, width,\n",
    "            channels).\n",
    "        target_dimension (int): The desired target dimension for the larger dimension\n",
    "            (height or width).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The padded image as a NumPy array with shape (new_height, new_width,\n",
    "            channels).\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    larger_dimension = max(height, width)\n",
    "\n",
    "    if larger_dimension < target_dimension:\n",
    "        pad_amount = target_dimension - larger_dimension\n",
    "        first_pad_amount = pad_amount // 2\n",
    "        second_pad_amount = pad_amount - first_pad_amount\n",
    "\n",
    "        if height > width:\n",
    "            top_pad = np.ones((first_pad_amount, width, 3), dtype=np.uint8) * 255\n",
    "            bottom_pad = np.ones((second_pad_amount, width, 3), dtype=np.uint8) * 255\n",
    "            padded_image = np.vstack((top_pad, image, bottom_pad))\n",
    "        else:\n",
    "            left_pad = np.ones((height, first_pad_amount, 3), dtype=np.uint8) * 255\n",
    "            right_pad = np.ones((height, second_pad_amount, 3), dtype=np.uint8) * 255\n",
    "            padded_image = np.hstack((left_pad, image, right_pad))\n",
    "    else:\n",
    "        padded_image = image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "\n",
    "def pixel_value_within_radius(\n",
    "    image: np.ndarray,\n",
    "    pixel_location: Tuple[int, int],\n",
    "    radius: int,\n",
    "    reduction: str = \"median\",\n",
    ") -> Union[float, int]:\n",
    "    \"\"\"Returns the maximum pixel value within a given radius of a specified pixel\n",
    "    location in the given image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image as a 2D numpy array.\n",
    "        pixel_location (Tuple[int, int]): The location of the pixel as a tuple (row,\n",
    "            column).\n",
    "        radius (int): The radius within which to find the maximum pixel value.\n",
    "        reduction (str, optional): The method to use to reduce the cropped image to a\n",
    "            single value. Defaults to \"median\".\n",
    "\n",
    "    Returns:\n",
    "        Union[float, int]: The maximum pixel value within the given radius of the pixel\n",
    "            location.\n",
    "    \"\"\"\n",
    "    # Ensure that the pixel location is within the image\n",
    "    assert (\n",
    "        0 <= pixel_location[0] < image.shape[0] and 0 <= pixel_location[1] < image.shape[1]\n",
    "    ), \"Pixel location is outside the image.\"\n",
    "\n",
    "    top_left_x = max(0, pixel_location[0] - radius)\n",
    "    top_left_y = max(0, pixel_location[1] - radius)\n",
    "    bottom_right_x = min(image.shape[0], pixel_location[0] + radius + 1)\n",
    "    bottom_right_y = min(image.shape[1], pixel_location[1] + radius + 1)\n",
    "    cropped_image = image[top_left_x:bottom_right_x, top_left_y:bottom_right_y]\n",
    "\n",
    "    # Draw a circular mask for the cropped image\n",
    "    circle_mask = np.zeros(cropped_image.shape[:2], dtype=np.uint8)\n",
    "    circle_mask = cv2.circle(\n",
    "        circle_mask,\n",
    "        (radius, radius),\n",
    "        radius,\n",
    "        color=255,\n",
    "        thickness=-1,\n",
    "    )\n",
    "    overlap_values = cropped_image[circle_mask > 0]\n",
    "    # Filter out any values that are 0 (i.e. pixels that weren't seen yet)\n",
    "    overlap_values = overlap_values[overlap_values > 0]\n",
    "    if overlap_values.size == 0:\n",
    "        return -1\n",
    "    elif reduction == \"mean\":\n",
    "        return np.mean(overlap_values)  # type: ignore\n",
    "    elif reduction == \"max\":\n",
    "        return np.max(overlap_values)\n",
    "    elif reduction == \"median\":\n",
    "        return np.median(overlap_values)  # type: ignore\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction method: {reduction}\")\n",
    "\n",
    "\n",
    "def median_blur_normalized_depth_image(depth_image: np.ndarray, ksize: int) -> np.ndarray:\n",
    "    \"\"\"Applies a median blur to a normalized depth image.\n",
    "\n",
    "    This function first converts the normalized depth image to a uint8 image,\n",
    "    then applies a median blur, and finally converts the blurred image back\n",
    "    to a normalized float32 image.\n",
    "\n",
    "    Args:\n",
    "        depth_image (np.ndarray): The input depth image. This should be a\n",
    "            normalized float32 image.\n",
    "        ksize (int): The size of the kernel to be used in the median blur.\n",
    "            This should be an odd number greater than 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The blurred depth image. This is a normalized float32 image.\n",
    "    \"\"\"\n",
    "    # Convert the normalized depth image to a uint8 image\n",
    "    depth_image_uint8 = (depth_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply median blur\n",
    "    blurred_depth_image_uint8 = cv2.medianBlur(depth_image_uint8, ksize)\n",
    "\n",
    "    # Convert the blurred image back to a normalized float32 image\n",
    "    blurred_depth_image = blurred_depth_image_uint8.astype(np.float32) / 255\n",
    "\n",
    "    return blurred_depth_image\n",
    "\n",
    "\n",
    "def reorient_rescale_map(vis_map_img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Reorient and rescale a visual map image for display.\n",
    "\n",
    "    This function preprocesses a visual map image by:\n",
    "    1. Cropping whitespace borders\n",
    "    2. Padding the smaller dimension to at least 150px\n",
    "    3. Padding the image to a square\n",
    "    4. Adding a 50px whitespace border\n",
    "\n",
    "    Args:\n",
    "        vis_map_img (np.ndarray): The input visual map image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reoriented and rescaled visual map image\n",
    "    \"\"\"\n",
    "    # Remove unnecessary white space around the edges\n",
    "    vis_map_img = crop_white_border(vis_map_img)\n",
    "    # Make the image at least 150 pixels tall or wide\n",
    "    vis_map_img = pad_larger_dim(vis_map_img, 150)\n",
    "    # Pad the shorter dimension to be the same size as the longer\n",
    "    vis_map_img = pad_to_square(vis_map_img, extra_pad=50)\n",
    "    # Pad the image border with some white space\n",
    "    vis_map_img = cv2.copyMakeBorder(vis_map_img, 50, 50, 50, 50, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "    return vis_map_img\n",
    "\n",
    "\n",
    "def remove_small_blobs(image: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    # Find all contours in the image\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        # Calculate area of the contour\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # If area is smaller than the threshold, remove the contour\n",
    "        if area < min_area:\n",
    "            cv2.drawContours(image, [contour], -1, 0, -1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_image(img: np.ndarray, new_height: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes an image to a given height while maintaining the aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): The input image.\n",
    "        new_height (int): The desired height for the resized image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resized image.\n",
    "    \"\"\"\n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = img.shape[1] / img.shape[0]\n",
    "\n",
    "    # Calculate the new width\n",
    "    new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def fill_small_holes(depth_img: np.ndarray, area_thresh: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Identifies regions in the depth image that have a value of 0 and fills them in\n",
    "    with 1 if the region is smaller than a given area threshold.\n",
    "\n",
    "    Args:\n",
    "        depth_img (np.ndarray): The input depth image\n",
    "        area_thresh (int): The area threshold for filling in holes\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The depth image with small holes filled in\n",
    "    \"\"\"\n",
    "    # Create a binary image where holes are 1 and the rest is 0\n",
    "    binary_img = np.where(depth_img == 0, 1, 0).astype(\"uint8\")\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    filled_holes = np.zeros_like(binary_img)\n",
    "\n",
    "    for cnt in contours:\n",
    "        # If the area of the contour is smaller than the threshold\n",
    "        if cv2.contourArea(cnt) < area_thresh:\n",
    "            # Fill the contour\n",
    "            cv2.drawContours(filled_holes, [cnt], 0, 1, -1)\n",
    "\n",
    "    # Create the filled depth image\n",
    "    filled_depth_img = np.where(filled_holes == 1, 1, depth_img)\n",
    "\n",
    "    return filled_depth_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b15ba722-6848-49e8-9144-1dbdd85cc7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3387.557] global loadsave.cpp:275 findDecoder imread_('depth.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 461\u001b[0m\n\u001b[1;32m    458\u001b[0m     quit()\n\u001b[1;32m    460\u001b[0m v \u001b[38;5;241m=\u001b[39m ValueMap(value_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 461\u001b[0m depth \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    462\u001b[0m img \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39m_process_local_data(\n\u001b[1;32m    463\u001b[0m     depth\u001b[38;5;241m=\u001b[39mdepth,\n\u001b[1;32m    464\u001b[0m     fov\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdeg2rad(\u001b[38;5;241m79\u001b[39m),\n\u001b[1;32m    465\u001b[0m     min_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    466\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m,\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    468\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m, (img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "SAVE_VISUALIZATIONS = False\n",
    "RECORDING = os.environ.get(\"RECORD_VALUE_MAP\", \"0\") == \"1\"\n",
    "PLAYING = os.environ.get(\"PLAY_VALUE_MAP\", \"0\") == \"1\"\n",
    "RECORDING_DIR = \"value_map_recordings\"\n",
    "JSON_PATH = osp.join(RECORDING_DIR, \"data.json\")\n",
    "KWARGS_JSON = osp.join(RECORDING_DIR, \"kwargs.json\")\n",
    "\n",
    "\n",
    "class ValueMap(BaseMap):\n",
    "    \"\"\"Generates a map representing how valuable explored regions of the environment\n",
    "    are with respect to finding and navigating to the target object.\"\"\"\n",
    "\n",
    "    _confidence_masks: Dict[Tuple[float, float], np.ndarray] = {}\n",
    "    _camera_positions: List[np.ndarray] = []\n",
    "    _last_camera_yaw: float = 0.0\n",
    "    _min_confidence: float = 0.25\n",
    "    _decision_threshold: float = 0.35\n",
    "    _map: np.ndarray\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        value_channels: int,\n",
    "        size: int = 1000,\n",
    "        use_max_confidence: bool = True,\n",
    "        fusion_type: str = \"default\",\n",
    "        obstacle_map: Optional[\"ObstacleMap\"] = None,  # type: ignore # noqa: F821\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            value_channels: The number of channels in the value map.\n",
    "            size: The size of the value map in pixels.\n",
    "            use_max_confidence: Whether to use the maximum confidence value in the value\n",
    "                map or a weighted average confidence value.\n",
    "            fusion_type: The type of fusion to use when combining the value map with the\n",
    "                obstacle map.\n",
    "            obstacle_map: An optional obstacle map to use for overriding the occluded\n",
    "                areas of the FOV\n",
    "        \"\"\"\n",
    "        if PLAYING:\n",
    "            size = 2000\n",
    "        super().__init__(size)\n",
    "        self._value_map = np.zeros((size, size, value_channels), np.float32)\n",
    "        self._value_channels = value_channels\n",
    "        self._use_max_confidence = use_max_confidence\n",
    "        self._fusion_type = fusion_type\n",
    "        self._obstacle_map = obstacle_map\n",
    "        if self._obstacle_map is not None:\n",
    "            assert self._obstacle_map.pixels_per_meter == self.pixels_per_meter\n",
    "            assert self._obstacle_map.size == self.size\n",
    "        if os.environ.get(\"MAP_FUSION_TYPE\", \"\") != \"\":\n",
    "            self._fusion_type = os.environ[\"MAP_FUSION_TYPE\"]\n",
    "\n",
    "        if RECORDING:\n",
    "            if osp.isdir(RECORDING_DIR):\n",
    "                warnings.warn(f\"Recording directory {RECORDING_DIR} already exists. Deleting it.\")\n",
    "                shutil.rmtree(RECORDING_DIR)\n",
    "            os.mkdir(RECORDING_DIR)\n",
    "            # Dump all args to a file\n",
    "            with open(KWARGS_JSON, \"w\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"value_channels\": value_channels,\n",
    "                        \"size\": size,\n",
    "                        \"use_max_confidence\": use_max_confidence,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "            # Create a blank .json file inside for now\n",
    "            with open(JSON_PATH, \"w\") as f:\n",
    "                f.write(\"{}\")\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self._value_map.fill(0)\n",
    "\n",
    "    def update_map(\n",
    "        self,\n",
    "        values: np.ndarray,\n",
    "        depth: np.ndarray,\n",
    "        tf_camera_to_episodic: np.ndarray,\n",
    "        min_depth: float,\n",
    "        max_depth: float,\n",
    "        fov: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Updates the value map with the given depth image, pose, and value to use.\n",
    "\n",
    "        Args:\n",
    "            values: The value to use for updating the map.\n",
    "            depth: The depth image to use for updating the map; expected to be already\n",
    "                normalized to the range [0, 1].\n",
    "            tf_camera_to_episodic: The transformation matrix from the episodic frame to\n",
    "                the camera frame.\n",
    "            min_depth: The minimum depth value in meters.\n",
    "            max_depth: The maximum depth value in meters.\n",
    "            fov: The field of view of the camera in RADIANS.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(values) == self._value_channels\n",
    "        ), f\"Incorrect number of values given ({len(values)}). Expected {self._value_channels}.\"\n",
    "\n",
    "        curr_map = self._localize_new_data(depth, tf_camera_to_episodic, min_depth, max_depth, fov)\n",
    "\n",
    "        # Fuse the new data with the existing data\n",
    "        self._fuse_new_data(curr_map, values)\n",
    "\n",
    "        if RECORDING:\n",
    "            idx = len(glob.glob(osp.join(RECORDING_DIR, \"*.png\")))\n",
    "            img_path = osp.join(RECORDING_DIR, f\"{idx:04d}.png\")\n",
    "            cv2.imwrite(img_path, (depth * 255).astype(np.uint8))\n",
    "            with open(JSON_PATH, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            data[img_path] = {\n",
    "                \"values\": values.tolist(),\n",
    "                \"tf_camera_to_episodic\": tf_camera_to_episodic.tolist(),\n",
    "                \"min_depth\": min_depth,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"fov\": fov,\n",
    "            }\n",
    "            with open(JSON_PATH, \"w\") as f:\n",
    "                json.dump(data, f)\n",
    "\n",
    "    def sort_waypoints(\n",
    "        self, waypoints: np.ndarray, radius: float, reduce_fn: Optional[Callable] = None\n",
    "    ) -> Tuple[np.ndarray, List[float]]:\n",
    "        \"\"\"Selects the best waypoint from the given list of waypoints.\n",
    "\n",
    "        Args:\n",
    "            waypoints (np.ndarray): An array of 2D waypoints to choose from.\n",
    "            radius (float): The radius in meters to use for selecting the best waypoint.\n",
    "            reduce_fn (Callable, optional): The function to use for reducing the values\n",
    "                within the given radius. Defaults to np.max.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, List[float]]: A tuple of the sorted waypoints and\n",
    "                their corresponding values.\n",
    "        \"\"\"\n",
    "        radius_px = int(radius * self.pixels_per_meter)\n",
    "\n",
    "        def get_value(point: np.ndarray) -> Union[float, Tuple[float, ...]]:\n",
    "            x, y = point\n",
    "            px = int(-x * self.pixels_per_meter) + self._episode_pixel_origin[0]\n",
    "            py = int(-y * self.pixels_per_meter) + self._episode_pixel_origin[1]\n",
    "            point_px = (self._value_map.shape[0] - px, py)\n",
    "            all_values = [\n",
    "                pixel_value_within_radius(self._value_map[..., c], point_px, radius_px)\n",
    "                for c in range(self._value_channels)\n",
    "            ]\n",
    "            if len(all_values) == 1:\n",
    "                return all_values[0]\n",
    "            return tuple(all_values)\n",
    "\n",
    "        values = [get_value(point) for point in waypoints]\n",
    "\n",
    "        if self._value_channels > 1:\n",
    "            assert reduce_fn is not None, \"Must provide a reduction function when using multiple value channels.\"\n",
    "            values = reduce_fn(values)\n",
    "\n",
    "        # Use np.argsort to get the indices of the sorted values\n",
    "        sorted_inds = np.argsort([-v for v in values])  # type: ignore\n",
    "        sorted_values = [values[i] for i in sorted_inds]\n",
    "        sorted_frontiers = np.array([waypoints[i] for i in sorted_inds])\n",
    "\n",
    "        return sorted_frontiers, sorted_values\n",
    "\n",
    "    def visualize(\n",
    "        self,\n",
    "        markers: Optional[List[Tuple[np.ndarray, Dict[str, Any]]]] = None,\n",
    "        reduce_fn: Callable = lambda i: np.max(i, axis=-1),\n",
    "        obstacle_map: Optional[\"ObstacleMap\"] = None,  # type: ignore # noqa: F821\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Return an image representation of the map\"\"\"\n",
    "        # Must negate the y values to get the correct orientation\n",
    "        reduced_map = reduce_fn(self._value_map).copy()\n",
    "        if obstacle_map is not None:\n",
    "            reduced_map[obstacle_map.explored_area == 0] = 0\n",
    "        map_img = np.flipud(reduced_map)\n",
    "        # Make all 0s in the value map equal to the max value, so they don't throw off\n",
    "        # the color mapping (will revert later)\n",
    "        zero_mask = map_img == 0\n",
    "        map_img[zero_mask] = np.max(map_img)\n",
    "        map_img = monochannel_to_inferno_rgb(map_img)\n",
    "        # Revert all values that were originally zero to white\n",
    "        map_img[zero_mask] = (255, 255, 255)\n",
    "        if len(self._camera_positions) > 0:\n",
    "            self._traj_vis.draw_trajectory(\n",
    "                map_img,\n",
    "                self._camera_positions,\n",
    "                self._last_camera_yaw,\n",
    "            )\n",
    "\n",
    "            if markers is not None:\n",
    "                for pos, marker_kwargs in markers:\n",
    "                    map_img = self._traj_vis.draw_circle(map_img, pos, **marker_kwargs)\n",
    "\n",
    "        return map_img\n",
    "\n",
    "    def _process_local_data(self, depth: np.ndarray, fov: float, min_depth: float, max_depth: float) -> np.ndarray:\n",
    "        \"\"\"Using the FOV and depth, return the visible portion of the FOV.\n",
    "\n",
    "        Args:\n",
    "            depth: The depth image to use for determining the visible portion of the\n",
    "                FOV.\n",
    "        Returns:\n",
    "            A mask of the visible portion of the FOV.\n",
    "        \"\"\"\n",
    "        # Squeeze out the channel dimension if depth is a 3D array\n",
    "        if len(depth.shape) == 3:\n",
    "            depth = depth.squeeze(2)\n",
    "        # Squash depth image into one row with the max depth value for each column\n",
    "        depth_row = np.max(depth, axis=0) * (max_depth - min_depth) + min_depth\n",
    "\n",
    "        # Create a linspace of the same length as the depth row from -fov/2 to fov/2\n",
    "        angles = np.linspace(-fov / 2, fov / 2, len(depth_row))\n",
    "\n",
    "        # Assign each value in the row with an x, y coordinate depending on 'angles'\n",
    "        # and the max depth value for that column\n",
    "        x = depth_row\n",
    "        y = depth_row * np.tan(angles)\n",
    "\n",
    "        # Get blank cone mask\n",
    "        cone_mask = self._get_confidence_mask(fov, max_depth)\n",
    "\n",
    "        # Convert the x, y coordinates to pixel coordinates\n",
    "        x = (x * self.pixels_per_meter + cone_mask.shape[0] / 2).astype(int)\n",
    "        y = (y * self.pixels_per_meter + cone_mask.shape[1] / 2).astype(int)\n",
    "\n",
    "        # Create a contour from the x, y coordinates, with the top left and right\n",
    "        # corners of the image as the first two points\n",
    "        last_row = cone_mask.shape[0] - 1\n",
    "        last_col = cone_mask.shape[1] - 1\n",
    "        start = np.array([[0, last_col]])\n",
    "        end = np.array([[last_row, last_col]])\n",
    "        contour = np.concatenate((start, np.stack((y, x), axis=1), end), axis=0)\n",
    "\n",
    "        # Draw the contour onto the cone mask, in filled-in black\n",
    "        visible_mask = cv2.drawContours(cone_mask, [contour], -1, 0, -1)  # type: ignore\n",
    "\n",
    "        if DEBUG:\n",
    "            vis = cv2.cvtColor((cone_mask * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "            cv2.drawContours(vis, [contour], -1, (0, 0, 255), -1)\n",
    "            for point in contour:\n",
    "                vis[point[1], point[0]] = (0, 255, 0)\n",
    "            if SAVE_VISUALIZATIONS:\n",
    "                # Create visualizations directory if it doesn't exist\n",
    "                if not os.path.exists(\"visualizations\"):\n",
    "                    os.makedirs(\"visualizations\")\n",
    "                # Expand the depth_row back into a full image\n",
    "                depth_row_full = np.repeat(depth_row.reshape(1, -1), depth.shape[0], axis=0)\n",
    "                # Stack the depth images with the visible mask\n",
    "                depth_rgb = cv2.cvtColor((depth * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "                depth_row_full = cv2.cvtColor((depth_row_full * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "                vis = np.flipud(vis)\n",
    "                new_width = int(vis.shape[1] * (depth_rgb.shape[0] / vis.shape[0]))\n",
    "                vis_resized = cv2.resize(vis, (new_width, depth_rgb.shape[0]))\n",
    "                vis = np.hstack((depth_rgb, depth_row_full, vis_resized))\n",
    "                time_id = int(time.time() * 1000)\n",
    "                cv2.imwrite(f\"visualizations/{time_id}.png\", vis)\n",
    "            else:\n",
    "                cv2.imshow(\"obstacle mask\", vis)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "        return visible_mask\n",
    "\n",
    "    def _localize_new_data(\n",
    "        self,\n",
    "        depth: np.ndarray,\n",
    "        tf_camera_to_episodic: np.ndarray,\n",
    "        min_depth: float,\n",
    "        max_depth: float,\n",
    "        fov: float,\n",
    "    ) -> np.ndarray:\n",
    "        # Get new portion of the map\n",
    "        curr_data = self._process_local_data(depth, fov, min_depth, max_depth)\n",
    "\n",
    "        # Rotate this new data to match the camera's orientation\n",
    "        yaw = extract_yaw(tf_camera_to_episodic)\n",
    "        if PLAYING:\n",
    "            if yaw > 0:\n",
    "                yaw = 0\n",
    "            else:\n",
    "                yaw = np.deg2rad(30)\n",
    "        curr_data = rotate_image(curr_data, -yaw)\n",
    "\n",
    "        # Determine where this mask should be overlaid\n",
    "        cam_x, cam_y = tf_camera_to_episodic[:2, 3] / tf_camera_to_episodic[3, 3]\n",
    "\n",
    "        # Convert to pixel units\n",
    "        px = int(cam_x * self.pixels_per_meter) + self._episode_pixel_origin[0]\n",
    "        py = int(-cam_y * self.pixels_per_meter) + self._episode_pixel_origin[1]\n",
    "\n",
    "        # Overlay the new data onto the map\n",
    "        curr_map = np.zeros_like(self._map)\n",
    "        curr_map = place_img_in_img(curr_map, curr_data, px, py)\n",
    "\n",
    "        return curr_map\n",
    "\n",
    "    def _get_blank_cone_mask(self, fov: float, max_depth: float) -> np.ndarray:\n",
    "        \"\"\"Generate a FOV cone without any obstacles considered\"\"\"\n",
    "        size = int(max_depth * self.pixels_per_meter)\n",
    "        cone_mask = np.zeros((size * 2 + 1, size * 2 + 1))\n",
    "        cone_mask = cv2.ellipse(  # type: ignore\n",
    "            cone_mask,\n",
    "            (size, size),  # center_pixel\n",
    "            (size, size),  # axes lengths\n",
    "            0,  # angle circle is rotated\n",
    "            -np.rad2deg(fov) / 2 + 90,  # start_angle\n",
    "            np.rad2deg(fov) / 2 + 90,  # end_angle\n",
    "            1,  # color\n",
    "            -1,  # thickness\n",
    "        )\n",
    "        return cone_mask\n",
    "\n",
    "    def _get_confidence_mask(self, fov: float, max_depth: float) -> np.ndarray:\n",
    "        \"\"\"Generate a FOV cone with central values weighted more heavily\"\"\"\n",
    "        if (fov, max_depth) in self._confidence_masks:\n",
    "            return self._confidence_masks[(fov, max_depth)].copy()\n",
    "        cone_mask = self._get_blank_cone_mask(fov, max_depth)\n",
    "        adjusted_mask = np.zeros_like(cone_mask).astype(np.float32)\n",
    "        for row in range(adjusted_mask.shape[0]):\n",
    "            for col in range(adjusted_mask.shape[1]):\n",
    "                horizontal = abs(row - adjusted_mask.shape[0] // 2)\n",
    "                vertical = abs(col - adjusted_mask.shape[1] // 2)\n",
    "                angle = np.arctan2(vertical, horizontal)\n",
    "                angle = remap(angle, 0, fov / 2, 0, np.pi / 2)\n",
    "                confidence = np.cos(angle) ** 2\n",
    "                confidence = remap(confidence, 0, 1, self._min_confidence, 1)\n",
    "                adjusted_mask[row, col] = confidence\n",
    "        adjusted_mask = adjusted_mask * cone_mask\n",
    "        self._confidence_masks[(fov, max_depth)] = adjusted_mask.copy()\n",
    "\n",
    "        return adjusted_mask\n",
    "\n",
    "    def _fuse_new_data(self, new_map: np.ndarray, values: np.ndarray) -> None:\n",
    "        \"\"\"Fuse the new data with the existing value and confidence maps.\n",
    "\n",
    "        Args:\n",
    "            new_map: The new new_map map data to fuse. Confidences are between\n",
    "                0 and 1, with 1 being the most confident.\n",
    "            values: The values attributed to the new portion of the map.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            len(values) == self._value_channels\n",
    "        ), f\"Incorrect number of values given ({len(values)}). Expected {self._value_channels}.\"\n",
    "\n",
    "        if self._obstacle_map is not None:\n",
    "            # If an obstacle map is provided, we will use it to mask out the\n",
    "            # new map\n",
    "            explored_area = self._obstacle_map.explored_area\n",
    "            new_map[explored_area == 0] = 0\n",
    "            self._map[explored_area == 0] = 0\n",
    "            self._value_map[explored_area == 0] *= 0\n",
    "\n",
    "        if self._fusion_type == \"replace\":\n",
    "            # Ablation. The values from the current observation will overwrite any\n",
    "            # existing values\n",
    "            print(\"VALUE MAP ABLATION:\", self._fusion_type)\n",
    "            new_value_map = np.zeros_like(self._value_map)\n",
    "            new_value_map[new_map > 0] = values\n",
    "            self._map[new_map > 0] = new_map[new_map > 0]\n",
    "            self._value_map[new_map > 0] = new_value_map[new_map > 0]\n",
    "            return\n",
    "        elif self._fusion_type == \"equal_weighting\":\n",
    "            # Ablation. Updated values will always be the mean of the current and\n",
    "            # new values, meaning that confidence scores are forced to be the same.\n",
    "            print(\"VALUE MAP ABLATION:\", self._fusion_type)\n",
    "            self._map[self._map > 0] = 1\n",
    "            new_map[new_map > 0] = 1\n",
    "        else:\n",
    "            assert self._fusion_type == \"default\", f\"Unknown fusion type {self._fusion_type}\"\n",
    "\n",
    "        # Any values in the given map that are less confident than\n",
    "        # self._decision_threshold AND less than the new_map in the existing map\n",
    "        # will be silenced into 0s\n",
    "        new_map_mask = np.logical_and(new_map < self._decision_threshold, new_map < self._map)\n",
    "        new_map[new_map_mask] = 0\n",
    "\n",
    "        if self._use_max_confidence:\n",
    "            # For every pixel that has a higher new_map in the new map than the\n",
    "            # existing value map, replace the value in the existing value map with\n",
    "            # the new value\n",
    "            higher_new_map_mask = new_map > self._map\n",
    "            self._value_map[higher_new_map_mask] = values\n",
    "            # Update the new_map map with the new new_map values\n",
    "            self._map[higher_new_map_mask] = new_map[higher_new_map_mask]\n",
    "        else:\n",
    "            # Each pixel in the existing value map will be updated with a weighted\n",
    "            # average of the existing value and the new value. The weight of each value\n",
    "            # is determined by the current and new new_map values. The new_map map\n",
    "            # will also be updated with using a weighted average in a similar manner.\n",
    "            confidence_denominator = self._map + new_map\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "                weight_1 = self._map / confidence_denominator\n",
    "                weight_2 = new_map / confidence_denominator\n",
    "\n",
    "            weight_1_channeled = np.repeat(np.expand_dims(weight_1, axis=2), self._value_channels, axis=2)\n",
    "            weight_2_channeled = np.repeat(np.expand_dims(weight_2, axis=2), self._value_channels, axis=2)\n",
    "\n",
    "            self._value_map = self._value_map * weight_1_channeled + values * weight_2_channeled\n",
    "            self._map = self._map * weight_1 + new_map * weight_2\n",
    "\n",
    "            # Because confidence_denominator can have 0 values, any nans in either the\n",
    "            # value or confidence maps will be replaced with 0\n",
    "            self._value_map = np.nan_to_num(self._value_map)\n",
    "            self._map = np.nan_to_num(self._map)\n",
    "\n",
    "\n",
    "def remap(value: float, from_low: float, from_high: float, to_low: float, to_high: float) -> float:\n",
    "    \"\"\"Maps a value from one range to another.\n",
    "\n",
    "    Args:\n",
    "        value (float): The value to be mapped.\n",
    "        from_low (float): The lower bound of the input range.\n",
    "        from_high (float): The upper bound of the input range.\n",
    "        to_low (float): The lower bound of the output range.\n",
    "        to_high (float): The upper bound of the output range.\n",
    "\n",
    "    Returns:\n",
    "        float: The mapped value.\n",
    "    \"\"\"\n",
    "    return (value - from_low) * (to_high - to_low) / (from_high - from_low) + to_low\n",
    "\n",
    "\n",
    "def replay_from_dir() -> None:\n",
    "    with open(KWARGS_JSON, \"r\") as f:\n",
    "        kwargs = json.load(f)\n",
    "    with open(JSON_PATH, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    v = ValueMap(**kwargs)\n",
    "\n",
    "    sorted_keys = sorted(list(data.keys()))\n",
    "\n",
    "    for img_path in sorted_keys:\n",
    "        tf_camera_to_episodic = np.array(data[img_path][\"tf_camera_to_episodic\"])\n",
    "        values = np.array(data[img_path][\"values\"])\n",
    "        depth = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "        v.update_map(\n",
    "            values,\n",
    "            depth,\n",
    "            tf_camera_to_episodic,\n",
    "            float(data[img_path][\"min_depth\"]),\n",
    "            float(data[img_path][\"max_depth\"]),\n",
    "            float(data[img_path][\"fov\"]),\n",
    "        )\n",
    "\n",
    "        img = v.visualize()\n",
    "        cv2.imshow(\"img\", img)\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if PLAYING:\n",
    "        replay_from_dir()\n",
    "        quit()\n",
    "\n",
    "    v = ValueMap(value_channels=1)\n",
    "    depth = cv2.imread(\"depth.png\", cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "    img = v._process_local_data(\n",
    "        depth=depth,\n",
    "        fov=np.deg2rad(79),\n",
    "        min_depth=0.5,\n",
    "        max_depth=5.0,\n",
    "    )\n",
    "    cv2.imshow(\"img\", (img * 255).astype(np.uint8))\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    num_points = 20\n",
    "\n",
    "    x = [0, 10, 10, 0]\n",
    "    y = [0, 0, 10, 10]\n",
    "    angles = [0, np.pi / 2, np.pi, 3 * np.pi / 2]\n",
    "\n",
    "    points = np.stack((x, y), axis=1)\n",
    "\n",
    "    for pt, angle in zip(points, angles):\n",
    "        tf = np.eye(4)\n",
    "        tf[:2, 3] = pt\n",
    "        tf[:2, :2] = get_rotation_matrix(angle)\n",
    "        v.update_map(\n",
    "            np.array([1]),\n",
    "            depth,\n",
    "            tf,\n",
    "            min_depth=0.5,\n",
    "            max_depth=5.0,\n",
    "            fov=np.deg2rad(79),\n",
    "        )\n",
    "        img = v.visualize()\n",
    "        cv2.imshow(\"img\", img)\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord(\"q\"):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5850d5-955b-4b3f-9ffa-5e604026055f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
